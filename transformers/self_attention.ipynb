{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- https://www.youtube.com/watch?v=QCJQG4DuHT0"
      ],
      "metadata": {
        "id": "DbNMYMzyxJyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pprdCld5e1O_"
      },
      "outputs": [],
      "source": [
        "# https://www.youtube.com/watch?v=QCJQG4DuHT0\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of input sequence (my name is Ajay)\n",
        "# size of vectors (illustrative purposes choose 8)\n",
        "L, d_k, d_v = 4, 8, 8\n",
        "\n",
        "# q, k, v represent vectors\n",
        "# q = what I am looking for\n",
        "# w = what I can offer\n",
        "# v = what I actually offer\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)"
      ],
      "metadata": {
        "id": "nctgT_6GfxkU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q\\n\", q) # each word represented as 8x1 vector\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AotjbNn3fp8P",
        "outputId": "e9f6483f-71b0-4793-8ca3-6d6b1b708868"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
            "   1.57921282  0.76743473]\n",
            " [-0.46947439  0.54256004 -0.46341769 -0.46572975  0.24196227 -1.91328024\n",
            "  -1.72491783 -0.56228753]\n",
            " [-1.01283112  0.31424733 -0.90802408 -1.4123037   1.46564877 -0.2257763\n",
            "   0.0675282  -1.42474819]\n",
            " [-0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
            "  -0.60170661  1.85227818]]\n",
            "K\n",
            " [[-0.01349722 -1.05771093  0.82254491 -1.22084365  0.2088636  -1.95967012\n",
            "  -1.32818605  0.19686124]\n",
            " [ 0.73846658  0.17136828 -0.11564828 -0.3011037  -1.47852199 -0.71984421\n",
            "  -0.46063877  1.05712223]\n",
            " [ 0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
            "   1.03099952  0.93128012]\n",
            " [-0.83921752 -0.30921238  0.33126343  0.97554513 -0.47917424 -0.18565898\n",
            "  -1.10633497 -1.19620662]]\n",
            "V\n",
            " [[ 0.81252582  1.35624003 -0.07201012  1.0035329   0.36163603 -0.64511975\n",
            "   0.36139561  1.53803657]\n",
            " [-0.03582604  1.56464366 -2.6197451   0.8219025   0.08704707 -0.29900735\n",
            "   0.09176078 -1.98756891]\n",
            " [-0.21967189  0.35711257  1.47789404 -0.51827022 -0.8084936  -0.50175704\n",
            "   0.91540212  0.32875111]\n",
            " [-0.5297602   0.51326743  0.09707755  0.96864499 -0.70205309 -0.32766215\n",
            "  -0.39210815 -1.46351495]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention\n",
        "\n",
        "$$\n",
        "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{new V} = \\text{self attention}.V\n",
        "$$ "
      ],
      "metadata": {
        "id": "woTnH1WgitQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# leads to a 4x4 matrix because we had a sequence of 4 words\n",
        "# each case it's going to be proportional to exactly how much attention\n",
        "# we want to focus on each word\n",
        "np.matmul(q, k.T)\n",
        "\n",
        "# for example\n",
        "# the first line is going to be the my vector and how much it's going\n",
        "# to focus on other vectors\n",
        "# in this example, going to focus the most on the word name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpyCw-GVitwa",
        "outputId": "fbda608d-b75d-4a94-99f1-5eb98ba09d0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.72357421,  0.40818741,  2.39601116, -1.18323729],\n",
              "       [ 5.60012069,  1.1597874 , -4.7248515 ,  2.43859568],\n",
              "       [ 1.03699903, -3.70553788, -3.0399309 ,  0.04345596],\n",
              "       [ 0.09460324,  2.97027193,  0.43247995, -0.80026704]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# why do we need the sqrt(d_k)\n",
        "# we want to minimize variance and hence stabilize the values of the\n",
        "# Q dot K transpose vector\n",
        "\n",
        "q.var(), k.var(), np.matmul(q, k.T).var()\n",
        "\n",
        "# the variance of the multiplication is much higher\n",
        "# so in order to make sure we stabilize these values and reduce its variance\n",
        "# we divide it by the sqrt() of the dimension of the query vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVZT6M2BnHSU",
        "outputId": "b4ddbe82-a076-4527-f295-40318f418a25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8669372677550163, 0.7119028180363141, 6.837565240983166)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled = np.matmul(q, k.T) / np.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()\n",
        "\n",
        "# if we apply the scaling you'll see that the vector generated\n",
        "# will now have much lower variance and in the same range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptVyTggyoKbu",
        "outputId": "efb6254f-74f1-4fef-b41d-0ebdeac6f64c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8669372677550163, 0.7119028180363141, 0.8546956551228956)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking"
      ],
      "metadata": {
        "id": "jmlW90lbxR-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Masking\n",
        "# required in decoder part so that we don't look at a future word when trying\n",
        "# to generate the current context of the current word, this will be considered\n",
        "# cheating\n",
        "\n",
        "# not required in encoder part because all of our inputs are passed into the\n",
        "# Transformer simultaneously\n",
        "\n",
        "mask = np.tril(np.ones((L, L)))\n",
        "mask\n",
        "\n",
        "# for example, my can only look at itself\n",
        "# name can only look at my and name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dsQU-e-oc9Q",
        "outputId": "e5e46bcf-05df-4d81-80a4-77c46e174db8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some transformation\n",
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu1_RN_MpOuU",
        "outputId": "44760aa4-f9cf-4bd2-ca2a-c742d584c2a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled + mask\n",
        "# the values above the mask are considered -infinity\n",
        "# why negative infinity?\n",
        "# because of softmax operation - used to convert a vector into a \n",
        "# probability distribution\n",
        "# their values add up to 1 and they're also very interpretable and stable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APFs1LXQpjhj",
        "outputId": "0536a326-a9d0-4066-e797-5171b6c9a458"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.9629289 ,        -inf,        -inf,        -inf],\n",
              "       [ 1.97994166,  0.41004677,        -inf,        -inf],\n",
              "       [ 0.36663452, -1.31010548, -1.07477788,        -inf],\n",
              "       [ 0.0334473 ,  1.05014971,  0.15290475, -0.28293713]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax\n",
        "\n",
        "$$\n",
        "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
        "$$"
      ],
      "metadata": {
        "id": "LyBNHjTsp8x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
      ],
      "metadata": {
        "id": "yUA96TOJp9N9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no mask\n",
        "attention = softmax(scaled)\n",
        "attention\n",
        "\n",
        "# every row adds up to 1 because it's a probability distribution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYayNew6qEuG",
        "outputId": "857a1728-51e9-40c2-9076-cf57e89c3505"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.08431243, 0.25513027, 0.51521078, 0.14534652],\n",
              "       [0.64059204, 0.1332861 , 0.01664257, 0.2094793 ],\n",
              "       [0.47006414, 0.08789379, 0.11121405, 0.33082801],\n",
              "       [0.17794451, 0.49185018, 0.20052305, 0.12968226]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mask\n",
        "attention = softmax(scaled + mask)\n",
        "attention\n",
        "\n",
        "# notice that the attention vector doesn't incorporate anything or any word\n",
        "# that comes after it\n",
        "# this is required for the decoder, but not for the encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fVFQW2VqJXW",
        "outputId": "eedccc1b-b889-4b7f-bd23-38e4b51265fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.82776862, 0.17223138, 0.        , 0.        ],\n",
              "       [0.7024564 , 0.13134709, 0.16619652, 0.        ],\n",
              "       [0.17794451, 0.49185018, 0.20052305, 0.12968226]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# before applying attention\n",
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD3fCg0LrG9V",
        "outputId": "35b34fee-58ad-4005-d675-2326be719a00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.81252582,  1.35624003, -0.07201012,  1.0035329 ,  0.36163603,\n",
              "        -0.64511975,  0.36139561,  1.53803657],\n",
              "       [-0.03582604,  1.56464366, -2.6197451 ,  0.8219025 ,  0.08704707,\n",
              "        -0.29900735,  0.09176078, -1.98756891],\n",
              "       [-0.21967189,  0.35711257,  1.47789404, -0.51827022, -0.8084936 ,\n",
              "        -0.50175704,  0.91540212,  0.32875111],\n",
              "       [-0.5297602 ,  0.51326743,  0.09707755,  0.96864499, -0.70205309,\n",
              "        -0.32766215, -0.39210815, -1.46351495]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we multiply the attention matrix and the value matrix\n",
        "# we get these new set of matrices which should better encapsulate the context\n",
        "# of a word\n",
        "new_v = np.matmul(attention, v)\n",
        "new_v\n",
        "\n",
        "# v and new_v, because they are masked, the first row looks exactly the same\n",
        "# go to later words\n",
        "# notice how different these vectors actually become"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6otZx2YxqbkF",
        "outputId": "65e95ae4-a463-46a3-ac2e-ed66a3bf6b81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.81252582,  1.35624003, -0.07201012,  1.0035329 ,  0.36163603,\n",
              "        -0.64511975,  0.36139561,  1.53803657],\n",
              "       [ 0.66641301,  1.39213367, -0.51081002,  0.97225045,  0.31434319,\n",
              "        -0.58550834,  0.31495603,  0.93081668],\n",
              "       [ 0.52954961,  1.21756173, -0.14905901,  0.7267579 ,  0.1310981 ,\n",
              "        -0.57583252,  0.41805381,  0.87397953],\n",
              "       [ 0.01421368,  1.14907671, -0.99239485,  0.60451701, -0.14600018,\n",
              "        -0.40496816,  0.24215067, -0.82777073]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q, k.T) / np.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, v)\n",
        "  return out, attention"
      ],
      "metadata": {
        "id": "7fT5A4oMrapw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in the encoder, we don't need to pass in the mask\n",
        "# we're going to have new vectors\n",
        "# and the attention vectors that can actually pay attention to any word"
      ],
      "metadata": {
        "id": "Aw7eAsfprfoe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we only did this for a single attention head\n",
        "# we can have multiply attention heads and stack their results on \n",
        "# top of each other in order to get \n",
        "# multi-headed attention"
      ],
      "metadata": {
        "id": "iox0mFwjscfK"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}