{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xlorSbccWEDa",
        "outputId": "b030abe3-ec0c-4e95-e4bc-7173c8c225d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.137)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.4)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.9/dist-packages (1.7.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (0.3.3)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.47)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from pypdf) (4.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai pypdf faiss-cpu tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "from pypdf import PdfReader, PdfFileWriter, PdfFileReader\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import (\n",
        "    ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
        ")\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "nq0vKGFeW1KD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI API"
      ],
      "metadata": {
        "id": "klEbpXWxUS5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY\""
      ],
      "metadata": {
        "id": "yKaKB_GjWKjL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program"
      ],
      "metadata": {
        "id": "2WuqL_pQVwAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load PDF File as Text String"
      ],
      "metadata": {
        "id": "fK7J9Y1SZIXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_url = 'https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf'\n",
        "remote_file = urlopen(Request(pdf_url)).read()\n",
        "memory_file = io.BytesIO(remote_file)\n",
        "reader = PdfReader(memory_file)\n",
        "reader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddbQwQA9YC8w",
        "outputId": "ea340fa2-7d08-4035-a93f-dae28c38ab16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pypdf._reader.PdfReader at 0x7f022cc8c7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read data from the file and put them into a variable called raw_text\n",
        "raw_text = ''\n",
        "for i, page in enumerate(reader.pages):\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        raw_text += text"
      ],
      "metadata": {
        "id": "2VXlucKiW7bX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CQkqUBlzW-Xv",
        "outputId": "d95305b9-d59e-400a-87ab-3d55527796c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nY'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Splitting"
      ],
      "metadata": {
        "id": "gh98Kd1LY6IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(        \n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "text_ls = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "VdXzkpf9XAfP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_ls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozkNTiNuZ0TX",
        "outputId": "a7bea665-3d35-4e3b-95d8-6f0368120fa9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ls[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "1SqdR3wFZ3Ih",
        "outputId": "8f37fd4c-5c73-4940-ce7f-77a69063cd26"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ls[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "059PoKYUZ6dJ",
        "outputId": "3804df36-2b8d-4c98-b1b4-a5aeb3a3731c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "BeOr1XPeZFrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "embeddings"
      ],
      "metadata": {
        "id": "TcZUsQVyXBPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb61619-ced7-480e-977c-18a575c05881"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', document_model_name='text-embedding-ada-002', query_model_name='text-embedding-ada-002', embedding_ctx_length=8191, openai_api_key=None, openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Store"
      ],
      "metadata": {
        "id": "op60tSlSgUmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = FAISS.from_texts(text_ls, embeddings)\n",
        "docsearch"
      ],
      "metadata": {
        "id": "9C8py6wQXE5_",
        "outputId": "5c42df7c-fbe7-459e-d8df-8d7735abb543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain.vectorstores.faiss.FAISS at 0x7f01ff1d93a0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large Language Model"
      ],
      "metadata": {
        "id": "p3D2dz9Rgbva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name='text-davinci-003') # text-davinci-003\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZktWUBoQalNg",
        "outputId": "b1db1948-e8c1-4423-9b48-ec10b945746c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f021d6a7ca0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Your Chain"
      ],
      "metadata": {
        "id": "ZVeW7iPigeQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "chain"
      ],
      "metadata": {
        "id": "_L_Ywm-iXLhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c773b126-807c-4255-f4b8-46bf259f54f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StuffDocumentsChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f021d6a7ca0>, verbose=False, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f021d6a7ca0>, verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f021d6a7ca0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False), output_key='text'), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='context')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question and Answer"
      ],
      "metadata": {
        "id": "8vmVQVrye8cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who are the authors of the article?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "3mtAth2jXNKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "735c0146-0a23-48dc-e621-4ec240381e42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What was the cost of training the GPT4All model?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "RahXBIXjXO7X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e9489d3-9311-4329-9292-f40081a97ccb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' $800 in GPU costs (rented from Lambda Labs and Paperspace) including several failed trains, and $500 in OpenAI API spend.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How was the model trained?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "EzNcvjRJXSZ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "978ddf73-24a5-48be-e0d5-9ad292d59fc4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The model was trained with LoRA (Hu et al., 2021) on the 437,605 post-processed examples for four epochs. Detailed model hyper-parameters and training code can be found in the associated repository and model training log.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What was the size of the training dataset?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "Nhx-kpvAXUl3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5173c98a-5899-4599-fd28-2a1a645aec3c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The size of the final training dataset was 437,605 prompt-generation pairs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is this different from other models?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "kIg91Z0YXXCB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a18daaa0-7bec-4233-d80c-fefbf19ae600"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" GPT4All is based on LLaMA and has a non-commercial license. It is trained with LoRA and has data from OpenAI's GPT-3.5-Turbo. It is intended only for research purposes and any commercial use is prohibited.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Google Bard?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "D02sIID3XagO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b89af503-1ced-41d4-9279-028fc9ebf938"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "MvPbx3x5l3Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install session-info"
      ],
      "metadata": {
        "id": "qLynnMo0cj8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab36d3c-7927-42dd-c258-a12a9a0152a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.9/dist-packages (from session-info) (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import session_info\n",
        "\n",
        "session_info.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "969ozgBuxULg",
        "outputId": "e9578480-8b83-4916-97bc-54b6fb93cc4d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "langchain           0.0.137\n",
              "pypdf               3.7.1\n",
              "session_info        1.0.0\n",
              "-----\n",
              "</pre>\n",
              "<details>\n",
              "<summary>Click to view modules imported as dependencies</summary>\n",
              "<pre>\n",
              "PIL                         8.4.0\n",
              "aiohttp                     3.8.4\n",
              "aiosignal                   1.3.1\n",
              "async_timeout               4.0.2\n",
              "attr                        22.2.0\n",
              "backcall                    0.2.0\n",
              "certifi                     2022.12.07\n",
              "cffi                        1.15.1\n",
              "chardet                     4.0.0\n",
              "charset_normalizer          2.0.12\n",
              "cloudpickle                 2.2.1\n",
              "cycler                      0.10.0\n",
              "cython_runtime              NA\n",
              "dateutil                    2.8.2\n",
              "debugpy                     1.6.6\n",
              "decorator                   4.4.2\n",
              "defusedxml                  0.7.1\n",
              "faiss                       1.7.3\n",
              "frozenlist                  1.3.3\n",
              "google                      NA\n",
              "greenlet                    2.0.2\n",
              "httplib2                    0.21.0\n",
              "idna                        3.4\n",
              "importlib_resources         NA\n",
              "ipykernel                   5.5.6\n",
              "ipython_genutils            0.2.0\n",
              "kiwisolver                  1.4.4\n",
              "matplotlib                  3.7.1\n",
              "matplotlib_inline           0.1.6\n",
              "mpl_toolkits                NA\n",
              "multidict                   6.0.4\n",
              "numexpr                     2.8.4\n",
              "numpy                       1.22.4\n",
              "openai                      NA\n",
              "openapi_schema_pydantic     NA\n",
              "packaging                   23.0\n",
              "pandas                      1.5.3\n",
              "pexpect                     4.8.0\n",
              "pickleshare                 0.7.5\n",
              "pkg_resources               NA\n",
              "platformdirs                3.2.0\n",
              "portpicker                  NA\n",
              "prompt_toolkit              3.0.38\n",
              "psutil                      5.9.4\n",
              "ptyprocess                  0.7.0\n",
              "pyarrow                     9.0.0\n",
              "pydantic                    1.10.7\n",
              "pydev_ipython               NA\n",
              "pydevconsole                NA\n",
              "pydevd                      2.9.5\n",
              "pydevd_file_utils           NA\n",
              "pydevd_plugins              NA\n",
              "pydevd_tracing              NA\n",
              "pygments                    2.14.0\n",
              "pyparsing                   3.0.9\n",
              "pytz                        2022.7.1\n",
              "regex                       2.5.123\n",
              "requests                    2.27.1\n",
              "sitecustomize               NA\n",
              "six                         1.16.0\n",
              "socks                       1.7.1\n",
              "sphinxcontrib               NA\n",
              "sqlalchemy                  1.4.47\n",
              "storemagic                  NA\n",
              "swig_runtime_data4          NA\n",
              "tenacity                    NA\n",
              "tiktoken                    NA\n",
              "tiktoken_ext                NA\n",
              "tornado                     6.2\n",
              "traitlets                   5.7.1\n",
              "typing_extensions           NA\n",
              "urllib3                     1.26.15\n",
              "wcwidth                     0.2.6\n",
              "yaml                        6.0\n",
              "yarl                        1.8.2\n",
              "zipp                        NA\n",
              "zmq                         23.2.1\n",
              "zoneinfo                    NA\n",
              "</pre>\n",
              "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
              "<pre>\n",
              "-----\n",
              "IPython             7.34.0\n",
              "jupyter_client      6.1.12\n",
              "jupyter_core        5.3.0\n",
              "notebook            6.4.8\n",
              "-----\n",
              "Python 3.9.16 (main, Dec  7 2022, 01:11:51) [GCC 9.4.0]\n",
              "Linux-5.10.147+-x86_64-with-glibc2.31\n",
              "-----\n",
              "Session information updated at 2023-04-12 08:06\n",
              "</pre>\n",
              "</details>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}