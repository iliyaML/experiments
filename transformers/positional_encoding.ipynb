{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- https://www.youtube.com/watch?v=ZMxVe-HK174"
      ],
      "metadata": {
        "id": "HJDbfKMaE7DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# max number of words that can be passed into the transformer\n",
        "# in reality this would be in the the thousands\n",
        "max_sequence_length = 10\n",
        "\n",
        "# dimension of the embeddings (typically 512)\n",
        "d_model = 6"
      ],
      "metadata": {
        "id": "eSggQhtnE-Ut"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "PE(\\text{position}, 2i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE(\\text{position}, 2i+1) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
        "$$\n",
        "\n",
        "We can rewrite these as\n",
        "\n",
        "$$\n",
        "PE(\\text{position}, i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{i}{d_{model}}} \\bigg) \\text{ when i is even}\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE(\\text{position}, i) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{i-1}{d_{model}}} \\bigg) \\text{ when i is odd}\n",
        "$$"
      ],
      "metadata": {
        "id": "9XdyM5DJCZF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49l0Drr6EgA8",
        "outputId": "656a6817-4376-4934-8474-d21116c28b96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# get a set of values between 0 and D model, incrementing by 2\n",
        "even_i = torch.arange(0, d_model, 2).float()\n",
        "even_i"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# user defined scalar\n",
        "n = 10000\n",
        "\n",
        "even_denominator = torch.pow(n, even_i / d_model)\n",
        "even_denominator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiKKF4GpFi2q",
        "outputId": "ce3c3364-426c-4791-cd23-c88fc8a54133"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  1.0000,  21.5443, 464.1590])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odd_i = torch.arange(1, d_model, 2).float()\n",
        "odd_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDrzwGRVFnZz",
        "outputId": "be6534a7-049d-4cd7-9f91-636da2d3b206"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 3., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odd_denominator = torch.pow(10000, (odd_i - 1)/d_model)\n",
        "odd_denominator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhm5iqgEFpFM",
        "outputId": "90eb2dab-f1d6-48a4-9f7f-fba0aa1cf959"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  1.0000,  21.5443, 464.1590])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "vector that we got for the even denominator and the\n",
        "vector that we got for the odd denominator are exactly the same\n",
        "\n",
        "you'll notice that the odd indices are one more than the even\n",
        "indices and in the formulation we always subtract one from the odd indices so\n",
        "they effectively just became the same thing\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GqcPAJfNHwDN",
        "outputId": "db2be48d-1204-472b-aab1-781765b5dad0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nvector that we got for the even denominator and the\\nvector that we got for the odd denominator are exactly the same\\n\\nyou'll notice that the odd indices are one more than the even\\nindices and in the formulation we always subtract one from the odd indices so\\nthey effectively just became the same thing\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just gonna use one denominator\n",
        "denominator = even_denominator"
      ],
      "metadata": {
        "id": "MMZDPpLVFqG0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "let's just determine every single position for the sequence \n",
        "\n",
        "we can define every position by just taking all the values from 1 to 10 and \n",
        "\n",
        "then we'll reshape it to be a two-dimensional Matrix with \n",
        "the second dimension as one and you'll get this two-dimensional Matrix \n",
        "here \n",
        "\n",
        "one for every word\n",
        "\"\"\"\n",
        "position = torch.arange(\n",
        "              max_sequence_length, \n",
        "              dtype=torch.float\n",
        "          ).reshape(max_sequence_length, 1)\n",
        "position"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYcD0iRAFseJ",
        "outputId": "8fe90151-9d6c-471c-86ed-4351335fe049"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [6.],\n",
              "        [7.],\n",
              "        [8.],\n",
              "        [9.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for even cases we're going to take sine and \n",
        "for odd instances we're going to take the cosine\n",
        "\"\"\"\n",
        "even_PE = torch.sin(position / denominator)\n",
        "odd_PE = torch.cos(position / denominator)\n",
        "\n",
        "even_PE.shape, even_PE, "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxPATmyfF6RH",
        "outputId": "6335ce5b-ce36-46ca-c791-3efde9a9f4f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 3]), tensor([[ 0.0000,  0.0000,  0.0000],\n",
              "         [ 0.8415,  0.0464,  0.0022],\n",
              "         [ 0.9093,  0.0927,  0.0043],\n",
              "         [ 0.1411,  0.1388,  0.0065],\n",
              "         [-0.7568,  0.1846,  0.0086],\n",
              "         [-0.9589,  0.2300,  0.0108],\n",
              "         [-0.2794,  0.2749,  0.0129],\n",
              "         [ 0.6570,  0.3192,  0.0151],\n",
              "         [ 0.9894,  0.3629,  0.0172],\n",
              "         [ 0.4121,  0.4057,  0.0194]]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "what we want though is to interleave these two matrices (even_PE, odd_PE)\n",
        "\n",
        "so for example for the even position we want this to be the first\n",
        "index and then we want this to be the second index\n",
        "and then the third index and then the fourth index \n",
        "\n",
        "but starting at zero so it'll be the zeroth index first index\n",
        "second index third index and \n",
        "\n",
        "so on in order to do that I basically stack them together \n",
        "well on the second dimension\n",
        "\n",
        "so that the two that we need to stack on top of each other are right next \n",
        "to each other this will give us a 10 x 3 x 2 tensor\n",
        "\"\"\"\n",
        "stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "stacked.shape, stacked "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl9E5FVkGfxB",
        "outputId": "3ede4f40-a213-4914-d846-050721666477"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 3, 2]), tensor([[[ 0.0000,  1.0000],\n",
              "          [ 0.0000,  1.0000],\n",
              "          [ 0.0000,  1.0000]],\n",
              " \n",
              "         [[ 0.8415,  0.5403],\n",
              "          [ 0.0464,  0.9989],\n",
              "          [ 0.0022,  1.0000]],\n",
              " \n",
              "         [[ 0.9093, -0.4161],\n",
              "          [ 0.0927,  0.9957],\n",
              "          [ 0.0043,  1.0000]],\n",
              " \n",
              "         [[ 0.1411, -0.9900],\n",
              "          [ 0.1388,  0.9903],\n",
              "          [ 0.0065,  1.0000]],\n",
              " \n",
              "         [[-0.7568, -0.6536],\n",
              "          [ 0.1846,  0.9828],\n",
              "          [ 0.0086,  1.0000]],\n",
              " \n",
              "         [[-0.9589,  0.2837],\n",
              "          [ 0.2300,  0.9732],\n",
              "          [ 0.0108,  0.9999]],\n",
              " \n",
              "         [[-0.2794,  0.9602],\n",
              "          [ 0.2749,  0.9615],\n",
              "          [ 0.0129,  0.9999]],\n",
              " \n",
              "         [[ 0.6570,  0.7539],\n",
              "          [ 0.3192,  0.9477],\n",
              "          [ 0.0151,  0.9999]],\n",
              " \n",
              "         [[ 0.9894, -0.1455],\n",
              "          [ 0.3629,  0.9318],\n",
              "          [ 0.0172,  0.9999]],\n",
              " \n",
              "         [[ 0.4121, -0.9111],\n",
              "          [ 0.4057,  0.9140],\n",
              "          [ 0.0194,  0.9998]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "we just flatten it and effectively we're going to be \n",
        "getting that interleavement here too\n",
        "\"\"\"\n",
        "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "\n",
        "\"\"\"\n",
        "so for our first word this will be the positional encoding (first row)\n",
        "for the second word it's this one (second row)\n",
        "for the third word it's here (third row) and so on\n",
        "\"\"\"\n",
        "PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lCVbJN7HFUi",
        "outputId": "4eb23de1-0547-47a7-b777-d866830b157d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class"
      ],
      "metadata": {
        "id": "qTFRYTaqHK0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE"
      ],
      "metadata": {
        "id": "MBkHiblcHIij"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(d_model=6, max_sequence_length=10)\n",
        "pe.forward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa5C1D9jHM9c",
        "outputId": "0ed5f8da-92f5-4900-86a2-e13d49a986b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}