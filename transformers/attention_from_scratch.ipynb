{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- https://machinelearningmastery.com/the-attention-mechanism-from-scratch/"
      ],
      "metadata": {
        "id": "iV36gfNES65y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "J3RYLRRdRjMn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "# for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder representations of four different words\n",
        "word_1 = np.array([1, 0, 0])\n",
        "word_2 = np.array([0, 1, 0])\n",
        "word_3 = np.array([1, 1, 0])\n",
        "word_4 = np.array([0, 0, 1])\n",
        "\n",
        "word_1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRIu5b3bRk61",
        "outputId": "c89c6a21-a0ef-41a4-e827-991eafb984db"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stacking the word embeddings into a single array\n",
        "words = np.array([word_1, word_2, word_3, word_4])\n",
        "\n",
        "# seq_length (num of words = 4) x embedding dimension (3)\n",
        "words.shape, words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOxO3b-vXDDG",
        "outputId": "ba3e05ed-0709-4c49-a32d-ca7e9cb5b8dd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 3), array([[1, 0, 0],\n",
              "        [0, 1, 0],\n",
              "        [1, 1, 0],\n",
              "        [0, 0, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the weight matrices\n",
        "W_Q = np.random.randint(3, size=(3, 3))\n",
        "W_K = np.random.randint(3, size=(3, 3))\n",
        "W_V = np.random.randint(3, size=(3, 3))\n",
        "\n",
        "W_Q.shape, W_Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_obKhOiVRqwe",
        "outputId": "bce0d77c-bd01-4fee-8012-83d6c3548624"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3, 3), array([[2, 0, 2],\n",
              "        [2, 0, 0],\n",
              "        [2, 1, 2]]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the queries, keys and values\n",
        "query_1 = word_1 @ W_Q # 1x3 @ 3x3 = 1x3\n",
        "key_1 = word_1 @ W_K\n",
        "value_1 = word_1 @ W_V\n",
        " \n",
        "query_2 = word_2 @ W_Q\n",
        "key_2 = word_2 @ W_K\n",
        "value_2 = word_2 @ W_V\n",
        " \n",
        "query_3 = word_3 @ W_Q\n",
        "key_3 = word_3 @ W_K\n",
        "value_3 = word_3 @ W_V\n",
        " \n",
        "query_4 = word_4 @ W_Q\n",
        "key_4 = word_4 @ W_K\n",
        "value_4 = word_4 @ W_V\n",
        "\n",
        "print(query_1.shape, key_1.shape, value_1.shape)\n",
        "\n",
        "Q = np.array([query_1, query_2, query_3, query_4])\n",
        "K = np.array([key_1, key_2, key_3, key_4])\n",
        "V = np.array([value_1, value_2, value_3, value_4])\n",
        "\n",
        "Q.shape, K.shape, V.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi8XtyL_RxXh",
        "outputId": "6c525928-e370-45f0-e73a-18edd99a59dd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,) (3,) (3,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 3), (4, 3), (4, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix version of above\n",
        "# generating the queries, keys and values\n",
        "Q = words @ W_Q\n",
        "K = words @ W_K\n",
        "V = words @ W_V\n",
        "\n",
        "print('Q', Q.shape, Q)\n",
        "print('K', K.shape, K)\n",
        "print('V', V.shape, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFsrzEioVWyl",
        "outputId": "303f9e6b-6a03-4578-9f11-03eac00e3b74"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q (4, 3) [[2 0 2]\n",
            " [2 0 0]\n",
            " [4 0 2]\n",
            " [2 1 2]]\n",
            "K (4, 3) [[2 2 2]\n",
            " [0 2 1]\n",
            " [2 4 3]\n",
            " [0 1 1]]\n",
            "V (4, 3) [[1 1 0]\n",
            " [0 1 1]\n",
            " [1 2 1]\n",
            " [0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scoring the first query vector against all key vectors\n",
        "scores = np.array([\n",
        "    np.dot(query_1, key_1), \n",
        "    np.dot(query_1, key_2), \n",
        "    np.dot(query_1, key_3), \n",
        "    np.dot(query_1, key_4)\n",
        "])\n",
        "scores.shape, scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7noIUW9MR2A7",
        "outputId": "5e47bec1-a1e6-42cc-df3c-ffb3ca6b30bf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4,), array([ 8,  2, 10,  2]))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the weights by a softmax operation\n",
        "weights = scipy.special.softmax(scores / key_1.shape[0] ** 0.5)\n",
        "weights.shape, weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCbeFDgXR3ha",
        "outputId": "b68495e8-29a1-4d3f-91c8-1f657d78b362"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4,), array([0.23608986, 0.00738988, 0.74913039, 0.00738988]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the attention by a weighted sum of the value vectors\n",
        "attention = (weights[0] * value_1) + (weights[1] * value_2) + (weights[2] * value_3) + (weights[3] * value_4)\n",
        "attention.shape, attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5WGP9UISH_s",
        "outputId": "171cc74b-c4c2-4793-d8a6-d8ce123f9460"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3,), array([0.98522025, 1.74174051, 0.75652026]))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# equivalent to above\n",
        "attention = weights @ np.array([value_1, value_2, value_3, value_4])\n",
        "attention.shape, attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4xGyK25R9tM",
        "outputId": "03ac2d3d-9270-42f6-e7aa-f9a99d7deca9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3,), array([0.98522025, 1.74174051, 0.75652026]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q.shape, Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk9xN8P7b-kF",
        "outputId": "0e2a30f8-2474-4fe4-e32f-514617ad44d2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 3), array([[2, 0, 2],\n",
              "        [2, 0, 0],\n",
              "        [4, 0, 2],\n",
              "        [2, 1, 2]]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K.shape, K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaX9pYyMcAol",
        "outputId": "52c63379-3304-4e44-e87a-82e3afd722f9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 3), array([[2, 2, 2],\n",
              "        [0, 2, 1],\n",
              "        [2, 4, 3],\n",
              "        [0, 1, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K.transpose().shape, K.transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZv82yzzcQZn",
        "outputId": "3bea80af-90b9-4c39-9b6b-69cb7bfb6044"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3, 4), array([[2, 0, 2, 0],\n",
              "        [2, 2, 4, 1],\n",
              "        [2, 1, 3, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scoring the query vectors against all key vectors\n",
        "scores = Q @ K.transpose()\n",
        "scores.shape, scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtPo9gx3X6Cm",
        "outputId": "348f110f-1819-406c-bc11-8ee13b5b1b5b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 4), array([[ 8,  2, 10,  2],\n",
              "        [ 4,  0,  4,  0],\n",
              "        [12,  2, 14,  2],\n",
              "        [10,  4, 14,  3]]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the weights by a softmax operation\n",
        "weights = scipy.special.softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
        "weights.shape, weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAVOI-0MYmJQ",
        "outputId": "558b2d4f-3571-4dde-94df-31e2aa8ad2a5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 4),\n",
              " array([[2.36089863e-01, 7.38987555e-03, 7.49130386e-01, 7.38987555e-03],\n",
              "        [4.54826323e-01, 4.51736775e-02, 4.54826323e-01, 4.51736775e-02],\n",
              "        [2.39275049e-01, 7.43870015e-04, 7.59237211e-01, 7.43870015e-04],\n",
              "        [8.99501754e-02, 2.81554063e-03, 9.05653685e-01, 1.58059922e-03]]))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the attention by a weighted sum of the value vectors\n",
        "attention = weights @ V\n",
        "attention.shape, attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrLWKLBlYpuK",
        "outputId": "7dc9492a-c81f-45b7-b3dc-567395e771cd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 3), array([[0.98522025, 1.74174051, 0.75652026],\n",
              "        [0.90965265, 1.40965265, 0.5       ],\n",
              "        [0.99851226, 1.75849334, 0.75998108],\n",
              "        [0.99560386, 1.90407309, 0.90846923]]))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "bO8-eRn7S5F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        " \n",
        "# for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# encoder representations of four different words\n",
        "word_1 = np.array([1, 0, 0])\n",
        "word_2 = np.array([0, 1, 0])\n",
        "word_3 = np.array([1, 1, 0])\n",
        "word_4 = np.array([0, 0, 1])\n",
        " \n",
        "# stacking the word embeddings into a single array\n",
        "words = np.array([word_1, word_2, word_3, word_4])\n",
        " \n",
        "# generating the weight matrices\n",
        "W_Q = np.random.randint(3, size=(3, 3))\n",
        "W_K = np.random.randint(3, size=(3, 3))\n",
        "W_V = np.random.randint(3, size=(3, 3))\n",
        " \n",
        "# generating the queries, keys and values\n",
        "Q = words @ W_Q\n",
        "K = words @ W_K\n",
        "V = words @ W_V\n",
        " \n",
        "# scoring the query vectors against all key vectors\n",
        "scores = Q @ K.transpose()\n",
        " \n",
        "# computing the weights by a softmax operation\n",
        "weights = scipy.special.softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
        " \n",
        "# computing the attention by a weighted sum of the value vectors\n",
        "attention = weights @ V\n",
        " \n",
        "print(attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw5gfQfhTqJu",
        "outputId": "351d22cc-56ce-4735-fa79-19ad9a05c5ab"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.98522025 1.74174051 0.75652026]\n",
            " [0.90965265 1.40965265 0.5       ]\n",
            " [0.99851226 1.75849334 0.75998108]\n",
            " [0.99560386 1.90407309 0.90846923]]\n"
          ]
        }
      ]
    }
  ]
}