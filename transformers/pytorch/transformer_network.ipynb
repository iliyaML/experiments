{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9XL-FumL62"
      },
      "source": [
        "References:\n",
        "- https://www.youtube.com/watch?v=Nw_PJdmydZY\n",
        "- https://www.youtube.com/watch?v=Xg5JG30bYik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nh2InzX8mJem"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IlQYhrO015x",
        "outputId": "d908ebbb-ee6d-414a-b0d7-8ab2b4f278ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy version: 1.22.4\n",
            "PyTorch version: 2.0.0+cu118\n"
          ]
        }
      ],
      "source": [
        "print('NumPy version:', np.__version__)\n",
        "print('PyTorch version:', torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3iAKR2Ojm7uz"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT-ccv3U0htZ"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHLEYNuEyB5O"
      },
      "source": [
        "### Scaled-Dot Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RqurU2YxmfZk"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "        scaled = scaled.permute(1, 0, 2, 3)\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6-tqO9zyExg"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PbpHbTkIm1dD"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = (torch.arange(self.max_sequence_length)\n",
        "                          .reshape(self.max_sequence_length, 1))\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACYP2xnuybaD"
      },
      "source": [
        "### Sentence Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eY4o4mMsm4EO"
      },
      "outputs": [],
      "source": [
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, \n",
        "                 START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, \n",
        "                                                   max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "    \n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indicies = [self.language_to_index[token] \n",
        "                                      for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, \n",
        "                                              self.language_to_index[\n",
        "                                                  self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(\n",
        "                    self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indicies), \n",
        "                           self.max_sequence_length):\n",
        "                sentence_word_indicies.append(\n",
        "                    self.language_to_index[self.PADDING_TOKEN])\n",
        "            return torch.tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], \n",
        "                                      start_token, \n",
        "                                      end_token) )\n",
        "        tokenized = torch.stack(tokenized)\n",
        "        return tokenized.to(get_device())\n",
        "    \n",
        "    def forward(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder().to(get_device())\n",
        "        x = self.dropout(x + pos)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkUh6CryydXa"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qNVj9PcGm6Qr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        batch_size, sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, \n",
        "                          self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.reshape(batch_size, sequence_length,\n",
        "                                self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx03iD8oyitz"
      },
      "source": [
        "### Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SOMogkwpm6aX"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REY3U2-nymG2"
      },
      "source": [
        "### Position-wise Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JLg7avGCm-j4"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR4z1gveyueb"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uClhKwmEnAsI"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, \n",
        "                                            num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, \n",
        "                                           drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x.clone()\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6c8kfhdyv37"
      },
      "source": [
        "### Sequential Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GcYZ9FzvnB23"
      },
      "outputs": [],
      "source": [
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK-jUnJByxoz"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tDtTZbKPnDx-"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 d_model, \n",
        "                 ffn_hidden, \n",
        "                 num_heads, \n",
        "                 drop_prob, \n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN, \n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, \n",
        "                                                    d_model, language_to_index, \n",
        "                                                    START_TOKEN, END_TOKEN, \n",
        "                                                    PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, \n",
        "                                                       num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
        "        '''\n",
        "        x: batch of english sentences\n",
        "        '''\n",
        "\n",
        "        # construct the sentence embeddings\n",
        "        # take every single sentence, we're going to use the character\n",
        "        # we have it map to integer values for every single sentence\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "\n",
        "        # pass it to the encoder\n",
        "        # self_attention_mask is just the padding mask because in the English\n",
        "        # sentence we are allowed to look forward and backwards\n",
        "        # don't need the look ahead mask here\n",
        "        x = self.layers(x, self_attention_mask)\n",
        "\n",
        "        # going to get a list of contextually aware characters in the form of\n",
        "        # vectors\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCCZZ45HyzS1"
      },
      "source": [
        "### Multi-Head Cross Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0ykpkfAOnGQh"
      },
      "outputs": [],
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model , d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = x.size() \n",
        "        # in practice, this is the same for both languages...\n",
        "        # so we can technically combine with normal attention\n",
        "        \n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, \n",
        "                        2 * self.head_dim)\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, \n",
        "                      self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) \n",
        "        # We don't need the mask for cross attention, \n",
        "        # removing in outer function!\n",
        "        \n",
        "        values = values.reshape(batch_size, sequence_length, d_model)\n",
        "        out = self.linear_layer(values)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34geEyY0yMWS"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e7BkWMeenIwy"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, \n",
        "                                                 num_heads=num_heads)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(\n",
        "                                          d_model=d_model, \n",
        "                                          num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, \n",
        "                                           drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y.clone()\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEdSKs22y1ss"
      },
      "source": [
        "### Sequential Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b3iuGy9MnMSl"
      },
      "outputs": [],
      "source": [
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMn4Jf3fy3pw"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XT4RYb4inM-t"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 d_model, \n",
        "                 ffn_hidden, \n",
        "                 num_heads, \n",
        "                 drop_prob, \n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN, \n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, \n",
        "                                                    d_model, \n",
        "                                                    language_to_index, \n",
        "                                                    START_TOKEN, \n",
        "                                                    END_TOKEN, \n",
        "                                                    PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, \n",
        "                                                       ffn_hidden, \n",
        "                                                       num_heads, \n",
        "                                                       drop_prob\n",
        "                                        ) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask, \n",
        "                start_token, end_token):\n",
        "        '''\n",
        "        x: batch of vectors x\n",
        "        y: batch of targeted sentences\n",
        "        '''\n",
        "\n",
        "        # perform sentence embeddings\n",
        "        # start of Decoder will include the start_token\n",
        "        # end of Decoder will include the end_token\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "\n",
        "        # pass to Decoder layers\n",
        "        # pass the self_attention_mask and cross_attention_mask\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "\n",
        "        # output is a set of Kannada character vectors\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afugCtX4yHiH"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ch5Tp0d9mAwV"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, \n",
        "                d_model, # dimensions for every single character vector\n",
        "                         # typically take it as 512\n",
        "                ffn_hidden, # hidden layers for feed forward network\n",
        "                            # set at 2048\n",
        "                num_heads, # number of heads in Multi-Head Attention as well as \n",
        "                            # Multi-Head Cross Attention\n",
        "                drop_prob, # Dropout as the probability we will switch of\n",
        "                          # neurons to promote generalization\n",
        "                          # typically set at 0.1\n",
        "                num_layers, # number of Encoder and Decoder layers\n",
        "                            # can set at 5\n",
        "                max_sequence_length, # maximum number of characters in a given\n",
        "                                  # input sentence which we set it to about 200\n",
        "                kn_vocab_size, # all the possible characters that can occur in\n",
        "                              # Kannada sentence or translation\n",
        "                english_to_index, # dictionary that maps a character to a unique\n",
        "                                  # number\n",
        "                kannada_to_index,\n",
        "                 # Start, End and Padding tokens respectively\n",
        "                START_TOKEN, \n",
        "                END_TOKEN, \n",
        "                PADDING_TOKEN\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, \n",
        "                               num_layers, max_sequence_length, \n",
        "                               english_to_index, START_TOKEN, END_TOKEN, \n",
        "                               PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, \n",
        "                               num_layers, max_sequence_length, \n",
        "                               kannada_to_index, START_TOKEN, END_TOKEN, \n",
        "                               PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, kn_vocab_size)\n",
        "        self.device = get_device()\n",
        "  \n",
        "    def forward(self, \n",
        "                x, # batch of English snetences\n",
        "                y, # batch of target Kannada sentences\n",
        "\n",
        "                # Masks\n",
        "                encoder_self_attention_mask=None, \n",
        "                # incorporate the padding mask\n",
        "                \n",
        "                decoder_self_attention_mask=None, \n",
        "                # incorporate both padding and look ahead mask\n",
        "                \n",
        "                decoder_cross_attention_mask=None, \n",
        "                # also incorporate the padding mask\n",
        "                \n",
        "                \n",
        "                # these four other parameters are\n",
        "                # going to determine whether we should\n",
        "                # include a start token and an end token\n",
        "                # to our encoder and decoder inputs and\n",
        "                # outputs in this case false means that we\n",
        "                # do not include it and true means that we do\n",
        "                \n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        '''\n",
        "        Transformer call to the forward pass\n",
        "        '''\n",
        "\n",
        "        # take the batch of sentences x and pass it through the Encoder\n",
        "        # get the list of character vectors that are eventually going to be \n",
        "        # context aware\n",
        "        x = self.encoder(x, encoder_self_attention_mask, \n",
        "                         start_token=enc_start_token, \n",
        "                         end_token=enc_end_token)\n",
        "        \n",
        "        # take those vectors and pass it to the Decoder along with the \n",
        "        # targeted batch Kannada sentences y\n",
        "        # get the output of the decoder\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, \n",
        "                           decoder_cross_attention_mask, \n",
        "                           start_token=dec_start_token, \n",
        "                           end_token=dec_end_token)\n",
        "        \n",
        "        \n",
        "        out = self.linear(out)\n",
        "\n",
        "        # evetually going to pass it to a softmax activation \n",
        "        # when we're computing the loss function\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlghUy4Dy57Q"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8_pZLAgmDBL",
        "outputId": "cb5168bb-38dc-47a0-b4f6-ce78d0acf67a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting session-info\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m972.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8042 sha256=e6f8d64da0a887802a711689b7f9d0ccc4a6bf443a2d071ba7f3e8e34521cc7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, session-info\n",
            "Successfully installed session-info-1.0.0 stdlib_list-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install session-info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cNwHLvQOmFG1",
        "outputId": "0d34bfa8-e518-48b3-ed1b-7ca92353d69c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "numpy               1.22.4\n",
              "session_info        1.0.0\n",
              "torch               2.0.0+cu118\n",
              "-----\n",
              "</pre>\n",
              "<details>\n",
              "<summary>Click to view modules imported as dependencies</summary>\n",
              "<pre>\n",
              "PIL                 8.4.0\n",
              "astunparse          1.6.3\n",
              "backcall            0.2.0\n",
              "certifi             2022.12.07\n",
              "cffi                1.15.1\n",
              "cycler              0.10.0\n",
              "cython_runtime      NA\n",
              "dateutil            2.8.2\n",
              "debugpy             1.6.6\n",
              "decorator           4.4.2\n",
              "defusedxml          0.7.1\n",
              "dot_parser          NA\n",
              "google              NA\n",
              "httplib2            0.21.0\n",
              "ipykernel           5.5.6\n",
              "ipython_genutils    0.2.0\n",
              "kiwisolver          1.4.4\n",
              "matplotlib          3.7.1\n",
              "matplotlib_inline   0.1.6\n",
              "mpl_toolkits        NA\n",
              "mpmath              1.3.0\n",
              "nvfuser             NA\n",
              "opt_einsum          v3.3.0\n",
              "packaging           23.1\n",
              "pexpect             4.8.0\n",
              "pickleshare         0.7.5\n",
              "pkg_resources       NA\n",
              "platformdirs        3.3.0\n",
              "portpicker          NA\n",
              "prompt_toolkit      3.0.38\n",
              "psutil              5.9.5\n",
              "ptyprocess          0.7.0\n",
              "pydev_ipython       NA\n",
              "pydevconsole        NA\n",
              "pydevd              2.9.5\n",
              "pydevd_file_utils   NA\n",
              "pydevd_plugins      NA\n",
              "pydevd_tracing      NA\n",
              "pydot               1.4.2\n",
              "pygments            2.14.0\n",
              "pyparsing           3.0.9\n",
              "sitecustomize       NA\n",
              "six                 1.16.0\n",
              "socks               1.7.1\n",
              "sphinxcontrib       NA\n",
              "storemagic          NA\n",
              "sympy               1.11.1\n",
              "tornado             6.2\n",
              "tqdm                4.65.0\n",
              "traitlets           5.7.1\n",
              "typing_extensions   NA\n",
              "wcwidth             0.2.6\n",
              "zmq                 23.2.1\n",
              "</pre>\n",
              "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
              "<pre>\n",
              "-----\n",
              "IPython             7.34.0\n",
              "jupyter_client      6.1.12\n",
              "jupyter_core        5.3.0\n",
              "notebook            6.4.8\n",
              "-----\n",
              "Python 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
              "Linux-5.10.147+-x86_64-with-glibc2.31\n",
              "-----\n",
              "Session information updated at 2023-04-29 10:39\n",
              "</pre>\n",
              "</details>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import session_info\n",
        "\n",
        "session_info.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "01fd550304b9e7cfb3e25c8e3ca63c0030fb0173de6652dd3d36373edc7c122f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
