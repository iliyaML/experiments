{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- https://www.youtube.com/watch?v=g3sEsBGkLU0"
      ],
      "metadata": {
        "id": "d_bXDGFWnH0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "UdTblxTF_byq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mYbIgWZ3_JXg"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    print(f\"scaled.size() : {scaled.size()}\")\n",
        "    if mask is not None:\n",
        "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
        "        # Broadcasting add. So just the last N dimensions need to match\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, max_sequence_length, d_model = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.reshape(batch_size, max_sequence_length, \n",
        "                          self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"\"\"values.size(): {values.size()}, \n",
        "        attention.size:{ attention.size()} \"\"\")\n",
        "        values = values.reshape(batch_size, max_sequence_length, \n",
        "                                self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        print(f\"Mean ({mean.size()})\")\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        print(f\"Standard Deviation  ({std.size()})\")\n",
        "        y = (inputs - mean) / std\n",
        "        print(f\"y: {y.size()}\")\n",
        "        out = self.gamma * y  + self.beta\n",
        "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
        "        print(f\"out: {out.size()}\")\n",
        "        return out\n",
        "\n",
        "  \n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        print(f\"x after first linear layer: {x.size()}\")\n",
        "        x = self.relu(x)\n",
        "        print(f\"x after activation: {x.size()}\")\n",
        "        x = self.dropout(x)\n",
        "        print(f\"x after dropout: {x.size()}\")\n",
        "        x = self.linear2(x)\n",
        "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, \n",
        "                                            num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, \n",
        "                                           hidden=ffn_hidden, \n",
        "                                           drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual_x = x\n",
        "        print(\"------- ATTENTION 1 ------\")\n",
        "        x = self.attention(x, mask=None)\n",
        "        print(\"------- DROPOUT 1 ------\")\n",
        "        x = self.dropout1(x)\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        print(\"------- ATTENTION 2 ------\")\n",
        "        x = self.ffn(x)\n",
        "        print(\"------- DROPOUT 2 ------\")\n",
        "        x = self.dropout2(x)\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        # this sequential unit is essentially going to take \n",
        "        # all the comma separated values here and \n",
        "        # it's going to execute them in the forward pass \n",
        "        # one at a time one after the other \n",
        "\n",
        "        # in this example, we have a sequence of five encoder layer objects\n",
        "        self.layers = nn.Sequential(*[EncoderLayer(d_model, \n",
        "                                                   ffn_hidden, \n",
        "                                                   num_heads, \n",
        "                                                   drop_prob)\n",
        "                                     for _ in range(num_layers)])\n",
        "\n",
        "    # override the default forward method from Module\n",
        "    def forward(self, x):\n",
        "\n",
        "        # pass the input to our layers\n",
        "        # essentially it's passing it through all of the five encoder layers\n",
        "        x = self.layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512 # the size of every single vector throughout the encoder\n",
        "'''\n",
        "every vector that you'll see kind of throughout the encoder architecture \n",
        "will be somewhat related to 512 dimensions\n",
        "and this is the reason why I've just defined it as a parameter here\n",
        "'''\n",
        "\n",
        "num_heads = 8 # used in the concept of multi-headed attention\n",
        "'''\n",
        "in the Transformer neural network architecture \n",
        "we see these multi-headed attention units well essentially when\n",
        "we're performing the concept of attention we are actually going \n",
        "to perform it eight times in parallel\n",
        "\n",
        "you can see consider this as the number of parallelized operations that \n",
        "we perform within the encoder\n",
        "'''\n",
        "\n",
        "drop_prob = 0.1 # a Dropout\n",
        "'''\n",
        "where we're going to randomly turn off certain neurons and what this\n",
        "allows the neural network to do is it forces it to learn along different paths\n",
        "over here and thus make weight updates accordingly this will help \n",
        "the neural network be better able to generalize data instead of \n",
        "accidentally memorizing specific data\n",
        "and effectively this acts as a regularizer \n",
        "if you've heard the concept of regularization for neural networks\n",
        "and it's pretty useful when it comes to very deep networks with a lot of\n",
        "connections and parameters now I've set this probability to 0.1 which basically\n",
        "means that there is a 10% chance that a given neuron will be turned off on a\n",
        "given stage \n",
        "\n",
        "we can adjust this value to be anywhere between zero and one\n",
        "'''\n",
        "\n",
        "batch_size = 30\n",
        "'''\n",
        "when we're dealing with neural networks and we want to perform some\n",
        "training typically we would pass in multiple examples at the same time those\n",
        "multiple examples constitute a batch\n",
        "\n",
        "there's actually a couple of reasons why I would do this so first of all it's\n",
        "faster training and second it's also more stable training\n",
        "\n",
        "typically a good middle ground for many of these machine learning \n",
        "and deep learning problems is to use mini batches so do somewhere \n",
        "along the Middle where we will batch some some arbitrary number of examples\n",
        "together in order to just learn effectively quickly and also in a way\n",
        "that's more stable so that you every time that you know you'll \n",
        "see examples you will see that the loss on average\n",
        "will always decrease\n",
        "\n",
        "we're going to be it's saying that we are going to look at 30 examples of \n",
        "let's say 30 sentences in some English\n",
        "language and it's only then that it's going to propagate \n",
        "through the entire network\n",
        "that is go through the encoder and the decoder then we have \n",
        "a loss function which is going to be computed and then\n",
        "the gradients are going to be calculated in the reverse Direction and \n",
        "will have gradient updates all the parameters will\n",
        "be updated only after seeing 30 examples and so we have like a mini batch\n",
        "gradient descent that we're going to be performing\n",
        "'''\n",
        "\n",
        "max_sequence_length = 200\n",
        "'''\n",
        "the largest number of words that we can be passing in at a time through the\n",
        "encoder \n",
        "\n",
        "in reality this is always going to be the number of words that we\n",
        "pass in through the encoder\n",
        "\n",
        "we're going to be also passing a lot of padding tokens here and \n",
        "this padding token will be\n",
        "let's say you know if there's the maximum sequence length is \n",
        "like 200 words and the sentence is only four\n",
        "words and there'll be 196 padding tokens here that are just\n",
        "going to be passed in and \n",
        "\n",
        "this is always going to be the case where we'll have a\n",
        "sentence and then we will add some padding tokens such that \n",
        "the maximum sequence length is achieved \n",
        "\n",
        "so there's always a fixed lengths input for any kind of sentence \n",
        "that we decide to input to our Transformer encoder \n",
        "'''\n",
        "\n",
        "ffn_hidden = 2048\n",
        "'''\n",
        "Feed Forward Network\n",
        "While most of the cases like throughout\n",
        "the entire encoder you can even like through almost all the entire decoder \n",
        "we will have 512 dimensional vectors like I\n",
        "mentioned before it's only at this step over here that I'm going to be expanding\n",
        "the number of neurons at some point to be 2048 before making it eventually back\n",
        "down to 512. and this is simply to just learn additional information if \n",
        "and while we can like any other feed forward layer is designed to do \n",
        "'''\n",
        "\n",
        "num_layers = 5\n",
        "'''\n",
        "number of Transformer encoder units that we want to include in our architecture\n",
        "\n",
        "this can vary depending on complexity so you can change this to a higher number \n",
        "if you have a lot more data and also it can\n",
        "pick up more complex patterns otherwise you can also keep it pretty low \n",
        "'''\n",
        "\n",
        "# create an encoder object\n",
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
      ],
      "metadata": {
        "id": "0FzFhz5N_5qd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # includes \n",
        "                                                              # positional enc\n",
        "out = encoder(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysrs-8UlADVd",
        "outputId": "46ec427b-b887-400b-b466-8e283bdba851"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), \n",
            "        attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), \n",
            "        attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), \n",
            "        attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), \n",
            "        attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), \n",
            "        attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install session-info"
      ],
      "metadata": {
        "id": "wwOOsnedJwaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0b99a6-0b84-4da6-c61f-0d3b76a7bef3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.9/dist-packages (from session-info) (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import session_info\n",
        "\n",
        "session_info.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "rooXYu4UX74Q",
        "outputId": "67cafd0a-43b0-4c1c-c32c-7511cf49a42a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "session_info        1.0.0\n",
              "torch               1.13.1+cu116\n",
              "-----\n",
              "</pre>\n",
              "<details>\n",
              "<summary>Click to view modules imported as dependencies</summary>\n",
              "<pre>\n",
              "PIL                 8.4.0\n",
              "astunparse          1.6.3\n",
              "backcall            0.2.0\n",
              "certifi             2022.12.07\n",
              "cffi                1.15.1\n",
              "cycler              0.10.0\n",
              "cython_runtime      NA\n",
              "dateutil            2.8.2\n",
              "debugpy             1.6.6\n",
              "decorator           4.4.2\n",
              "defusedxml          0.7.1\n",
              "google              NA\n",
              "httplib2            0.17.4\n",
              "ipykernel           5.3.4\n",
              "ipython_genutils    0.2.0\n",
              "ipywidgets          7.7.1\n",
              "kiwisolver          1.4.4\n",
              "matplotlib          3.5.3\n",
              "mpl_toolkits        NA\n",
              "numpy               1.22.4\n",
              "opt_einsum          v3.3.0\n",
              "packaging           23.0\n",
              "pexpect             4.8.0\n",
              "pickleshare         0.7.5\n",
              "pkg_resources       NA\n",
              "platformdirs        3.1.0\n",
              "portpicker          NA\n",
              "prompt_toolkit      2.0.10\n",
              "psutil              5.4.8\n",
              "ptyprocess          0.7.0\n",
              "pydev_ipython       NA\n",
              "pydevconsole        NA\n",
              "pydevd              2.9.5\n",
              "pydevd_file_utils   NA\n",
              "pydevd_plugins      NA\n",
              "pydevd_tracing      NA\n",
              "pygments            2.6.1\n",
              "pyparsing           3.0.9\n",
              "sitecustomize       NA\n",
              "six                 1.15.0\n",
              "socks               1.7.1\n",
              "sphinxcontrib       NA\n",
              "storemagic          NA\n",
              "tornado             6.2\n",
              "tqdm                4.65.0\n",
              "traitlets           5.7.1\n",
              "typing_extensions   NA\n",
              "wcwidth             0.2.6\n",
              "zmq                 23.2.1\n",
              "</pre>\n",
              "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
              "<pre>\n",
              "-----\n",
              "IPython             7.9.0\n",
              "jupyter_client      6.1.12\n",
              "jupyter_core        5.2.0\n",
              "notebook            6.3.0\n",
              "-----\n",
              "Python 3.9.16 (main, Dec  7 2022, 01:11:51) [GCC 9.4.0]\n",
              "Linux-5.10.147+-x86_64-with-glibc2.31\n",
              "-----\n",
              "Session information updated at 2023-03-14 13:48\n",
              "</pre>\n",
              "</details>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}