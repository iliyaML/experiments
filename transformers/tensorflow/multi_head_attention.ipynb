{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/"
      ],
      "metadata": {
        "id": "Q5NivO1cy-Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "E0AWVaL8zTPT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('NumPy version:', np.__version__)\n",
        "print('TensorFlow version:', tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lppM0egA0KPR",
        "outputId": "ea5f37c9-c2ee-49f7-befa-0a90c9de71c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.22.4\n",
            "TensorFlow version: 2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled-Dot Product Attention"
      ],
      "metadata": {
        "id": "2pb7nw2o1GmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the Scaled-Dot Product Attention\n",
        "class DotProductAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        " \n",
        "    def call(self, queries, keys, values, d_k, mask=None):\n",
        "        # Scoring the queries against the keys after transposing the latter, \n",
        "        # and scaling\n",
        "        scores = tf.matmul(queries, keys, transpose_b=True) \\\n",
        "        / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        " \n",
        "        # Apply mask to the attention scores\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        " \n",
        "        # Computing the weights by a softmax operation\n",
        "        weights = tf.keras.activations.softmax(scores)\n",
        " \n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return tf.matmul(weights, values)"
      ],
      "metadata": {
        "id": "Er3ts9Kby9dB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "LSy_A_FM0-jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq_length = 5  # Maximum length of the input sequence\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "batch_size = 64  # Batch size from the training process\n",
        " \n",
        "queries = np.random.random((batch_size, input_seq_length, d_k))\n",
        "keys = np.random.random((batch_size, input_seq_length, d_k))\n",
        "values = np.random.random((batch_size, input_seq_length, d_v))\n",
        "\n",
        "print(queries.shape, keys.shape, values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZanZVmrz7T5",
        "outputId": "9be5d033-e7eb-4414-c773-95ad806957e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 5, 64) (64, 5, 64) (64, 5, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = DotProductAttention()\n",
        "attention_output = attention(queries, keys, values, d_k)\n",
        "print(attention_output.shape)\n",
        "print(attention_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae1awlmF1O5S",
        "outputId": "2144fe8a-4369-4ca3-d9af-953c65d04756"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 5, 64)\n",
            "tf.Tensor(\n",
            "[[[0.35911444 0.27986366 0.43175602 ... 0.4172761  0.7320518  0.5585019 ]\n",
            "  [0.35966444 0.28836572 0.42785645 ... 0.42110783 0.7298768  0.54711026]\n",
            "  [0.37074962 0.25623086 0.4194161  ... 0.4300927  0.7316338  0.5674546 ]\n",
            "  [0.36677894 0.2786251  0.42215994 ... 0.42453918 0.73416126 0.5516065 ]\n",
            "  [0.37361792 0.25851774 0.42539382 ... 0.42708644 0.73355174 0.5652497 ]]\n",
            "\n",
            " [[0.6587192  0.53937733 0.55646425 ... 0.40068153 0.5077247  0.58336484]\n",
            "  [0.6719176  0.5389432  0.5485055  ... 0.4065726  0.50623816 0.5716106 ]\n",
            "  [0.64990133 0.54997545 0.55187446 ... 0.40260786 0.509494   0.5744652 ]\n",
            "  [0.6299228  0.5615862  0.5461196  ... 0.39455885 0.5144636  0.5840039 ]\n",
            "  [0.6494949  0.53913033 0.5454416  ... 0.41275638 0.49786437 0.5701406 ]]\n",
            "\n",
            " [[0.58725727 0.60501516 0.64156836 ... 0.53461623 0.6168226  0.2626707 ]\n",
            "  [0.5684277  0.6140933  0.638802   ... 0.50707954 0.61244035 0.26776162]\n",
            "  [0.5767349  0.5994956  0.633645   ... 0.52529156 0.5953866  0.2644099 ]\n",
            "  [0.56026244 0.6152314  0.6353574  ... 0.49947852 0.6054966  0.26950482]\n",
            "  [0.57824814 0.6185067  0.6306199  ... 0.5133381  0.6248715  0.271672  ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.49191505 0.6166096  0.32757393 ... 0.39578345 0.3318537  0.3553764 ]\n",
            "  [0.4943025  0.6341451  0.3166023  ... 0.3960164  0.33901626 0.34892148]\n",
            "  [0.48523885 0.60425997 0.33806023 ... 0.390577   0.3220658  0.36084148]\n",
            "  [0.46151108 0.5904642  0.34792754 ... 0.37847704 0.32117954 0.38060704]\n",
            "  [0.48971245 0.61944836 0.32550055 ... 0.39403462 0.3351989  0.35542524]]\n",
            "\n",
            " [[0.4655379  0.54686064 0.44753608 ... 0.43240464 0.7789866  0.5851255 ]\n",
            "  [0.4603964  0.5628647  0.43906236 ... 0.4466993  0.79347074 0.6149012 ]\n",
            "  [0.46037656 0.5747784  0.43556198 ... 0.43237287 0.7798095  0.5793128 ]\n",
            "  [0.47212407 0.5733822  0.4425671  ... 0.44411153 0.7826208  0.60657704]\n",
            "  [0.4789774  0.5646714  0.45314625 ... 0.44693232 0.7756229  0.5950127 ]]\n",
            "\n",
            " [[0.35604995 0.6103906  0.6627829  ... 0.6684663  0.26735622 0.4583776 ]\n",
            "  [0.3754967  0.6142519  0.6683441  ... 0.6485602  0.26365128 0.47226107]\n",
            "  [0.35607606 0.6343331  0.6616721  ... 0.67415184 0.25146887 0.4714081 ]\n",
            "  [0.37176007 0.62286186 0.66275847 ... 0.6606594  0.25284767 0.45729816]\n",
            "  [0.34628546 0.6127689  0.6594045  ... 0.672227   0.26981696 0.46797553]]], shape=(64, 5, 64), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "MRSEQZED2reG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the Multi-Head Attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.attention = DotProductAttention() # Scaled dot product attention\n",
        "        self.heads = h  # Number of attention heads to use\n",
        "        self.d_k = d_k  \n",
        "        # Dimensionality of the linearly projected queries and keys\n",
        "        self.d_v = d_v  \n",
        "        # Dimensionality of the linearly projected values\n",
        "        self.d_model = d_model  # Dimensionality of the model\n",
        "        self.W_q = tf.keras.layers.Dense(d_k)  \n",
        "        # Learned projection matrix for the queries\n",
        "        self.W_k = tf.keras.layers.Dense(d_k)  \n",
        "        # Learned projection matrix for the keys\n",
        "        self.W_v = tf.keras.layers.Dense(d_v)  \n",
        "        # Learned projection matrix for the values\n",
        "        self.W_o = tf.keras.layers.Dense(d_model)  \n",
        "        # Learned projection matrix for the multi-head output\n",
        " \n",
        "    def reshape_tensor(self, x, heads, flag):\n",
        "        if flag:\n",
        "            # Tensor shape after reshaping and transposing: \n",
        "            # (batch_size, heads, seq_length, -1)\n",
        "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], heads, -1))\n",
        "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
        "        else:\n",
        "            # Reverting the reshaping and transposing operations: \n",
        "            # (batch_size, seq_length, d_k)\n",
        "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
        "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], self.d_k))\n",
        "        return x\n",
        " \n",
        "    def call(self, queries, keys, values, mask=None):\n",
        "        # Rearrange the queries to be able to compute all heads in parallel\n",
        "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange the keys to be able to compute all heads in parallel\n",
        "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange the values to be able to compute all heads in parallel\n",
        "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Compute the multi-head attention output using \n",
        "        # the reshaped queries, keys and values\n",
        "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, \n",
        "                                    self.d_k, mask)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange back the output into concatenated form\n",
        "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
        " \n",
        "        # Apply one final linear projection to the output to generate \n",
        "        # the multi-head attention\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
        "        return self.W_o(output)"
      ],
      "metadata": {
        "id": "n5rtO2Rp2tWk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "KY-h4OGr3XHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq_length = 5  # Maximum length of the input sequence\n",
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
        "batch_size = 64  # Batch size from the training process\n",
        " \n",
        "queries = np.random.random((batch_size, input_seq_length, d_k))\n",
        "keys = np.random.random((batch_size, input_seq_length, d_k))\n",
        "values = np.random.random((batch_size, input_seq_length, d_v))\n",
        "\n",
        "print(queries.shape, keys.shape, values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhWaX_i13LGz",
        "outputId": "63345146-82e2-44b3-be9b-97cbdffea701"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 5, 64) (64, 5, 64) (64, 5, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "multihead_attention_output = multihead_attention(queries, keys, values)\n",
        "print(multihead_attention_output.shape)\n",
        "print(multihead_attention_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVpBQznJ3TaZ",
        "outputId": "431fccf3-0f98-4072-e2db-7ce59c45cc17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 5, 512)\n",
            "tf.Tensor(\n",
            "[[[ 0.20084411 -0.04835438  0.08506437 ... -0.30154708 -0.09826905\n",
            "    0.02063181]\n",
            "  [ 0.19723743 -0.05085796  0.07998941 ... -0.30138454 -0.09685958\n",
            "    0.01925156]\n",
            "  [ 0.2006126  -0.05019293  0.08474492 ... -0.301715   -0.0968755\n",
            "    0.01739117]\n",
            "  [ 0.20130381 -0.04796845  0.08251181 ... -0.3006359  -0.09820308\n",
            "    0.0220839 ]\n",
            "  [ 0.20102677 -0.04776622  0.0855033  ... -0.3031875  -0.0991528\n",
            "    0.01686744]]\n",
            "\n",
            " [[ 0.28139213  0.06235213  0.12181309 ... -0.33304513 -0.22435343\n",
            "    0.00647064]\n",
            "  [ 0.27891335  0.06149861  0.12167642 ... -0.333166   -0.22487411\n",
            "    0.00797319]\n",
            "  [ 0.28005055  0.0647476   0.12119246 ... -0.33376288 -0.22502379\n",
            "    0.00722794]\n",
            "  [ 0.28137016  0.0615837   0.12217928 ... -0.33417442 -0.22468513\n",
            "    0.00510436]\n",
            "  [ 0.2817069   0.05971115  0.12134649 ... -0.33609265 -0.22557211\n",
            "    0.0078532 ]]\n",
            "\n",
            " [[ 0.36538604  0.15977253  0.15339254 ... -0.3448407  -0.19333048\n",
            "    0.02876215]\n",
            "  [ 0.36616623  0.16131091  0.1524326  ... -0.34518772 -0.19551459\n",
            "    0.02788787]\n",
            "  [ 0.36413082  0.1602931   0.15485631 ... -0.34556347 -0.19460881\n",
            "    0.02772031]\n",
            "  [ 0.364416    0.1612746   0.15341182 ... -0.3471488  -0.19513729\n",
            "    0.03136574]\n",
            "  [ 0.36658293  0.15867878  0.151233   ... -0.34238383 -0.19595565\n",
            "    0.03061179]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.2523254   0.06243822  0.08851787 ... -0.35430798 -0.19876815\n",
            "    0.01472357]\n",
            "  [ 0.2521859   0.06299695  0.08698187 ... -0.35396382 -0.19978599\n",
            "    0.0165994 ]\n",
            "  [ 0.25284976  0.06032095  0.09011178 ... -0.35395718 -0.1951595\n",
            "    0.01610056]\n",
            "  [ 0.2536262   0.06085094  0.08682556 ... -0.350079   -0.19699074\n",
            "    0.01439262]\n",
            "  [ 0.2529665   0.06201028  0.0877134  ... -0.35326907 -0.19851123\n",
            "    0.0159268 ]]\n",
            "\n",
            " [[ 0.38791713  0.14612424  0.1699835  ... -0.47534746 -0.1828103\n",
            "    0.01023994]\n",
            "  [ 0.387943    0.15133798  0.171636   ... -0.47816318 -0.18171571\n",
            "    0.00821752]\n",
            "  [ 0.38744488  0.14705668  0.16959551 ... -0.4756313  -0.18068965\n",
            "    0.00905257]\n",
            "  [ 0.3873587   0.14748551  0.17031667 ... -0.47868633 -0.18091004\n",
            "    0.00957185]\n",
            "  [ 0.3873204   0.14677568  0.16931719 ... -0.47478914 -0.18348728\n",
            "    0.01011671]]\n",
            "\n",
            " [[ 0.3282966  -0.01922839  0.0566536  ... -0.36441585 -0.1484912\n",
            "    0.00142641]\n",
            "  [ 0.3315196  -0.01953668  0.05903492 ... -0.36279833 -0.14823796\n",
            "    0.00320444]\n",
            "  [ 0.33007085 -0.0187678   0.05705021 ... -0.36370894 -0.14794503\n",
            "    0.00159679]\n",
            "  [ 0.33164346 -0.02017044  0.0572882  ... -0.3597465  -0.14723693\n",
            "    0.00331418]\n",
            "  [ 0.33092386 -0.01857059  0.05982165 ... -0.36319372 -0.14667058\n",
            "    0.00351119]]], shape=(64, 5, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "hirVTm8i1E33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install session-info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQw8c6FyzEOp",
        "outputId": "fcec4e79-d7fb-4c3c-83eb-c93599a44c31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.10/dist-packages (from session-info) (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import session_info\n",
        "\n",
        "session_info.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "jCSVx8WlzNrH",
        "outputId": "c0c9bf88-8f12-403d-d5d8-973b603611be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "numpy               1.22.4\n",
              "session_info        1.0.0\n",
              "tensorflow          2.12.0\n",
              "-----\n",
              "</pre>\n",
              "<details>\n",
              "<summary>Click to view modules imported as dependencies</summary>\n",
              "<pre>\n",
              "PIL                                         8.4.0\n",
              "aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42    NA\n",
              "absl                                        NA\n",
              "astunparse                                  1.6.3\n",
              "attr                                        23.1.0\n",
              "backcall                                    0.2.0\n",
              "cachetools                                  5.3.0\n",
              "certifi                                     2022.12.07\n",
              "cffi                                        1.15.1\n",
              "chardet                                     4.0.0\n",
              "charset_normalizer                          2.0.12\n",
              "cloudpickle                                 2.2.1\n",
              "cryptography                                40.0.2\n",
              "cycler                                      0.10.0\n",
              "cython_runtime                              NA\n",
              "dateutil                                    2.8.2\n",
              "debugpy                                     1.6.6\n",
              "decorator                                   4.4.2\n",
              "defusedxml                                  0.7.1\n",
              "etils                                       1.2.0\n",
              "flatbuffers                                 23.3.3\n",
              "fsspec                                      2023.4.0\n",
              "gast                                        NA\n",
              "google                                      NA\n",
              "google_auth_httplib2                        NA\n",
              "googleapiclient                             NA\n",
              "h5py                                        3.8.0\n",
              "httplib2                                    0.21.0\n",
              "idna                                        3.4\n",
              "importlib_resources                         NA\n",
              "ipykernel                                   5.5.6\n",
              "ipython_genutils                            0.2.0\n",
              "jax                                         0.4.8\n",
              "jaxlib                                      0.4.7\n",
              "keras                                       2.12.0\n",
              "kiwisolver                                  1.4.4\n",
              "matplotlib                                  3.7.1\n",
              "matplotlib_inline                           0.1.6\n",
              "ml_dtypes                                   0.1.0\n",
              "mpl_toolkits                                NA\n",
              "numexpr                                     2.8.4\n",
              "oauth2client                                4.1.3\n",
              "opt_einsum                                  v3.3.0\n",
              "packaging                                   23.1\n",
              "pandas                                      1.5.3\n",
              "pexpect                                     4.8.0\n",
              "pickleshare                                 0.7.5\n",
              "pkg_resources                               NA\n",
              "platformdirs                                3.3.0\n",
              "portpicker                                  NA\n",
              "prompt_toolkit                              3.0.38\n",
              "psutil                                      5.9.5\n",
              "ptyprocess                                  0.7.0\n",
              "pyarrow                                     9.0.0\n",
              "pyasn1                                      0.5.0\n",
              "pyasn1_modules                              0.3.0\n",
              "pydev_ipython                               NA\n",
              "pydevconsole                                NA\n",
              "pydevd                                      2.9.5\n",
              "pydevd_file_utils                           NA\n",
              "pydevd_plugins                              NA\n",
              "pydevd_tracing                              NA\n",
              "pydot_ng                                    2.0.0\n",
              "pygments                                    2.14.0\n",
              "pyparsing                                   3.0.9\n",
              "pytz                                        2022.7.1\n",
              "requests                                    2.27.1\n",
              "rich                                        NA\n",
              "rsa                                         4.9\n",
              "scipy                                       1.10.1\n",
              "sitecustomize                               NA\n",
              "six                                         1.16.0\n",
              "socks                                       1.7.1\n",
              "sphinxcontrib                               NA\n",
              "storemagic                                  NA\n",
              "tblib                                       1.7.0\n",
              "tensorboard                                 2.12.2\n",
              "termcolor                                   NA\n",
              "tornado                                     6.2\n",
              "traitlets                                   5.7.1\n",
              "typing_extensions                           NA\n",
              "uritemplate                                 4.1.1\n",
              "urllib3                                     1.26.15\n",
              "wcwidth                                     0.2.6\n",
              "wrapt                                       1.14.1\n",
              "zmq                                         23.2.1\n",
              "zoneinfo                                    NA\n",
              "</pre>\n",
              "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
              "<pre>\n",
              "-----\n",
              "IPython             7.34.0\n",
              "jupyter_client      6.1.12\n",
              "jupyter_core        5.3.0\n",
              "notebook            6.4.8\n",
              "-----\n",
              "Python 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
              "Linux-5.10.147+-x86_64-with-glibc2.31\n",
              "-----\n",
              "Session information updated at 2023-05-01 15:16\n",
              "</pre>\n",
              "</details>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}