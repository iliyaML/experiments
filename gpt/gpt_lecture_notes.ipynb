{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwFvdJlWT5Ls"
      },
      "source": [
        "References:\n",
        "- https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
        "\n",
        "Papers:\n",
        "- Attention is All You Need (2017) https://arxiv.org/abs/1706.03762\n",
        "- GPT-3 https://arxiv.org/abs/2005.14165"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiwHonaCEHCz",
        "outputId": "c41d2ad7-5d3d-4b3b-cbb4-b9b0a2d893a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "print('PyTorch version:', torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a Generative-Pretrained Transformer (GPT)\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.\n",
        "\n",
        "- Building nanoGPT\n",
        "- Decoder-only Transformer\n",
        "- Model Size: ~1M parameters\n",
        "- Training code is ~200 lines of code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GN-WBa8jscn_",
        "outputId": "33f0ae64-11a0-470c-d757-4974efde83b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI'd like to focus on is just to train a Transformer-based language model and \\nin our case it's going to be a character level language model \\nI still think that is a very educational with respect to how\\nthese systems work so I don't want to train on the chunk of Internet \\nwe need a smaller data set in this case I propose\\nthat we work with my favorite toy data set it's called Tiny Shakespeare\\n\\nDecoder-only model\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "I'd like to focus on is just to train a Transformer-based language model and \n",
        "in our case it's going to be a character level language model \n",
        "I still think that is a very educational with respect to how\n",
        "these systems work so I don't want to train on the chunk of Internet \n",
        "we need a smaller data set in this case I propose\n",
        "that we work with my favorite toy data set it's called Tiny Shakespeare\n",
        "\n",
        "Decoder-only model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVrbwQTiCvu-"
      },
      "source": [
        "### Reading and Exploring the Data\n",
        "Dataset: TinyShakespeare\n",
        "- Link: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "- Size: 1MB\n",
        "- Length in characters: 1.1M\n",
        "- Vocab size in characters: 65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "321cb509-1a2e-4040-e563-ec00f16366e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 15:11:10--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-06 15:11:10 (21.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. \n",
        "# Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# File size is about 1MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "99a22918-d15d-46d5-8320-49cca8912f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters:\", len(text))\n",
        "\n",
        "# working with roughly 1M characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "ec2f50fe-a72d-4096-e673-01d25828f8da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "8d35e890-9018-46d8-c556-98ed3370b711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocab Size: 65\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# All the characters in our dataset in sorted order\n",
        "print('Characters:', ''.join(chars))\n",
        "print('Vocab Size:', vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLIZxSv7C0zj"
      },
      "source": [
        "### Tokenization\n",
        "- encode and decode\n",
        "\n",
        "Advanced Tokenizers (not used in this notebook):\n",
        "- https://github.com/openai/tiktoken\n",
        "- https://github.com/google/sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zYOCzM-lA5WE",
        "outputId": "92ec3d45-8f5f-440f-848e-a92e05ac53e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndevelop some strategy to tokenize the input text\\n\\ntokenize they mean convert the raw text as a string to some\\nsequence of integers \\n\\nhere we are going to be building a character level language model \\nso we're simply going to be translating individual characters into integers\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\"\"\"\n",
        "develop some strategy to tokenize the input text\n",
        "\n",
        "tokenize they mean convert the raw text as a string to some\n",
        "sequence of integers \n",
        "\n",
        "here we are going to be building a character level language model \n",
        "so we're simply going to be translating individual characters into integers\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "a97e1c32-6ca3-4eda-dde1-64e5ed722446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded String: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoded String: hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s] \n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) \n",
        "\n",
        "print('Encoded String:', encode(\"hii there\"))\n",
        "print('Decoded String:', decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ur_2aCVCtgZO",
        "outputId": "ede7350b-894a-4ce8-879f-d898996b2f5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nencode and decode are essentially our tokenizers\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\"\"\"\n",
        "encode and decode are essentially our tokenizers\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l6OZ3vFKBWs2",
        "outputId": "f690da61-7ad6-429e-f64e-82e7d3702d93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nthis is only one of many possible encodings or many possible sort of\\ntokenizers and it's a very simple one but there's many other schemas \\nthat people have come up with in practice so\\nfor example Google uses a sentence piece \\nhttps://github.com/google/sentencepiece\\n\\nuh so sentence piece \\nwill also encode text into integers but in a different\\nschema and using a different vocabulary and sentencepiece is a sub-word \\ntokenizer and what that means is that you're not encoding entire words \\nbut you're not also encoding individual\\ncharacters it's it's a sub word unit level and that's usually what's adopted\\nin practice\\n\\nopenai has this Library called tick token that uses \\na byte pair encoding tokenizer\\num and that's what GPT uses\\nhttps://github.com/openai/tiktoken\\n\\nbasically you can trade off the code book size and the sequence lengths so\\nyou can have a very long sequences of integers with very small vocabularies \\nor you can have a short sequences of integers with very large vocabularies \\nand so typically people use\\nin practice the sub word encodings\\n\\nbut I'd like to keep our tokenizer very simple so we're using character level\\ntokenizer and that means that we have very small code books \\nwe have very simple encode\\nand decode functions but we do get very long sequences as a result \\nbut that's the level at which we're going to stick with this lecture \\nbecause it's the simplest thing\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\"\"\"\n",
        "this is only one of many possible encodings or many possible sort of\n",
        "tokenizers and it's a very simple one but there's many other schemas \n",
        "that people have come up with in practice so\n",
        "for example Google uses a sentence piece \n",
        "https://github.com/google/sentencepiece\n",
        "\n",
        "uh so sentence piece \n",
        "will also encode text into integers but in a different\n",
        "schema and using a different vocabulary and sentencepiece is a sub-word \n",
        "tokenizer and what that means is that you're not encoding entire words \n",
        "but you're not also encoding individual\n",
        "characters it's it's a sub word unit level and that's usually what's adopted\n",
        "in practice\n",
        "\n",
        "openai has this Library called tick token that uses \n",
        "a byte pair encoding tokenizer\n",
        "um and that's what GPT uses\n",
        "https://github.com/openai/tiktoken\n",
        "\n",
        "basically you can trade off the code book size and the sequence lengths so\n",
        "you can have a very long sequences of integers with very small vocabularies \n",
        "or you can have a short sequences of integers with very large vocabularies \n",
        "and so typically people use\n",
        "in practice the sub word encodings\n",
        "\n",
        "but I'd like to keep our tokenizer very simple so we're using character level\n",
        "tokenizer and that means that we have very small code books \n",
        "we have very simple encode\n",
        "and decode functions but we do get very long sequences as a result \n",
        "but that's the level at which we're going to stick with this lecture \n",
        "because it's the simplest thing\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AvwvsPmBCDOp",
        "outputId": "baa5bc21-adf1-4eab-d11a-6d54f70b3e14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nso now that we have an encoder and a decoder effectively a\\ntokenizer we can tokenize the entire training set of Shakespeare \\nso here's a chunk of code that does that\\nand I'm going to start to use the pytorch library and specifically \\nthe torch.tensor from the pytorch library\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\"\"\"\n",
        "so now that we have an encoder and a decoder effectively a\n",
        "tokenizer we can tokenize the entire training set of Shakespeare \n",
        "so here's a chunk of code that does that\n",
        "and I'm going to start to use the pytorch library and specifically \n",
        "the torch.tensor from the pytorch library\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "39664353-4f9e-4959-fa67-b4910b1c5e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) \n",
        "# the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LOUP_97QCWM9",
        "outputId": "f02b1e96-bf38-4649-fd65-0e3ae9128fc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nthe entire data set of text is re-represented as just it just stretched out as \\na single very large uh sequence of integers\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\"\"\"\n",
        "the entire data set of text is re-represented as just it just stretched out as \n",
        "a single very large uh sequence of integers\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LccvHTXlEnML"
      },
      "source": [
        "### Split Dataset into Train and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f_WIXqxz0lU5",
        "outputId": "0f0b1947-8113-4d39-9f4b-2c468418b45f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nthis will help us understand to what extent our model is overfitting \\n\\nso we're going to basically hide and keep the validation data on the side \\nbecause we don't want just a perfect memorization\\nof this exact Shakespeare \\n\\nwe want a neural network that sort of creates Shakespeare like text and so \\nit should be fairly likely for it to produce the actual like stowed away uh true\\nShakespeare text\\n\\nwe're going to use this to get a sense of the overfitting \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\"\"\"\n",
        "split dataset into train and validation splits\n",
        "\"\"\"\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "\n",
        "# withold the last 10% at the end as validation data\n",
        "val_data = data[n:]\n",
        "\n",
        "\"\"\"\n",
        "this will help us understand to what extent our model is overfitting \n",
        "\n",
        "so we're going to basically hide and keep the validation data on the side \n",
        "because we don't want just a perfect memorization\n",
        "of this exact Shakespeare \n",
        "\n",
        "we want a neural network that sort of creates Shakespeare like text and so \n",
        "it should be fairly likely for it to produce the actual like stowed away uh true\n",
        "Shakespeare text\n",
        "\n",
        "we're going to use this to get a sense of the overfitting \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5IWSvjE2my"
      },
      "source": [
        "### Data Loader: Batches of Chunks of Data\n",
        "- block_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UkC15a4hDGcJ",
        "outputId": "aaf46597-7468-4ded-9223-aa74a933c05b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnow we would like to start plugging these text sequences or integer sequences \\ninto the Transformer so that it can train and learn those patterns\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\"\"\"\n",
        "now we would like to start plugging these text sequences or integer sequences \n",
        "into the Transformer so that it can train and learn those patterns\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G6pAl9sSDZSD",
        "outputId": "8b73914d-b107-4ae8-9974-f4bfffada494"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe important thing to realize is we're never going to actually feed the\\nentire text into Transformer all at once that would be computationally \\nvery expensive and prohibitive \\n\\nso when we actually train a Transformer on a lot of these data sets we only work \\nwith chunks of the data set and when we train the\\nTransformer we basically sample random little chunks out of the training \\nset and train them just chunks at a time and\\n\\nthese chunks have basically some kind of a length and as a maximum length \\nnow the maximum length typically at least in the code I usually write is \\ncalled block_size\\n\\ncan find it on different names like context length or something like that\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\"\"\"\n",
        "The important thing to realize is we're never going to actually feed the\n",
        "entire text into Transformer all at once that would be computationally \n",
        "very expensive and prohibitive \n",
        "\n",
        "so when we actually train a Transformer on a lot of these data sets we only work \n",
        "with chunks of the data set and when we train the\n",
        "Transformer we basically sample random little chunks out of the training \n",
        "set and train them just chunks at a time and\n",
        "\n",
        "these chunks have basically some kind of a length and as a maximum length \n",
        "now the maximum length typically at least in the code I usually write is \n",
        "called block_size\n",
        "\n",
        "can find it on different names like context length or something like that\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwD7yr4-Gmcv"
      },
      "source": [
        "#### Block Size Example - Time Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "3fb73073-47fa-47d0-90d8-902518dfe789"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "block_size = 8\n",
        "\n",
        "# let me look at the first train data characters the first block size + 1 chars\n",
        "# I'll explain why plus one in a second\n",
        "train_data[:block_size+1]\n",
        "\n",
        "# this is the first 9 characters in the sequence in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8VW3moT_Dww5",
        "outputId": "26d55830-84d9-4df2-d513-197bc985544e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwhen you sample a chunk of data like this so say that these nine characters\\nout of the training set, this actually has multiple examples packed into it\\n\\nthat's because all of these characters follow each other\\n\\nwhen we plug it into a Transformer is we're going to actually simultaneously \\ntrain it to make prediction at every one of these positions\\n\\nin the in a chunk of nine characters, there's actually eight individual\\nexamples packed in there\\n\\nExample:\\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\\n\\nwhen in the context of 18, 47 likely comes next \\nin the context of 18 and 47, 56 comes next \\nin the context of 18 47 56, 57 can come next and so on \\n\\nso that's the eight individual examples\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\"\"\"\n",
        "when you sample a chunk of data like this so say that these nine characters\n",
        "out of the training set, this actually has multiple examples packed into it\n",
        "\n",
        "that's because all of these characters follow each other\n",
        "\n",
        "when we plug it into a Transformer is we're going to actually simultaneously \n",
        "train it to make prediction at every one of these positions\n",
        "\n",
        "in the in a chunk of nine characters, there's actually eight individual\n",
        "examples packed in there\n",
        "\n",
        "Example:\n",
        "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
        "\n",
        "when in the context of 18, 47 likely comes next \n",
        "in the context of 18 and 47, 56 comes next \n",
        "in the context of 18 47 56, 57 can come next and so on \n",
        "\n",
        "so that's the eight individual examples\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "7a6e3d58-6f83-42a0-98df-562b8626fb53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "# Code Example to illustrate\n",
        "\n",
        "# x is the input to the Transformer\n",
        "# x is the first block size characters\n",
        "x = train_data[:block_size]\n",
        "\n",
        "# y will be the next block size characters so it's offset by one\n",
        "# y are the targets for each position in the input\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "# iterating over all the block size of 8\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "25hXnhhaHRf2",
        "outputId": "35096a7c-8e18-4ac4-a6de-b5b8956099f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nthese are the eight examples hidden in a chunk of nine characters that \\nwe uh sampled from the training set \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\"\"\"\n",
        "these are the eight examples hidden in a chunk of nine characters that \n",
        "we uh sampled from the training set \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_vz0NC-FE0ZA",
        "outputId": "ae6bc8d4-1bb3-411f-ead6-56e2f63bbf90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI want to mention one more thing we train on all the eight examples here\\nwith context between one all the way up to context of block size and \\nwe train on that not just for\\ncomputational reasons because we happen to have the sequence already or \\nsomething like that it's not just done for efficiency \\n\\nit's also done to make the Transformer Network be used to seeing contexts \\nall the way from as little as one all the way to block size\\nand we'd like the transform to be used to seeing everything in between \\nand that's going to be useful later during\\ninference because while we're sampling we can start \\nthe sampling generation with as little as one character of\\ncontext and the Transformer knows how to predict \\nthe next character with all the way up to just one context of one and so\\nthen it can predict everything up to block size and after block size \\nwe have to start truncating because the\\nTransformer will never receive more than block size inputs \\nwhen it's predicting the next character\\n\\nwe've looked at the Time Dimension of the tensors that are going to be feeding\\ninto the Transformer\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "\"\"\"\n",
        "I want to mention one more thing we train on all the eight examples here\n",
        "with context between one all the way up to context of block size and \n",
        "we train on that not just for\n",
        "computational reasons because we happen to have the sequence already or \n",
        "something like that it's not just done for efficiency \n",
        "\n",
        "it's also done to make the Transformer Network be used to seeing contexts \n",
        "all the way from as little as one all the way to block size\n",
        "and we'd like the transform to be used to seeing everything in between \n",
        "and that's going to be useful later during\n",
        "inference because while we're sampling we can start \n",
        "the sampling generation with as little as one character of\n",
        "context and the Transformer knows how to predict \n",
        "the next character with all the way up to just one context of one and so\n",
        "then it can predict everything up to block size and after block size \n",
        "we have to start truncating because the\n",
        "Transformer will never receive more than block size inputs \n",
        "when it's predicting the next character\n",
        "\n",
        "we've looked at the Time Dimension of the tensors that are going to be feeding\n",
        "into the Transformer\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNoVzQYAIDqk"
      },
      "source": [
        "#### Batch Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4RUu0t2YFUqq",
        "outputId": "e40285ef-5fb5-4afd-80d3-98e73a37ef98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nAnother Important Dimension: Batch\\n\\nas we're sampling these chunks of text we're going to be actually every time\\nwe're going to feed them into a Transformer \\n\\nwe're going to have many \\nbatches of multiple chunks of text that are all like stacked up in a single\\ntensor and that's just done for efficiency just so that we can \\nkeep the gpus busy because they are very good at\\nparallel processing of data and so we just want \\nto process multiple chunks all at the same\\ntime but those chunks are processed completely independently \\nthey don't talk to each other and so on\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "\"\"\"\n",
        "Another Important Dimension: Batch\n",
        "\n",
        "as we're sampling these chunks of text we're going to be actually every time\n",
        "we're going to feed them into a Transformer \n",
        "\n",
        "we're going to have many \n",
        "batches of multiple chunks of text that are all like stacked up in a single\n",
        "tensor and that's just done for efficiency just so that we can \n",
        "keep the gpus busy because they are very good at\n",
        "parallel processing of data and so we just want \n",
        "to process multiple chunks all at the same\n",
        "time but those chunks are processed completely independently \n",
        "they don't talk to each other and so on\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "c5696818-9b00-4c7c-f245-dd8350bfa2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: x\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets: y\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337) # for reproducibility\n",
        "\"\"\"\n",
        "because we're going to start sampling random locations in the dataset \n",
        "to pull chunks from I am setting the seed so that um in \n",
        "the random number generator so that the numbers I see here are going \n",
        "to be the same numbers you see later if you try to reproduce this\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "\"\"\"\n",
        "how many independent sequences we are processing every \n",
        "forward backward pass of the Transformer\n",
        "\"\"\"\n",
        "\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    generate a small batch of data of inputs x and targets y\n",
        "    \"\"\"\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    \"\"\"\n",
        "    when I Generate random positions to grab a chunk out of \n",
        "    I actually grab I actually generate\n",
        "    batch size number of random offsets so because this is four we are IX is\n",
        "    going to be a four numbers that are randomly generated between 0 \n",
        "    and len(data) - block_size \n",
        "    so it's just random offsets into the training set\n",
        "    \"\"\"\n",
        "\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    \"\"\"\n",
        "    X's as I explained are the\n",
        "    first block size characters starting at I\n",
        "    \"\"\"\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    \"\"\"\n",
        "    Y's are the offset by one of that so just add plus one\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    we're going to get those chunks for every one of integers I in IX and\n",
        "    use a torch.stack to take all those one-dimensional tensors as we saw here\n",
        "    and we're going to um stack them up at rows and so they all become \n",
        "    a row in a four by eight tensor (4x8)\n",
        "    \"\"\"\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs: x') # input to the Transformer\n",
        "print(xb.shape)\n",
        "\"\"\"\n",
        "the input X is the four by eight tensor\n",
        "four uh rows of eight columns\n",
        "\n",
        "each one of these is a chunk of the\n",
        "training set and then the targets here are in the associated array Y and \n",
        "they will come in\n",
        "through the Transformer all the way at the end to create the loss function so\n",
        "they will give us the correct answer for every single position inside X\n",
        "\n",
        "this four by eight array contains a total of 32 examples and \n",
        "they're completely independent as far as the Transformer is concerned\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(xb)\n",
        "print('targets: y')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "so you can sort of see this spelled out \n",
        "these are the 32 independent examples packed in \n",
        "to a single batch of the input X and then the desired targets are in y\n",
        "\n",
        "this integer tensor of X is going to feed into the Transformer and \n",
        "that Transformer is going  to simultaneously process all these\n",
        "examples and then look up the correct um integers to predict \n",
        "in every one of these positions in the tensor y\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BtmL8XBK_7sP",
        "outputId": "875d053c-d05c-4106-8cff-790fc7b17dac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nso you can sort of see this spelled out \\nthese are the 32 independent examples packed in \\nto a single batch of the input X and then the desired targets are in y\\n\\nthis integer tensor of X is going to feed into the Transformer and \\nthat Transformer is going  to simultaneously process all these\\nexamples and then look up the correct um integers to predict \\nin every one of these positions in the tensor y\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mueAUGjWKemf"
      },
      "source": [
        "### Simplest Baseline: Bigram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "11c52b2b-addd-4d30-f8b9-82ebbb0f684d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\"\n",
        "now that we have our batch of input, that we'd like to feed into a Transformer \n",
        "\n",
        "let's start basically feeding this into neural networks\n",
        "\"\"\"\n",
        "print(xb.shape)\n",
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkzWi88IpKnC",
        "outputId": "cf34a7d6-fe66-47bd-cc5b-6be40ee3a4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ],
      "source": [
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S1b7UdbsQ1C"
      },
      "source": [
        "#### Make Predictions about What Comes Next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dXa3NR6ihNv",
        "outputId": "8aa0c8f5-a7ae-4253-d987-17245c7e2a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 65])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "start off with the simplest possible neural network which in the case of \n",
        "language modeling in my opinion is the Bigram Language Model and \n",
        "we've covered the background language model in my Makemore series \n",
        "in a lot of depth\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    constructing a bigram language model which is a subclass of nn.Module\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next \n",
        "        # token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        # creating a token embedding table\n",
        "        # of size (vocab_size x vocab_size)\n",
        "\n",
        "        # we're using nn.embedding which is a very thin wrapper \n",
        "        # around basically a tensor of shape of (vocab_size x vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # inputs X here which I rename to idx\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        # arranged into a batch (B) by time (T) by channel (C) tensor\n",
        "        # in this case, batch is 4, time is 8, channel is vocab_size or 65\n",
        "\n",
        "        \"\"\"\n",
        "        we're going to interpret this as the logits \n",
        "        which are basically the scores for the next character in the sequence\n",
        "\n",
        "        and so what's happening here is we are predicting what comes next \n",
        "        based on just the individual identity of a single\n",
        "        token and you can do that because um I mean currently the tokens \n",
        "        are not talking to each other and they're not\n",
        "        seeing any context except for they're just seeing themselves \n",
        "        so I'm a I'm a token number five and then I can\n",
        "        actually make pretty decent predictions about what comes next just by \n",
        "        knowing that I'm token five because some characters know \n",
        "        cert follow other characters in in typical scenarios\n",
        "        \"\"\"\n",
        "\n",
        "        return logits\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "# calling the model by passing inputs (xb) and targets (yb)\n",
        "out = m(xb, yb)\n",
        "\n",
        "\"\"\"\n",
        "we currently get the predictions, the scores, the logits for\n",
        "every one of the four by eight positions \n",
        "\"\"\"\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Ms_Lo4sWrr"
      },
      "source": [
        "#### Evaluate Loss Function\n",
        "- Cross Entropy (Negative Log Likelihood Loss)\n",
        "- https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_VmjG5Zk-Ei",
        "outputId": "4b79d3f7-c7b7-4764-9064-11f2e6dc398d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "we'd like to evaluate the loss function and \n",
        "so in Makemore series we saw that a good way to measure a loss or \n",
        "like a quality of the predictions, is to use the negative log likelihood loss \n",
        "which is also implemented in PyTorch under the name\n",
        "cross entropy\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next \n",
        "        # token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # inputs X here which I rename to idx\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        # unpact those numbers\n",
        "        B, T, C = logits.shape\n",
        "\n",
        "        \"\"\"\n",
        "        what I like to do I like to take basically give names to the dimensions \n",
        "        so launches.shape is B by T by C and\n",
        "        unpack those numbers and then \n",
        "        \n",
        "        let's say that logits equals logits.view\n",
        "        and we want it to be a b times c b times T by C \n",
        "        so just a two-dimensional array\n",
        "        \n",
        "        right so we're going to take all the we're going to take all of these um\n",
        "        positions here and we're going to uh stretch them out in a \n",
        "        one-dimensional sequence and \n",
        "        preserve the channel Dimension as the second dimension \n",
        "        \n",
        "        so we're just kind of like stretching \n",
        "        out the array so it's two-dimensional and in that case \n",
        "        it's going to better conform to \n",
        "        what PyTorch sort of expects in its dimensions\n",
        "        \"\"\"\n",
        "        logits = logits.view(B*T, C)\n",
        "\n",
        "        \"\"\"\n",
        "        we have to do the same to targets\n",
        "        because currently targets are of shape B by T and \n",
        "        we want it to be just B times T so one dimensional now \n",
        "        \n",
        "        alternatively  you could always still just do -1 \n",
        "        because PyTorch will guess what this should be\n",
        "        \"\"\"\n",
        "        targets = targets.view(B*T) # targets.view(-1)\n",
        "\n",
        "        # loss is the cross entropy on the predictions (logits) and the targets\n",
        "        \"\"\"\n",
        "        this measures the quality of the logits with respect to the Targets \n",
        "        in other words we have the identity of the next character \n",
        "        so how well are we predicting the next character based on logits\n",
        "\n",
        "        intuitively the correct um the correct dimension of logits uh\n",
        "        depending on whatever the target is should have a very high number \n",
        "        and all the other dimensions should be very low number right\n",
        "\n",
        "        intuitively we want to measure this\n",
        "        \"\"\"\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "# passing inputs and targets\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "\n",
        "# can now evaluate our loss\n",
        "\"\"\"\n",
        "currently we see that the loss is 4.87\n",
        "\n",
        "because our we have 65 possible vocabulary elements \n",
        "we can actually guess at what the loss should be and \n",
        "\n",
        "in particular we covered negative log likelihood in a lot of detail \n",
        "we are expecting log or ln of 1 over 65 and negative of that (-ln(1/65))\n",
        "so we're expecting the loss to be about 4.1217 but we're getting 4.87\n",
        "\n",
        "that's telling us that the initial predictions are not super diffuse \n",
        "they've got a little bit of entropy so we're guessing wrong\n",
        "\n",
        "but we are able to evaluate the loss\n",
        "\"\"\"\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUO0f4qMwYId"
      },
      "source": [
        "#### Generate from the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpbKO9cQk-By",
        "outputId": "247b01af-9e4a-4963-afe6-8c7c264e7817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "now that we can evaluate the quality of the model on some data \n",
        "\n",
        "we'd likely also be able to generate from the model so let's do the generation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next \n",
        "        # token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # inputs X here which I rename to idx\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "          loss = None\n",
        "        else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B*T, C)\n",
        "          targets = targets.view(B*T)\n",
        "          # loss is the cross entropy on the predictions and the targets\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # take the the same kind of input idx here\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"\n",
        "        idx - basically is the current context of some characters in some batch\n",
        "            - of size (B,T) array\n",
        "\n",
        "        basically this (B,T) and \n",
        "        make it a B by T plus one plus two plus three \n",
        "        as many as we want max_new_tokens \n",
        "        \n",
        "        so this is the generation from the model\n",
        "        \"\"\"\n",
        "\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step (-1)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "    \n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "# passing inputs and targets\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "\n",
        "# can now evaluate our loss\n",
        "print(loss)\n",
        "\n",
        "# generating 100 tokens\n",
        "idx = torch.zeros((1, 1), dtype=torch.long) # (B, T) = (1, 1)\n",
        "\"\"\"\n",
        "I'm creating a batch will be just one time will be just one so \n",
        "I'm creating a little one by one\n",
        "tensor and it's holding a zero and the D type the data type is integer\n",
        "so 0 is going to be how we kick off the generation and remember that zero is uh\n",
        "is the element standing for a new line character so \n",
        "it's kind of like a reasonable thing to to feed in as the\n",
        "very first character in a sequence to be the new line\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "so it's going to be idx which \n",
        "we're going to feed in here then we're going to ask for 100 tokens\n",
        "\n",
        "and then m.generate() will continue that\n",
        "\"\"\"\n",
        "print(decode(m.generate(idx, \n",
        "                        max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL_hv03L6dnF"
      },
      "source": [
        "##### No Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPjNVe6b6coG",
        "outputId": "c5b303b4-8c37-453b-9bc9-ead7f34d24ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next \n",
        "        # token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), \n",
        "                        max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RAXPxrMI5-Kc",
        "outputId": "4bfd40ef-b425-4b5e-a538-623ac7d5521f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nso obviously it's garbage and the reason it's garbage is because \\nthis is a totally random model so next up\\nwe're going to want to train this model\\n\\none more thing I wanted to point out here is \\nthis function is written to be General\\nbut it's kind of like ridiculous right now because \\nwe're feeding in all this we're building\\nout this context and we're concatenating it all and \\nwe're always feeding it all\\ninto the model but that's kind of ridiculous because \\nthis is just a simple background model\\nso to make for example this prediction about K we only needed this W but \\nactually what we fed into the model is\\nwe fed the entire sequence and then \\nwe only looked at the very last piece and predicted k\\nso the only reason I'm writing it in this way is because right now \\nthis is a bygram model but I'd like to keep this\\nfunction fixed and I'd like it to work later when our character is actually\\nbasically look further in the history and \\n\\nso right now \\nthe history is not used so this looks silly but eventually the\\nhistory will be used and so that's why we want to do it this way \\n\\nso just a quick comment on that\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "\"\"\"\n",
        "so obviously it's garbage and the reason it's garbage is because \n",
        "this is a totally random model so next up\n",
        "we're going to want to train this model\n",
        "\n",
        "one more thing I wanted to point out here is \n",
        "this function is written to be General\n",
        "but it's kind of like ridiculous right now because \n",
        "we're feeding in all this we're building\n",
        "out this context and we're concatenating it all and \n",
        "we're always feeding it all\n",
        "into the model but that's kind of ridiculous because \n",
        "this is just a simple background model\n",
        "so to make for example this prediction about K we only needed this W but \n",
        "actually what we fed into the model is\n",
        "we fed the entire sequence and then \n",
        "we only looked at the very last piece and predicted k\n",
        "so the only reason I'm writing it in this way is because right now \n",
        "this is a bygram model but I'd like to keep this\n",
        "function fixed and I'd like it to work later when our character is actually\n",
        "basically look further in the history and \n",
        "\n",
        "so right now \n",
        "the history is not used so this looks silly but eventually the\n",
        "history will be used and so that's why we want to do it this way \n",
        "\n",
        "so just a quick comment on that\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4zCMAKMU17R"
      },
      "source": [
        "### Training the Bigram Model\n",
        "- Optimizer: AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rHZFWou_k9-x",
        "outputId": "1a063ecf-398b-476c-d163-56dc256d2886"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nin the make more series we've only ever used stochastic gradient descent\\nthe simplest possible Optimizer which you can get using the SGD instead \\n\\nbut I want to use Adam which is a much more advanced and popular Optimizer and \\nit works extremely well \\n\\nLearning Rate\\nfor a typical good setting for the learning rate is roughly 3e-4 \\nbut for very very small networks luck is the case here you can\\nget away with much much higher learning rates running -3 \\nor even higher probably\\n\\nbut let me create the optimizer object which will basically take the gradients \\nand update the parameters using the gradients\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "\"\"\"\n",
        "let's train the model so it becomes a bit less random\n",
        "\"\"\"\n",
        "\n",
        "# in Makemore we use SGD optimizer, here we use AdamW\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "\"\"\"\n",
        "in the make more series we've only ever used stochastic gradient descent\n",
        "the simplest possible Optimizer which you can get using the SGD instead \n",
        "\n",
        "but I want to use Adam which is a much more advanced and popular Optimizer and \n",
        "it works extremely well \n",
        "\n",
        "Learning Rate\n",
        "for a typical good setting for the learning rate is roughly 3e-4 \n",
        "but for very very small networks luck is the case here you can\n",
        "get away with much much higher learning rates running -3 \n",
        "or even higher probably\n",
        "\n",
        "but let me create the optimizer object which will basically take the gradients \n",
        "and update the parameters using the gradients\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIbDm2CSk98Y",
        "outputId": "c0815a76-e2f0-4a40-875c-b46562e8f457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 4.704006195068359\n",
            "Final loss: 2.5727508068084717\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "our batch size up above was only 4 \n",
        "\n",
        "so let me actually use something bigger let's say 32\n",
        "\"\"\"\n",
        "batch_size = 32\n",
        "n_steps = 10000\n",
        "\n",
        "for steps in range(n_steps): # increase number of steps for good results... \n",
        "    \n",
        "    \"\"\"\n",
        "    for some number of steps um \n",
        "    we are sampling a new batch of data\n",
        "    \"\"\"\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    \"\"\"\n",
        "    we're evaluating the loss \n",
        "    we're zeroing out all the gradients from the previous step\n",
        "    \"\"\"\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    \"\"\"\n",
        "    getting the gradients for all the parameters\n",
        "    \"\"\"\n",
        "    loss.backward()\n",
        "\n",
        "    \"\"\"\n",
        "    using those gradients to update our parameters \n",
        "    so typical training loop as we saw in the\n",
        "    Makemore series\n",
        "    \"\"\"\n",
        "    optimizer.step()\n",
        "    \n",
        "    if steps == 0:\n",
        "      print('Initial loss:', loss.item())\n",
        "\n",
        "print('Final loss:', loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8P7EaqoPk95m",
        "outputId": "829251cb-6b59-4264-db52-cbebd6c528c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht anjx?\n",
            "\n",
            "DUThinqunt.\n",
            "\n",
            "LaZAnde.\n",
            "athave l.\n",
            "KEONH:\n",
            "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
            "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
            "I hisgothers je are!-e!\n",
            "QLYotouciullle'z\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncertainly not Shakespeare but the model is making progress \\nso that is the simplest possible model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# so this is the simplest possible model\n",
        "print(decode(m.generate(idx, \n",
        "                        max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\"\"\"\n",
        "certainly not Shakespeare but the model is making progress \n",
        "so that is the simplest possible model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pPTn1N-1k93T",
        "outputId": "82e60deb-dc04-4aec-960c-7eb50f28d7a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nobviously that this is a very simple model because \\nthe tokens are not talking to each other \\n\\nso given the previous context of whatever was generated \\nwe're only looking at the very last character to make the predictions \\nabout what comes next\\n\\nso now these uh now these tokens\\nhave to start talking to each other and figuring out what is in the context \\nso that they can make better predictions for what comes next\\n\\nthis is how we're going to kick off the Transformer\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "\"\"\"\n",
        "obviously that this is a very simple model because \n",
        "the tokens are not talking to each other \n",
        "\n",
        "so given the previous context of whatever was generated \n",
        "we're only looking at the very last character to make the predictions \n",
        "about what comes next\n",
        "\n",
        "so now these uh now these tokens\n",
        "have to start talking to each other and figuring out what is in the context \n",
        "so that they can make better predictions for what comes next\n",
        "\n",
        "this is how we're going to kick off the Transformer\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th3sB15HVMHi"
      },
      "source": [
        "### Port Our Code to a Script: bigram.py\n",
        "- https://github.com/karpathy/ng-video-lecture/commit/83f7d22b80a866e337a069dbc17b677f53a6b5a9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YTnylCuk9x2",
        "outputId": "fbb5c9fc-3d43-4f83-b7b7-4c3401ce23dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "step 0: train loss 4.7305, val loss 4.7241\n",
            "step 300: train loss 2.8110, val loss 2.8249\n",
            "step 600: train loss 2.5434, val loss 2.5682\n",
            "step 900: train loss 2.4932, val loss 2.5088\n",
            "step 1200: train loss 2.4863, val loss 2.5035\n",
            "step 1500: train loss 2.4665, val loss 2.4921\n",
            "step 1800: train loss 2.4683, val loss 2.4936\n",
            "step 2100: train loss 2.4696, val loss 2.4846\n",
            "step 2400: train loss 2.4638, val loss 2.4879\n",
            "step 2700: train loss 2.4738, val loss 2.4911\n",
            "\n",
            "\n",
            "\n",
            "CEThik brid owindakis b, bth\n",
            "\n",
            "HAPet bobe d e.\n",
            "S:\n",
            "O:3 my d?\n",
            "LUCous:\n",
            "Wanthar u qur, t.\n",
            "War dXENDoate awice my.\n",
            "\n",
            "Hastarom oroup\n",
            "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\n",
            "Swanousel lind me l.\n",
            "HAshe ce hiry:\n",
            "Supr aisspllw y.\n",
            "Hentofu n Boopetelaves\n",
            "MPOLI s, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te\n",
            "\n",
            "AN ad nterupt f s ar igr t m:\n",
            "\n",
            "Thin maleronth,\n",
            "Mad\n",
            "RD:\n",
            "\n",
            "WISo myrangoube!\n",
            "KENob&y, wardsal thes ghesthinin couk ay aney IOUSts I&fr y ce.\n",
            "J\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "I took the code that we developed in this Jupiter notebook and \n",
        "I converted it to be a script and  \n",
        "I'm doing this because I just want to simplify our intermediate work into \n",
        "just the final product that we have at this point\n",
        "\n",
        "File: bigram.py\n",
        "\n",
        "New Additions:\n",
        "1. Enabled gpu if available - run on cuda\n",
        "2. estimate_loss()\n",
        "3. model.eval(), model.train() phases\n",
        "-- it is a good practice to Think Through what mode your neural network is in \n",
        "because some layers will have different behaviors at \n",
        "inference time or training time\n",
        "\n",
        "4. @torch.no_grad() - more memory efficient when we don't intend to do\n",
        "back propagation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "# ================================================================\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# added gpu capability if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device)\n",
        "\n",
        "eval_iters = 200\n",
        "# ================================================================\n",
        "\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Read Data\n",
        "# ================================================================ \n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "# ================================================================\n",
        "\n",
        "# Encoder and Decoder\n",
        "# ================================================================ \n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s] \n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "# ================================================================\n",
        "\n",
        "# Create Train and Test Splits\n",
        "# ================================================================\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# ================================================================\n",
        "\n",
        "# Data Loading or Loader - gets a batch of the inputs and targets\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # when we load the data, we move to device\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "this context manager torch.nograd and this is just telling pytorch \n",
        "that everything that happens\n",
        "inside this function we will not call that backward on and \n",
        "\n",
        "so pytorch can be a\n",
        "lot more efficient with its memory use because it doesn't have to store all \n",
        "the intermediate variables because we're\n",
        "never going to call backward and \n",
        "so it can it can be a lot more memory efficient in that way\n",
        "\n",
        "a good practice to tell PyTorch when we don't intend to do back propagation\n",
        "\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    it averages up the loss over multiple batches \n",
        "    so in particular \n",
        "    we're going to iterate eval_iter times and \n",
        "    we're going to\n",
        "    basically get our loss and then we're going to get the average loss \n",
        "    for both splits and so this will be a lot less\n",
        "    noisy\n",
        "\n",
        "    when we call the estimate loss we're going to report the pretty\n",
        "    accurate train and validation loss\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval() # setting phases\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # setting phases\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next \n",
        "        # token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device) # when we create the model, we want to move the model\n",
        "# parameters to device\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "# ================================================================\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        # when we call the estimate loss \n",
        "        # we're going to report the pretty accurate train and validation loss\n",
        "        losses = estimate_loss()\n",
        "        print(f\"\"\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\"\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# when I'm creating the context that feeds into generate \n",
        "# I have to make sure that I create on the device\n",
        "\n",
        "\"\"\"\n",
        "I ran this code it was giving me the train loss and val loss\n",
        "and we see that we convert to somewhere around 2.5 with the bigram model \n",
        "\n",
        "and then here's\n",
        "the sample that we produced at the end and so we have everything packaged up in\n",
        "the script and we're in a good position now to iterate on this\n",
        "\"\"\"\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9j6c9yR-0jC"
      },
      "source": [
        "#### No Comments\n",
        "- https://github.com/karpathy/ng-video-lecture/blob/master/bigram.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqviTeCn-z8W",
        "outputId": "1d457dfd-88b9-423d-de78-0c4284ef4f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7305, val loss 4.7241\n",
            "step 300: train loss 2.8110, val loss 2.8249\n",
            "step 600: train loss 2.5434, val loss 2.5682\n",
            "step 900: train loss 2.4932, val loss 2.5088\n",
            "step 1200: train loss 2.4863, val loss 2.5035\n",
            "step 1500: train loss 2.4665, val loss 2.4921\n",
            "step 1800: train loss 2.4683, val loss 2.4936\n",
            "step 2100: train loss 2.4696, val loss 2.4846\n",
            "step 2400: train loss 2.4638, val loss 2.4879\n",
            "step 2700: train loss 2.4738, val loss 2.4911\n",
            "\n",
            "\n",
            "\n",
            "CEThik brid owindakis b, bth\n",
            "\n",
            "HAPet bobe d e.\n",
            "S:\n",
            "O:3 my d?\n",
            "LUCous:\n",
            "Wanthar u qur, t.\n",
            "War dXENDoate awice my.\n",
            "\n",
            "Hastarom oroup\n",
            "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\n",
            "Swanousel lind me l.\n",
            "HAshe ce hiry:\n",
            "Supr aisspllw y.\n",
            "Hentofu n Boopetelaves\n",
            "MPOLI s, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te\n",
            "\n",
            "AN ad nterupt f s ar igr t m:\n",
            "\n",
            "Thin maleronth,\n",
            "Mad\n",
            "RD:\n",
            "\n",
            "WISo myrangoube!\n",
            "KENob&y, wardsal thes ghesthinin couk ay aney IOUSts I&fr y ce.\n",
            "J\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fy-u3ucCk9vS",
        "outputId": "f7924696-c960-4b50-dd5e-c47d56fc3269"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwe have everything packaged up in the script and we're in a good position \\nnow to iterate on this okay so we are almost ready to start writing our very\\nfirst self-attention block for processing these tokens \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "\"\"\"\n",
        "we have everything packaged up in the script and we're in a good position \n",
        "now to iterate on this okay so we are almost ready to start writing our very\n",
        "first self-attention block for processing these tokens \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7eMceEaJ5z1"
      },
      "source": [
        "### Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6UV8ISIYIAW"
      },
      "source": [
        "#### Version 1: Averaging Past Context with for loops\n",
        "The Weakest Form of Aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABnAn0DzxeId"
      },
      "source": [
        "##### The Mathematical Trick in Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lo9hR8OCxhUx",
        "outputId": "b1660d1f-5f43-443e-db8f-f696e801ec60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\na mathematical trick that is used in the Self-Attention inside a Transformer \\nand is really just like at the heart of an efficient implementation \\nof Self-Attention\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "\"\"\"\n",
        "a mathematical trick that is used in the Self-Attention inside a Transformer \n",
        "and is really just like at the heart of an efficient implementation \n",
        "of Self-Attention\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH93Myozxp6e",
        "outputId": "8b32ae68-04b2-457a-9e5c-40ea438991dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can \n",
        "# be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "\n",
        "B,T,C = 4,8,2 \n",
        "# batch, time, channels - we have some information at each point\n",
        "# in the sequence\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "TEPqP8K2yRXG",
        "outputId": "1743f8f5-5a9c-4b1e-e1f8-3c3a784df4c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nnow what we would like to do is \\nwe would like these um tokens \\nso we have up to eight tokens here in a batch and these\\neight tokens are currently not talking to each other and \\nwe would like them to talk to each other \\nwe'd like to couple them\\n\\nthe token for example at the fifth location \\nit should not communicate with tokens in the sixth seventh and eighth location \\nbecause those are future tokens in the sequence \\nthe token on the fifth location should only talk to \\nthe one in the fourth third second and first\\n\\nso it's only so information only flows from previous context \\nto the current timestamp and \\nwe cannot get any information from the future because \\nwe are about to try to predict the future\\n\\nso what is the easiest way for tokens to communicate \\nokay the easiest way I would say is okay if we are up to if we're a\\nfifth token and I'd like to communicate with my past \\nthe simplest way we can do that is to just do a weight is to just\\ndo an AVERAGE OF ALL THE PRECEDING ELEMENTS\\n\\nso for example if I'm the fifth token \\nI would like to take the channels that make up that are information at my step \\nbut then also the channels from the four step, third step, second step \\nand the first step \\nI'd like to average those up and then \\nthat would become sort of like a feature vector \\nthat summarizes me in the context of my history\\n\\nnow of course just doing a sum or like an average is an extremely weak form of\\ninteraction \\nlike this communication is extremely lossy \\nwe've lost a ton of information about the spatial arrangements of \\nall those tokens \\nbut that's okay for now \\n\\nwe'll see how we can bring that information back later\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "\"\"\"\n",
        "now what we would like to do is \n",
        "we would like these um tokens \n",
        "so we have up to eight tokens here in a batch and these\n",
        "eight tokens are currently not talking to each other and \n",
        "we would like them to talk to each other \n",
        "we'd like to couple them\n",
        "\n",
        "the token for example at the fifth location \n",
        "it should not communicate with tokens in the sixth seventh and eighth location \n",
        "because those are future tokens in the sequence \n",
        "the token on the fifth location should only talk to \n",
        "the one in the fourth third second and first\n",
        "\n",
        "so it's only so information only flows from previous context \n",
        "to the current timestamp and \n",
        "we cannot get any information from the future because \n",
        "we are about to try to predict the future\n",
        "\n",
        "so what is the easiest way for tokens to communicate \n",
        "okay the easiest way I would say is okay if we are up to if we're a\n",
        "fifth token and I'd like to communicate with my past \n",
        "the simplest way we can do that is to just do a weight is to just\n",
        "do an AVERAGE OF ALL THE PRECEDING ELEMENTS\n",
        "\n",
        "so for example if I'm the fifth token \n",
        "I would like to take the channels that make up that are information at my step \n",
        "but then also the channels from the four step, third step, second step \n",
        "and the first step \n",
        "I'd like to average those up and then \n",
        "that would become sort of like a feature vector \n",
        "that summarizes me in the context of my history\n",
        "\n",
        "now of course just doing a sum or like an average is an extremely weak form of\n",
        "interaction \n",
        "like this communication is extremely lossy \n",
        "we've lost a ton of information about the spatial arrangements of \n",
        "all those tokens \n",
        "but that's okay for now \n",
        "\n",
        "we'll see how we can bring that information back later\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KhwkobsHyRVC",
        "outputId": "0167c57c-eb3b-4bc9-9b3c-88b1a52b7163"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor now what we would like to do is\\nfor every single batch element independently \\nfor every teeth token in that sequence\\nwe'd like to now calculate the average of all the vectors in all the previous\\ntokens and also at this token\\n\\nso let's write that out\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "\"\"\"\n",
        "for now what we would like to do is\n",
        "for every single batch element independently \n",
        "for every teeth token in that sequence\n",
        "we'd like to now calculate the average of all the vectors in all the previous\n",
        "tokens and also at this token\n",
        "\n",
        "so let's write that out\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "D50dRlURyRSl"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\"\"\"\n",
        "bow = bag of words\n",
        "\n",
        "kind of like um a term that people use when you are just\n",
        "averaging up things so it's just a bag of words basically there's a \n",
        "word stored on every one of these eight locations\n",
        "and we're doing a bag of words such as averaging\n",
        "\n",
        "in the beginning we initialize at zero (torch.zeros)\n",
        "\"\"\"\n",
        "for b in range(B): # iterating over batch dimension\n",
        "    for t in range(T): # iterating over time dimension\n",
        "    \n",
        "        xprev = x[b,:t+1] # of shape (t,C) - t, how many elements in the past\n",
        "                          # C, all the 2D information fro mthese tokens\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "        \"\"\"\n",
        "        doing the average or the mean over the zeroth dimension \n",
        "        so I'm averaging out the time here\n",
        "\n",
        "        I'm just going to get a little C one-dimensional Vector which \n",
        "        I'm going to store in X background words\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2C2-CNLyRPj",
        "outputId": "2e237f50-7c22-42ad-ca3e-b81ad82f6414"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873],\n",
              "        [ 0.9007, -2.1055],\n",
              "        [ 0.6784, -1.2345],\n",
              "        [-0.0431, -1.6047],\n",
              "        [-0.7521,  1.6487],\n",
              "        [-0.3925, -1.4036],\n",
              "        [-0.7279, -0.5594],\n",
              "        [-0.7688,  0.7624]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "x[0] # the zeroth batch element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xDq5h5VyRMj",
        "outputId": "8569d763-efe9-4a16-bbd2-0ed7fa70bafc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873],\n",
              "        [ 1.4138, -0.3091],\n",
              "        [ 1.1687, -0.6176],\n",
              "        [ 0.8657, -0.8644],\n",
              "        [ 0.5422, -0.3617],\n",
              "        [ 0.3864, -0.5354],\n",
              "        [ 0.2272, -0.5388],\n",
              "        [ 0.1027, -0.3762]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "xbow[0] # the last row is an average of all the elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nGpPjiphyRI8",
        "outputId": "a1036640-f46d-48a0-bb20-703ac39c2f77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nyou see how the at the first location here you see that the two are equal and\\nthat's because it's we're just doing an average of this one token\\n\\nbut here this one is now an average of\\nthese two and now this one is an average of these three\\nand so on \\n\\nthis last one is the average of all of these elements so vertical\\naverage just averaging up all the tokens\\n\\nnow gives this outcome here\\nso this is all well and good but this is very inefficient\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "\"\"\"\n",
        "you see how the at the first location here you see that the two are equal and\n",
        "that's because it's we're just doing an average of this one token\n",
        "\n",
        "but here this one is now an average of\n",
        "these two and now this one is an average of these three\n",
        "and so on \n",
        "\n",
        "this last one is the average of all of these elements so vertical\n",
        "average just averaging up all the tokens\n",
        "\n",
        "now gives this outcome here\n",
        "so this is all well and good but this is very inefficient\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdTt333UiM1V"
      },
      "source": [
        "##### The Trick in Self-Attention: Matrix Multiply as Weighted Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EdzLIvljyRDU",
        "outputId": "9f363ff7-1fcb-40b0-89b2-a975bc086df8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nthe trick is that we can be very very efficient about\\ndoing this using matrix multiplication so that's the mathematical trick\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "\"\"\"\n",
        "the trick is that we can be very very efficient about\n",
        "doing this using matrix multiplication so that's the mathematical trick\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PykJbg-jk9tL",
        "outputId": "00fcb6fa-b40e-4e7b-cea3-f8ae661373ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=a*b\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be \n",
        "# used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.ones(3, 3) # 3x3 matrix\n",
        "b = torch.randint(0,10,(3,2)).float() # 3x2 matrix\n",
        "\n",
        "c = a @ b # (3x3) x (3x2) = 3x2 matrix\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('---')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('---')\n",
        "print('c=a*b')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kTU5Jh0ODjq3",
        "outputId": "d49924f2-ef36-44bc-c3b6-a37360c944c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nokay so how are these numbers in C achieved right \\n\\nso this number in the top of C\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "\"\"\"\n",
        "okay so how are these numbers in C achieved right \n",
        "\n",
        "so this number in the top of C\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aDU1DgQFPRN"
      },
      "source": [
        "###### Trick: torch.tril"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO1g-E4eEqet",
        "outputId": "7662b5e6-2c42-45ad-9b6f-086a85339efb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "\"\"\"\n",
        "now the trick here is uh the following this is just a boring number of\n",
        "um it's just a boring array of all ones but \n",
        "\n",
        "torch has this function called tril\n",
        "which is short for a triangular uh something like that and \n",
        "you can wrap it in torch.ones and \n",
        "it will just return the lower triangular portion of this\n",
        "\"\"\"\n",
        "\n",
        "torch.tril(torch.ones(3,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZNrVf99FEe0",
        "outputId": "7c0418ab-37bd-4770-8d66-1ab78bb1a94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=a*b\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be \n",
        "# used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones(3, 3)) # 3x3 matrix\n",
        "b = torch.randint(0,10,(3,2)).float() # 3x2 matrix\n",
        "\n",
        "c = a @ b # (3x3) x (3x2) = 3x2 matrix\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('---')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('---')\n",
        "print('c=a*b')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "qwSscDwcFNb5",
        "outputId": "76591ae4-d32b-4dec-a389-4dcccdb6fde9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nand so basically depending on how many ones and zeros \\nwe have here we are basically doing a sum currently of a\\nvariable number of these rows and that gets deposited into C\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "\"\"\"\n",
        "and so basically depending on how many ones and zeros \n",
        "we have here we are basically doing a sum currently of a\n",
        "variable number of these rows and that gets deposited into C\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y-Yi6_Fyk9qK",
        "outputId": "10b08503-b1fa-42b8-9c02-2e00d4ca5e5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDot Product\\nMatrix Multiplication\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "\"\"\"\n",
        "Dot Product\n",
        "Matrix Multiplication\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpcXzSq9GuYe"
      },
      "source": [
        "###### Trick: Divide by the Sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjHS_sQ0k9mE",
        "outputId": "16624cb6-00f6-441c-8522-5ce3248932f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=a*b\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be \n",
        "# used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('---')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('---')\n",
        "print('c=a*b')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tHck0B09k9fP",
        "outputId": "fb06f4cb-79fa-4100-fedb-829dbcb4933f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nthe trick here is uh the following this is just a boring number of\\num it's just a boring array of all ones but torch has this function called tril\\n\\nwhich is short for a triangular uh something like that and you can wrap\\nit in torched at once and it will just return \\nthe lower triangular portion of this\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "\"\"\"\n",
        "the trick here is uh the following this is just a boring number of\n",
        "um it's just a boring array of all ones but torch has this function called tril\n",
        "\n",
        "which is short for a triangular uh something like that and you can wrap\n",
        "it in torched at once and it will just return \n",
        "the lower triangular portion of this\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "JkZYtANTk9Wx",
        "outputId": "e3328e33-ef7b-4733-a8a1-5bfee8f5df69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nso basically depending on how many ones and zeros we have here we are basically doing a sum currently of a\\nvariable number of these rows and that gets deposited into C\\n\\nSo currently we're doing sums because\\nthese are ones but we can also do average right and you can start to see how we could do average of the rows of B\\nuh sort of in an incremental fashion\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "\"\"\"\n",
        "so basically depending on how many ones and zeros we have here we are basically doing a sum currently of a\n",
        "variable number of these rows and that gets deposited into C\n",
        "\n",
        "So currently we're doing sums because\n",
        "these are ones but we can also do average right and you can start to see how we could do average of the rows of B\n",
        "uh sort of in an incremental fashion\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfEgGnvCKDyW"
      },
      "source": [
        "#### Version 2: Using Matrix Multiply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "roSrItjypXHw",
        "outputId": "6ffa9c96-f1db-4dfd-f9dc-ceb1600e63b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nsee how we can vectorize this and make it much more efficient\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "\"\"\"\n",
        "see how we can vectorize this and make it much more efficient\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_nfCCgqvt4G",
        "outputId": "e8827237-450c-4afe-a8d0-a7ff165905a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "\n",
        "# wei = weights\n",
        "\"\"\"\n",
        "we are going to produce an array a but\n",
        "here I'm going to call it way short for weights but this is our \"a\" Matrix\n",
        "\n",
        "this is how much of every row we want to average up and it's going to \n",
        "be an average because you can see it in these rows sum to 1\n",
        "\"\"\"\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkULHXoFKMS6",
        "outputId": "7d60f2ef-55bc-4e9f-85c2-29641b8539a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "\"\"\"\n",
        "our b is going to be X\n",
        "\"\"\"\n",
        "xbow2 = wei @ x \n",
        "# wei is (T, T)\n",
        "# x is (B, T, C)\n",
        "# (T, T) @ (B, T, C) ----> (B, T, C)\n",
        "# it will create a batch Dimension here and this is a batch matrix multiply\n",
        "# and so it will apply this matrix multiplication in all \n",
        "# the batch elements in parallel\n",
        "# (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "\"\"\"\n",
        "and individually and then for each batch element there will \n",
        "be a T by T multiplying T by C exactly as we had\n",
        "below\n",
        "\"\"\"\n",
        "\n",
        "torch.allclose(xbow, xbow2) # will show True because they are the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI4-ziOxvtz1",
        "outputId": "2be8f0ba-c628-4a73-d428-f9f29ae171c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1.9269,  1.4873],\n",
              "         [ 1.4138, -0.3091],\n",
              "         [ 1.1687, -0.6176],\n",
              "         [ 0.8657, -0.8644],\n",
              "         [ 0.5422, -0.3617],\n",
              "         [ 0.3864, -0.5354],\n",
              "         [ 0.2272, -0.5388],\n",
              "         [ 0.1027, -0.3762]]),\n",
              " tensor([[ 1.9269,  1.4873],\n",
              "         [ 1.4138, -0.3091],\n",
              "         [ 1.1687, -0.6176],\n",
              "         [ 0.8657, -0.8644],\n",
              "         [ 0.5422, -0.3617],\n",
              "         [ 0.3864, -0.5354],\n",
              "         [ 0.2272, -0.5388],\n",
              "         [ 0.1027, -0.3762]]))"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "xbow[0], xbow2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "6rm4HP59vtw2",
        "outputId": "9dbe88d0-7428-47df-b6d1-c201b09a77bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nthe trick is we were able to use batched Matrix multiply \\nto do this uh aggregation really\\n\\nand it's weighted aggregation and the weights are specified in this T by T array\\n\\nand we're basically doing weighted sums and uh \\nthese weighted sums are according to the weights inside here (wei)\\n\\nthey take on sort of this triangular form\\nand so that means that a token at the T-th Dimension will only get uh sort of \\num information from the um tokens preceding it so \\n\\nthat's exactly what we want\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "\"\"\"\n",
        "the trick is we were able to use batched Matrix multiply \n",
        "to do this uh aggregation really\n",
        "\n",
        "and it's weighted aggregation and the weights are specified in this T by T array\n",
        "\n",
        "and we're basically doing weighted sums and uh \n",
        "these weighted sums are according to the weights inside here (wei)\n",
        "\n",
        "they take on sort of this triangular form\n",
        "and so that means that a token at the T-th Dimension will only get uh sort of \n",
        "um information from the um tokens preceding it so \n",
        "\n",
        "that's exactly what we want\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4pYJa8rN9mQ"
      },
      "source": [
        "#### Version 3: Adding Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1_YeYG5Ivtuf",
        "outputId": "eb479e5c-4793-4caa-b7a2-efef2b9be6a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfinally I would like to rewrite it in one more way\\nand we're going to see why that's useful\\n\\nthis is the third version and it's also identical to the first and second\\n\\nit uses Softmax\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "\"\"\"\n",
        "finally I would like to rewrite it in one more way\n",
        "and we're going to see why that's useful\n",
        "\n",
        "this is the third version and it's also identical to the first and second\n",
        "\n",
        "it uses Softmax\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzF_xBMmXWqS",
        "outputId": "44bd0ae2-eb79-4a2c-9553-af3250f970b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "# lower triangular matrix with all 1s\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "tril"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU1pDDK3XYkl",
        "outputId": "27ea9ea0-be00-41a8-ca38-c6aeea8b2215"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "# begins as all zeros\n",
        "wei = torch.zeros((T,T))\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS6QEU8rYilU",
        "outputId": "eb0d3598-fd87-4656-b5ba-74766f63093b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W25udp57YkF2",
        "outputId": "806680e7-944d-4da2-828e-29fa71f3c913"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "\"\"\"\n",
        "if I take a Softmax along every single row (since dim is -1)\n",
        "what is that going to do \n",
        "\n",
        "well softmax is um\n",
        "it's also like a normalization operation right and so spoiler \n",
        "alert you get the exact same Matrix\n",
        "\n",
        "in softmax we're going\n",
        "to exponentiate every single one of these and then we're going \n",
        "to divide by the sum\n",
        "\n",
        "this is also the uh the same way to produce this mask\n",
        "\"\"\"\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNf558-VvtsJ",
        "outputId": "4ae32453-0a0d-4340-d910-d913638b4806"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "\"\"\"\n",
        "now the reason that this is a bit more interesting and the reason \n",
        "we're going to end up using it and solve a tension\n",
        "is that these weights here begin uh with zero\n",
        "and \n",
        "\n",
        "you can think of this as like an interaction strength or like \n",
        "an affinity so basically it's telling us how much of\n",
        "each token from the past do we want to Aggregate and average up\n",
        "\"\"\"\n",
        "wei = torch.zeros((T,T))\n",
        "\n",
        "\"\"\"\n",
        "this line is saying tokens from the past cannot communicate by setting\n",
        "them to negative Infinity we're saying that we will not aggregate \n",
        "anything from those tokens\n",
        "\n",
        "so basically this then goes through softmax\n",
        "\"\"\"\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "\"\"\"\n",
        "this is the aggregation through matrix multiplication\n",
        "\"\"\"\n",
        "xbow3 = wei @ x\n",
        "\n",
        "\n",
        "torch.allclose(xbow, xbow3) # both matrix are equivalent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "DMITLqLSvtpY",
        "outputId": "d7dc1c4e-b7bb-4037-bff6-0e7ef2821d50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nFocus on wei\\nso what this is now is you can think of these as um these zeros are currently \\njust set by\\nus to be zero but a quick preview is that these affinities between the tokens\\nare not going to be just constant at zero they're going to be data dependent \\nthese tokens are going to start looking\\nat each other and some tokens will find other tokens more or less interesting \\nand depending on what their values are\\nthey're going to find each other interesting to different amounts \\nand I'm going to call those affinities I think\\n\\nwei = wei.masked_fill(tril == 0, float('-inf'))\\nhere we are saying the future cannot communicate with the past \\nwe're going to clamp them\\n\\nthen when we normalize and sum we're going to aggregate sort of their values \\ndepending on how interesting they find\\neach other\\n\\nthat's the preview for Self-Attenton\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "\"\"\"\n",
        "Focus on wei\n",
        "so what this is now is you can think of these as um these zeros are currently \n",
        "just set by\n",
        "us to be zero but a quick preview is that these affinities between the tokens\n",
        "are not going to be just constant at zero they're going to be data dependent \n",
        "these tokens are going to start looking\n",
        "at each other and some tokens will find other tokens more or less interesting \n",
        "and depending on what their values are\n",
        "they're going to find each other interesting to different amounts \n",
        "and I'm going to call those affinities I think\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "here we are saying the future cannot communicate with the past \n",
        "we're going to clamp them\n",
        "\n",
        "then when we normalize and sum we're going to aggregate sort of their values \n",
        "depending on how interesting they find\n",
        "each other\n",
        "\n",
        "that's the preview for Self-Attenton\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8j8FBDEJH9x"
      },
      "source": [
        "###### No Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-k2C7ASJHF4",
        "outputId": "4467219c-3a9a-4e08-e41b-b91078fae5b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8m7S9vwJapp"
      },
      "source": [
        "#### TLDR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rn-nI6wnJWuM",
        "outputId": "54cafc67-6612-456c-94d2-4a9f8a427b48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbasically long story short from this entire section is that \\n\\nyou can do weighted aggregations of your past elements\\nby having by using matrix multiplication of a lower triangular fashion\\n\\nthen the elements here in the lower triangular part are telling you \\nhow much of each element fuses into this position\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "\"\"\"\n",
        "basically long story short from this entire section is that \n",
        "\n",
        "you can do weighted aggregations of your past elements\n",
        "by having by using matrix multiplication of a lower triangular fashion\n",
        "\n",
        "then the elements here in the lower triangular part are telling you \n",
        "how much of each element fuses into this position\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HIsw7WubXQ6"
      },
      "source": [
        "### Minor Code Cleanup\n",
        "- https://github.com/karpathy/ng-video-lecture/commit/8050fde82a6380f7f7b0645e3dec8a02c984ef47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "DDRc8klBvtkL",
        "outputId": "dcf76530-da57-4046-abc6-f89f299b1c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3886, val loss 4.3734\n",
            "step 300: train loss 2.5267, val loss 2.5399\n",
            "step 600: train loss 2.4998, val loss 2.5315\n",
            "step 900: train loss 2.4903, val loss 2.5085\n",
            "step 1200: train loss 2.4967, val loss 2.5128\n",
            "step 1500: train loss 2.4809, val loss 2.5020\n",
            "step 1800: train loss 2.4858, val loss 2.5149\n",
            "step 2100: train loss 2.4865, val loss 2.5000\n",
            "step 2400: train loss 2.4882, val loss 2.5127\n",
            "step 2700: train loss 2.5006, val loss 2.5117\n",
            "\n",
            "\n",
            "\n",
            "CExthantrid owindike on, ble\n",
            "\n",
            "HAPen bube d e.\n",
            "S:\n",
            "Ond my d?\n",
            "LUMuss ar hthar usqur, t. bar dilasoaten wice my.\n",
            "\n",
            "Hastacom o mup\n",
            "Yowhthetof isth ble mil; dilll,\n",
            "\n",
            "W:\n",
            "\n",
            "Yees, hein lat Hetidrovets, and Wh p.\n",
            "Gore y jomes l lind me l.\n",
            "MAshe cechiry ptupr aisspllwhy.\n",
            "Hurinde n Boopetelaves\n",
            "MPORIII od mothakleo Windo wh t eiibys woutit,\n",
            "\n",
            "Hive cend iend t so mower; te\n",
            "\n",
            "AN ad nterupt f s ar irist m:\n",
            "\n",
            "Thin maleronth,\n",
            "Mad\n",
            "RD:\n",
            "\n",
            "Whio myr f-bube!\n",
            "KENobuisarardsal this aresthidin couk ay aney Iry ts I fr t ce.\n",
            "J\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI ran this code it was giving me the train loss and val loss\\nand we see that we convert to somewhere around 2.5 with the bigram model \\n\\nand then here's\\nthe sample that we produced at the end and so we have everything packaged up in\\nthe script and we're in a good position now to iterate on this\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "# ================================================================\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# added gpu capability if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200\n",
        "\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "# ================================================================\n",
        "\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Read Data\n",
        "# ================================================================ \n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "# ================================================================\n",
        "\n",
        "# Encoder and Decoder\n",
        "# ================================================================ \n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "# ================================================================\n",
        "\n",
        "# Create Train and Test Splits\n",
        "# ================================================================\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# ================================================================\n",
        "\n",
        "# data loading - gets a batch of the inputs and targets\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "this context manager torch.nograd and this is just telling pytorch \n",
        "that everything that happens\n",
        "inside this function we will not call that backward on and \n",
        "\n",
        "so pytorch can be a\n",
        "lot more efficient with its memory use because it doesn't have to store all \n",
        "the intermediate variables because we're\n",
        "never going to call backward and \n",
        "so it can it can be a lot more memory efficient in that way\n",
        "\n",
        "a good practice to tell Pi torch when we don't intend to do back propagation\n",
        "\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    it averages up the loss over multiple batches \n",
        "    so in particular \n",
        "    we're going to iterate invalider times and \n",
        "    we're going to\n",
        "    basically get our loss and then we're going to get the average loss \n",
        "    for both splits and so this will be a lot less\n",
        "    noisy\n",
        "\n",
        "    when we call the estimate loss we're going to report the pretty\n",
        "    accurate train and validation loss\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval() # setting phases\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # setting phases\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next \n",
        "        # token from a lookup table\n",
        "        \"\"\"\n",
        "        I want to do is I don't want to actually create I want to create like a\n",
        "        level of interaction here where we don't directly go to the embedding \n",
        "        for the um logits but instead we go through this\n",
        "        intermediate phase because we're going to start making that bigger\n",
        "        \"\"\"\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "        # lm_head = language model head\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C), C = n_embd\n",
        "        \"\"\"\n",
        "        going to give us token embeddings\n",
        "\n",
        "        then to go from the token embeddings to the logits \n",
        "        we're going to need a linear layer so self.lm head let's call it\n",
        "        short for language modeling head is n linear \n",
        "        from an embed up to vocab size\n",
        "        \"\"\"\n",
        "\n",
        "        logits = self.lm_head(tok_emb) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# no need to pass vocab_size into the constructor, already defined as a\n",
        "# global variable\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "# ================================================================\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\"\"\"\n",
        "I ran this code it was giving me the train loss and val loss\n",
        "and we see that we convert to somewhere around 2.5 with the bigram model \n",
        "\n",
        "and then here's\n",
        "the sample that we produced at the end and so we have everything packaged up in\n",
        "the script and we're in a good position now to iterate on this\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nV6J5Z4dbk-"
      },
      "source": [
        "### Positional Encoding\n",
        "- https://github.com/karpathy/ng-video-lecture/commit/28e5fd789dc24231d6047f7a897e3b6aec95642a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "SKs2RSFSdzUP",
        "outputId": "06e9921c-0832-4b34-86bc-4c223d03684f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nnext up so far we've taken these in indices (idx) and \\nwe've encoded them based on the identity of the tokens inside idx\\nthe next thing that people very often do is that we're not just encoding the identity of these tokens but also their\\nposition\\n\\nwe're going to have a second position uh embedding table here\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "\"\"\"\n",
        "next up so far we've taken these in indices (idx) and \n",
        "we've encoded them based on the identity of the tokens inside idx\n",
        "the next thing that people very often do is that we're not just encoding the identity of these tokens but also their\n",
        "position\n",
        "\n",
        "we're going to have a second position uh embedding table here\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPpn4zTjpXDw",
        "outputId": "c7dde831-bf2a-42f2-ff14-5f9c229adae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4801, val loss 4.4801\n",
            "step 300: train loss 2.5404, val loss 2.5566\n",
            "step 600: train loss 2.5160, val loss 2.5335\n",
            "step 900: train loss 2.4967, val loss 2.5149\n",
            "step 1200: train loss 2.5106, val loss 2.5254\n",
            "step 1500: train loss 2.4853, val loss 2.5109\n",
            "step 1800: train loss 2.4966, val loss 2.5198\n",
            "step 2100: train loss 2.4949, val loss 2.5100\n",
            "step 2400: train loss 2.4937, val loss 2.5102\n",
            "step 2700: train loss 2.5040, val loss 2.5114\n",
            "\n",
            "\n",
            "\n",
            "CExthantrid owindikis s, bll\n",
            "\n",
            "HAPen bube t e.\n",
            "S:\n",
            "O:\n",
            "IS:\n",
            "Folatangs:\n",
            "Wanthar u qurthe. bar dilasoate awice my.\n",
            "\n",
            "Hastatom o mup\n",
            "Yowhthatof isth ble mil; dilll,\n",
            "\n",
            "W:\n",
            "\n",
            "Ye s, hain latisttid ov ts, and Wh pomano.\n",
            "Swanous l lind me l.\n",
            "MIshe ce hiry ptupr aisspllw y. w'stoul noroopetelaves\n",
            "Momy ll, d mothake o Windo wh t eiibys the m douris TENGByore s poo mo th; te\n",
            "\n",
            "AN ad nthrupt f s ar irist m:\n",
            "\n",
            "Thin maleronth, af Pre?\n",
            "\n",
            "Whio myr f-\n",
            "LI har,\n",
            "S:\n",
            "\n",
            "\n",
            "Thardsal this ghesthidin cour ay aney Iry ts I f my ce hy\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "# ================================================================\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# added gpu capability if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200\n",
        "\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "# ================================================================\n",
        "\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Read Data\n",
        "# ================================================================ \n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "# ================================================================\n",
        "\n",
        "# Encoder and Decoder\n",
        "# ================================================================ \n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "# ================================================================\n",
        "\n",
        "# Create Train and Test Splits\n",
        "# ================================================================\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# ================================================================\n",
        "\n",
        "# data loading - gets a batch of the inputs and targets\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "this context manager torch.nograd and this is just telling pytorch \n",
        "that everything that happens\n",
        "inside this function we will not call that backward on and \n",
        "\n",
        "so pytorch can be a\n",
        "lot more efficient with its memory use because it doesn't have to store all \n",
        "the intermediate variables because we're\n",
        "never going to call backward and \n",
        "so it can it can be a lot more memory efficient in that way\n",
        "\n",
        "a good practice to tell Pi torch when we don't intend to do back propagation\n",
        "\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    it averages up the loss over multiple batches \n",
        "    so in particular \n",
        "    we're going to iterate invalider times and \n",
        "    we're going to\n",
        "    basically get our loss and then we're going to get the average loss \n",
        "    for both splits and so this will be a lot less\n",
        "    noisy\n",
        "\n",
        "    when we call the estimate loss we're going to report the pretty\n",
        "    accurate train and validation loss\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval() # setting phases\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # setting phases\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next \n",
        "        # token from a lookup table\n",
        "        \"\"\"\n",
        "        I want to do is I don't want to actually create I want to create like a\n",
        "        level of interaction here where we don't directly go to the embedding \n",
        "        for the um logits but instead we go through this\n",
        "        intermediate phase because we're going to start making that bigger\n",
        "        \"\"\"\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# no need to pass vocab_size into the constructor, already defined as a\n",
        "# global variable\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "# ================================================================\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "I ran this code it was giving me the train loss and val loss\n",
        "and we see that we convert to somewhere around 2.5 with the bigram model \n",
        "\n",
        "and then here's\n",
        "the sample that we produced at the end and so we have everything packaged up in\n",
        "the script and we're in a good position now to iterate on this\n",
        "\"\"\"\n",
        "\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKZzIDhTZIBc"
      },
      "source": [
        "### The Crux: Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b8mAC0Z6Za0S",
        "outputId": "8a535114-9bca-485a-ce62-7763f45720a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwe're going to implement a small Self-Attention for a single individual Head \\nas they're called\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "\"\"\"\n",
        "we're going to implement a small Self-Attention for a single individual Head \n",
        "as they're called\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7S0i0P9fY3S"
      },
      "source": [
        "#### Version 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDcH-IG0bdtw",
        "outputId": "8d59b35c-0ba9-4a07-a913-6d2e55bfee00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 32])\n",
            "tensor([[[ 0.1808, -0.0700, -0.3596,  ..., -0.8016,  1.5236,  2.5086],\n",
            "         [-0.6631, -0.2513,  1.0101,  ...,  1.5333,  1.6097, -0.4032],\n",
            "         [-0.8345,  0.5978, -0.0514,  ..., -0.4370, -1.0012, -0.4094],\n",
            "         ...,\n",
            "         [-0.8961,  0.0662, -0.0563,  ...,  2.1382,  0.5114,  1.2191],\n",
            "         [ 0.1910, -0.3425,  1.7955,  ...,  0.3699, -0.5556, -0.3983],\n",
            "         [-0.5819, -0.2208,  0.0135,  ..., -1.9079, -0.5276,  1.0807]],\n",
            "\n",
            "        [[ 0.4562, -1.0917, -0.8207,  ...,  0.0512, -0.6576, -2.5729],\n",
            "         [ 0.0210,  1.0060, -1.2492,  ...,  0.7859, -1.1501,  1.3132],\n",
            "         [ 2.2007, -0.2195,  0.5427,  ..., -0.6445,  1.0834, -0.7995],\n",
            "         ...,\n",
            "         [ 0.3091,  1.1661, -2.1821,  ...,  0.6151,  0.6763,  0.6228],\n",
            "         [ 0.0943, -0.3156,  0.7850,  ..., -1.5735,  1.3876,  0.7251],\n",
            "         [ 0.6455, -0.3313, -1.0390,  ...,  0.0895, -0.3748, -0.4781]],\n",
            "\n",
            "        [[-0.6067,  1.8328,  0.2931,  ...,  1.0041,  0.8656,  0.1688],\n",
            "         [-0.2352, -0.2586,  0.0131,  ...,  0.6690,  0.7535, -0.5359],\n",
            "         [-1.0277,  0.5347, -0.7958,  ...,  1.0711,  0.4901, -0.4876],\n",
            "         ...,\n",
            "         [-0.6896, -0.7080, -0.3152,  ..., -2.0662, -1.1418, -0.1391],\n",
            "         [ 1.0827,  1.1522,  0.5198,  ...,  0.4970,  0.0585,  0.1033],\n",
            "         [ 0.0720,  1.1080,  0.7293,  ...,  0.3967, -0.9755,  0.5122]],\n",
            "\n",
            "        [[ 0.3330,  1.0995,  0.4034,  ...,  1.6634, -0.4718,  0.5857],\n",
            "         [-0.9579,  0.9435, -2.1992,  ..., -0.7296,  0.1653, -0.3390],\n",
            "         [ 1.5416,  1.0231,  1.3392,  ..., -0.0433, -0.2505, -0.7493],\n",
            "         ...,\n",
            "         [ 0.7450,  0.7170,  1.2668,  ...,  1.9359,  2.0350,  2.0187],\n",
            "         [ 0.0323, -0.6337,  0.2938,  ..., -0.3297, -0.0192,  0.9225],\n",
            "         [ 0.9187,  0.2998,  0.6106,  ...,  0.8282, -0.4826,  1.8330]]])\n"
          ]
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\"\"\"\n",
        "so we have 4 by 8 arrangement of tokens\n",
        "at each token is currently 32-dimensional\n",
        "\"\"\"\n",
        "\n",
        "print(x.shape)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCWPrt_4ar8Z",
        "outputId": "128d7f38-0bec-47a7-d7d9-b3e28a4bbb6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "tril"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB-SM3z8aGyZ",
        "outputId": "782e57d1-ae79-4faa-8075-e486d97b5f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 32])\n"
          ]
        }
      ],
      "source": [
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu08V5FucNky"
      },
      "source": [
        "#### Single Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2YEeMBFZE5l",
        "outputId": "4871ffee-3ed4-44cb-eb35-15b014883706"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "\n",
        "# communication/interaction happens now\n",
        "# -2 = second last dimension, -1 = last dimension\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A17a5cvZEzP",
        "outputId": "77b7f56e-836b-4fa7-b39e-914528fb1afe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PxkWdkiZEra",
        "outputId": "f586d757-5f64-4279-8041-2fd971c2f480"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# look at zeroth row\n",
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXip-tO6d343"
      },
      "source": [
        "#### Notes on Attention\n",
        "1. Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights\n",
        "2. There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "3. Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "4. In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "5. \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "6. \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayau1Zzrf8Tn"
      },
      "source": [
        "##### Note 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "xpeCdYcjbdrr"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "\n",
        "# try with and without scaling by head_size\n",
        "wei = q @ k.transpose(-2, -1) # * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-sG6V0sbdmv",
        "outputId": "f654b6db-4a0e-46b8-dbef-7d0df3727668"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.0449), tensor(1.0700), tensor(17.4690))"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "k.var(), q.var(), wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "q7k4su9mhKND"
      },
      "outputs": [],
      "source": [
        "# try with and without scaling by head_size\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vZVKwALhLwv",
        "outputId": "9b285764-afc0-4196-b33a-ab52be09e05e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.0449), tensor(1.0700), tensor(1.0918))"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "k.var(), q.var(), wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT2JSoRPpW8i",
        "outputId": "1481be1e-57a8-4089-b6c5-1defd826b89f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE-cLbS1f31n",
        "outputId": "cdbe5bf3-6366-4d18-a111-af4b78b2fc26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) \n",
        "# gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKRQ0LOhgeY4"
      },
      "source": [
        "### Inserting a Single Self-Attention Block to Our Network\n",
        "- https://github.com/karpathy/ng-video-lecture/commit/10024b146809927c603aef91e8646f2a65e659ca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "5OyaSzBmf49X",
        "outputId": "55ef0d78-1706-4e15-c302-19e3b69766cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2000, val loss 4.2047\n",
            "step 500: train loss 2.6911, val loss 2.7087\n",
            "step 1000: train loss 2.5196, val loss 2.5303\n",
            "step 1500: train loss 2.4775, val loss 2.4829\n",
            "step 2000: train loss 2.4408, val loss 2.4523\n",
            "step 2500: train loss 2.4272, val loss 2.4435\n",
            "step 3000: train loss 2.4130, val loss 2.4327\n",
            "step 3500: train loss 2.3956, val loss 2.4212\n",
            "step 4000: train loss 2.4041, val loss 2.3992\n",
            "step 4500: train loss 2.3980, val loss 2.4084\n",
            "\n",
            "Whent iknt,\n",
            "Thowi, ht son, bth\n",
            "\n",
            "Hiset bobe ale.\n",
            "S:\n",
            "O-' st dalilanss:\n",
            "Want he us he, vet?\n",
            "Wedilas ate awice my.\n",
            "\n",
            "HDET:\n",
            "ANGo oug\n",
            "Yowhavetof is he ot mil ndill, aes iree sen cie lat Herid ovets, and Win ngarigoerabous lelind peal.\n",
            "-hule onchiry ptugr aiss hew ye wllinde norod atelaves\n",
            "Momy yowod mothake ont-wou whth eiiby we ati dourive wee, ired thoouso er; th\n",
            "To kad nteruptef so;\n",
            "ARID Wam:\n",
            "ENGCI inleront ffaf Pre?\n",
            "\n",
            "Wh om.\n",
            "\n",
            "He-\n",
            "LIERCKENIGUICar adsal aces ard thinin cour ay aney Iry ts I fr af ve y\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI ran this code it was giving me the train loss and val loss\\nand we see that we convert to somewhere around 2.5 with the bigram model \\n\\nand then here's\\nthe sample that we produced at the end and so we have everything packaged up in\\nthe script and we're in a good position now to iterate on this\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "# ================================================================\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 # 3000\n",
        "eval_interval = 500 # 300\n",
        "learning_rate = 1e-3 # 1e-2\n",
        "\n",
        "# added gpu capability if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200\n",
        "\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "# ================================================================\n",
        "\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Read Data\n",
        "# ================================================================ \n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "# ================================================================\n",
        "\n",
        "# Encoder and Decoder\n",
        "# ================================================================ \n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "# ================================================================\n",
        "\n",
        "# Create Train and Test Splits\n",
        "# ================================================================\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# ================================================================\n",
        "\n",
        "# data loading - gets a batch of the inputs and targets\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "this context manager torch.nograd and this is just telling pytorch \n",
        "that everything that happens\n",
        "inside this function we will not call that backward on and \n",
        "\n",
        "so pytorch can be a\n",
        "lot more efficient with its memory use because it doesn't have to store all \n",
        "the intermediate variables because we're\n",
        "never going to call backward and \n",
        "so it can it can be a lot more memory efficient in that way\n",
        "\n",
        "a good practice to tell Pi torch when we don't intend to do back propagation\n",
        "\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    it averages up the loss over multiple batches \n",
        "    so in particular \n",
        "    we're going to iterate invalider times and \n",
        "    we're going to\n",
        "    basically get our loss and then we're going to get the average loss \n",
        "    for both splits and so this will be a lot less\n",
        "    noisy\n",
        "\n",
        "    when we call the estimate loss we're going to report the pretty\n",
        "    accurate train and validation loss\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval() # setting phases\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # setting phases\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        \"\"\"\n",
        "        I want to do is I don't want to actually create I want to create like a\n",
        "        level of interaction here where we don't directly go to the embedding \n",
        "        for the um logits but instead we go through this\n",
        "        intermediate phase because we're going to start making that bigger\n",
        "        \"\"\"\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.sa_head(x) # apply one head of self-attention (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# no need to pass vocab_size into the constructor, already defined as a\n",
        "# global variable\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "# ================================================================\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\"\"\"\n",
        "I ran this code it was giving me the train loss and val loss\n",
        "and we see that we convert to somewhere around 2.5 with the bigram model \n",
        "\n",
        "and then here's\n",
        "the sample that we produced at the end and so we have everything packaged up in\n",
        "the script and we're in a good position now to iterate on this\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP33-h7hp7bS"
      },
      "source": [
        "#### No Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqGf9Zuaf46I",
        "outputId": "98e8e300-8a78-46a6-c916-1021a51c5f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2000, val loss 4.2047\n",
            "step 500: train loss 2.6911, val loss 2.7087\n",
            "step 1000: train loss 2.5196, val loss 2.5303\n",
            "step 1500: train loss 2.4775, val loss 2.4829\n",
            "step 2000: train loss 2.4408, val loss 2.4523\n",
            "step 2500: train loss 2.4272, val loss 2.4435\n",
            "step 3000: train loss 2.4130, val loss 2.4327\n",
            "step 3500: train loss 2.3956, val loss 2.4212\n",
            "step 4000: train loss 2.4041, val loss 2.3992\n",
            "step 4500: train loss 2.3980, val loss 2.4084\n",
            "\n",
            "Whent iknt,\n",
            "Thowi, ht son, bth\n",
            "\n",
            "Hiset bobe ale.\n",
            "S:\n",
            "O-' st dalilanss:\n",
            "Want he us he, vet?\n",
            "Wedilas ate awice my.\n",
            "\n",
            "HDET:\n",
            "ANGo oug\n",
            "Yowhavetof is he ot mil ndill, aes iree sen cie lat Herid ovets, and Win ngarigoerabous lelind peal.\n",
            "-hule onchiry ptugr aiss hew ye wllinde norod atelaves\n",
            "Momy yowod mothake ont-wou whth eiiby we ati dourive wee, ired thoouso er; th\n",
            "To kad nteruptef so;\n",
            "ARID Wam:\n",
            "ENGCI inleront ffaf Pre?\n",
            "\n",
            "Wh om.\n",
            "\n",
            "He-\n",
            "LIERCKENIGUICar adsal aces ard thinin cour ay aney Iry ts I fr af ve y\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se8KKuICi1Ie"
      },
      "source": [
        "### Multi-Head Self-Attention\n",
        "- https://github.com/karpathy/ng-video-lecture/commit/a6e0bee43163848076df568eb78799d524306ac9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ng-ueG2Urt90",
        "outputId": "7fbbb785-27b3-4973-a6cc-843ba7d54654"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nso now we've implemented the scale.product attention \\nnow next up in the attention is all you need paper \\nthere's something called multi-head attention and what is multi-head attention \\n\\nit's just applying multiple attentions in parallel and \\nconcatenating the results\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "\"\"\"\n",
        "so now we've implemented the scale.product attention \n",
        "now next up in the attention is all you need paper \n",
        "there's something called multi-head attention and what is multi-head attention \n",
        "\n",
        "it's just applying multiple attentions in parallel and \n",
        "concatenating the results\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "0fRSpDu6gd47",
        "outputId": "0ec2a543-28f9-4b02-d9de-d0362a088bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2248, val loss 4.2250\n",
            "step 500: train loss 2.6663, val loss 2.6809\n",
            "step 1000: train loss 2.5107, val loss 2.5189\n",
            "step 1500: train loss 2.4394, val loss 2.4447\n",
            "step 2000: train loss 2.3769, val loss 2.3890\n",
            "step 2500: train loss 2.3459, val loss 2.3606\n",
            "step 3000: train loss 2.3163, val loss 2.3361\n",
            "step 3500: train loss 2.2867, val loss 2.3138\n",
            "step 4000: train loss 2.2861, val loss 2.2796\n",
            "step 4500: train loss 2.2692, val loss 2.2816\n",
            "\n",
            "Whent if bridcowilfakis s, bt madiret bobe to tarver-'t thealleauss:\n",
            "Want he us hat vet?\n",
            "Wedtlaccane awice my.\n",
            "\n",
            "HDY'n om oroug\n",
            "Youts, tof is heirt mil nowlit,\n",
            "Whiiree--viecin lat Het drov the and Wing.\n",
            "\n",
            "DWAFeransesel lind peall liser cochiry ptur; aiss hiwty. Huntike normopeeelave whomy.\n",
            "Whoulllelake ont---o whr Ceviby wey thour rive wees ime st so mo lif thure kadmn,\n",
            "Turt for are;\n",
            "Dor my monge inledooth, af Pre?\n",
            "\n",
            "WISo myay I sok!\n",
            "Whied is:\n",
            "Sadsal the E'd steruin cour ay andy I yous I frouf voul\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI ran this code it was giving me the train loss and val loss\\nand we see that we convert to somewhere around 2.5 with the bigram model \\n\\nand then here's\\nthe sample that we produced at the end and so we have everything packaged up in\\nthe script and we're in a good position now to iterate on this\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "# ================================================================\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 # 3000\n",
        "eval_interval = 500 # 300\n",
        "learning_rate = 1e-3 # 1e-2\n",
        "\n",
        "# added gpu capability if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200\n",
        "\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "# ================================================================\n",
        "\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Read Data\n",
        "# ================================================================ \n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "# ================================================================\n",
        "\n",
        "# Encoder and Decoder\n",
        "# ================================================================ \n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "# ================================================================\n",
        "\n",
        "# Create Train and Test Splits\n",
        "# ================================================================\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# ================================================================\n",
        "\n",
        "# data loading - gets a batch of the inputs and targets\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "this context manager torch.nograd and this is just telling pytorch \n",
        "that everything that happens\n",
        "inside this function we will not call that backward on and \n",
        "\n",
        "so pytorch can be a\n",
        "lot more efficient with its memory use because it doesn't have to store all \n",
        "the intermediate variables because we're\n",
        "never going to call backward and \n",
        "so it can it can be a lot more memory efficient in that way\n",
        "\n",
        "a good practice to tell Pi torch when we don't intend to do back propagation\n",
        "\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    it averages up the loss over multiple batches \n",
        "    so in particular \n",
        "    we're going to iterate invalider times and \n",
        "    we're going to\n",
        "    basically get our loss and then we're going to get the average loss \n",
        "    for both splits and so this will be a lot less\n",
        "    noisy\n",
        "\n",
        "    when we call the estimate loss we're going to report the pretty\n",
        "    accurate train and validation loss\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval() # setting phases\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # setting phases\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        \"\"\"\n",
        "        I want to do is I don't want to actually create I want to create like a\n",
        "        level of interaction here where we don't directly go to the embedding \n",
        "        for the um logits but instead we go through this\n",
        "        intermediate phase because we're going to start making that bigger\n",
        "        \"\"\"\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # self.sa_head = Head(n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4) \n",
        "        # i.e. 4 heads of 8-dimensional self-attention\n",
        "        \n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # x = self.sa_head(x) # apply one head of self-attention (B,T,C)\n",
        "        x = self.sa_heads(x) # apply one head of self-attention. (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# no need to pass vocab_size into the constructor, already defined as a\n",
        "# global variable\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "# ================================================================\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\"\"\"\n",
        "I ran this code it was giving me the train loss and val loss\n",
        "and we see that we convert to somewhere around 2.5 with the bigram model \n",
        "\n",
        "and then here's\n",
        "the sample that we produced at the end and so we have everything packaged up in\n",
        "the script and we're in a good position now to iterate on this\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYTvwS2tsM-w"
      },
      "source": [
        "#### No Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF10ol0ugd1F",
        "outputId": "766de3a4-f8f1-4eda-c402-784ce7913013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2248, val loss 4.2250\n",
            "step 500: train loss 2.6663, val loss 2.6809\n",
            "step 1000: train loss 2.5107, val loss 2.5189\n",
            "step 1500: train loss 2.4394, val loss 2.4447\n",
            "step 2000: train loss 2.3769, val loss 2.3890\n",
            "step 2500: train loss 2.3459, val loss 2.3606\n",
            "step 3000: train loss 2.3163, val loss 2.3361\n",
            "step 3500: train loss 2.2867, val loss 2.3138\n",
            "step 4000: train loss 2.2861, val loss 2.2796\n",
            "step 4500: train loss 2.2692, val loss 2.2816\n",
            "\n",
            "Whent if bridcowilfakis s, bt madiret bobe to tarver-'t thealleauss:\n",
            "Want he us hat vet?\n",
            "Wedtlaccane awice my.\n",
            "\n",
            "HDY'n om oroug\n",
            "Youts, tof is heirt mil nowlit,\n",
            "Whiiree--viecin lat Het drov the and Wing.\n",
            "\n",
            "DWAFeransesel lind peall liser cochiry ptur; aiss hiwty. Huntike normopeeelave whomy.\n",
            "Whoulllelake ont---o whr Ceviby wey thour rive wees ime st so mo lif thure kadmn,\n",
            "Turt for are;\n",
            "Dor my monge inledooth, af Pre?\n",
            "\n",
            "WISo myay I sok!\n",
            "Whied is:\n",
            "Sadsal the E'd steruin cour ay andy I yous I frouf voul\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # i.e. 4 heads of 8-dimensional self-attention\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.sa_heads(x) # apply one head of self-attention. (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "wilI1zCxgdxu",
        "outputId": "74c7ecdf-d191-42c8-f777-040f8ef3c913"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI ran the same thing and then we now get\\nthis down to 2.28 roughly and the output is still the generation is still not\\namazing but clearly the validation loss is improving because \\nwe were at 2.4 just now\\n\\nand so it helps to have multiple communication channels because \\nobviously these tokens have a lot to talk about\\nand they want to find the consonants the vowels they want to find the vowels \\njust from certain positions they want to find\\nany kinds of different things and so it helps \\nto create multiple independent channels of communication gather lots of\\ndifferent types of data and then \\ndecode the output now \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "\"\"\"\n",
        "I ran the same thing and then we now get\n",
        "this down to 2.28 roughly and the output is still the generation is still not\n",
        "amazing but clearly the validation loss is improving because \n",
        "we were at 2.4 just now\n",
        "\n",
        "and so it helps to have multiple communication channels because \n",
        "obviously these tokens have a lot to talk about\n",
        "and they want to find the consonants the vowels they want to find the vowels \n",
        "just from certain positions they want to find\n",
        "any kinds of different things and so it helps \n",
        "to create multiple independent channels of communication gather lots of\n",
        "different types of data and then \n",
        "decode the output now \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP2SqL9k0ssH"
      },
      "source": [
        "### Feed Forward Layers of Transformer Block\n",
        "- https://github.com/karpathy/ng-video-lecture/commit/97dd3f9dee3dbb6445adcddb527370dc76010e41"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "da1fHcmDgduc",
        "outputId": "3bf2e342-0bbd-42b0-aa0e-300a6d8d81e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ngoing back to the paper for a second of course I didn't explain\\nthis figure in full detail but we are starting to see some components of \\nwhat we've already implemented \\n\\nwe have the positional encodings, the token encodings that add \\nwe have the masked multi-headed attention implemented\\n\\nnow here's another multi-headed tension which is a cross attention \\nto an encoder which we haven't we're not going to implement in this\\ncase I'm going to come back to that later but \\n\\nI want you to notice that there's a feed forward part here and then \\nthis is grouped into a block that gets repeated again and again \\n\\nnow the feed forward part here is just a simple multi-layer perceptron\\num so the multi-headed so here position wise feed forward networks is just a\\nsimple little MLP \\n\\nso I want to start basically in a similar fashion also adding computation\\ninto the network and this computation is on the per node level\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "\"\"\"\n",
        "going back to the paper for a second of course I didn't explain\n",
        "this figure in full detail but we are starting to see some components of \n",
        "what we've already implemented \n",
        "\n",
        "we have the positional encodings, the token encodings that add \n",
        "we have the masked multi-headed attention implemented\n",
        "\n",
        "now here's another multi-headed tension which is a cross attention \n",
        "to an encoder which we haven't we're not going to implement in this\n",
        "case I'm going to come back to that later but \n",
        "\n",
        "I want you to notice that there's a feed forward part here and then \n",
        "this is grouped into a block that gets repeated again and again \n",
        "\n",
        "now the feed forward part here is just a simple multi-layer perceptron\n",
        "um so the multi-headed so here position wise feed forward networks is just a\n",
        "simple little MLP \n",
        "\n",
        "so I want to start basically in a similar fashion also adding computation\n",
        "into the network and this computation is on the per node level\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "UohocMiFgdq7",
        "outputId": "d192b767-9af0-4fba-fd4b-6dad0faab0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2022, val loss 4.2019\n",
            "step 500: train loss 2.6144, val loss 2.6230\n",
            "step 1000: train loss 2.4766, val loss 2.4768\n",
            "step 1500: train loss 2.3985, val loss 2.3938\n",
            "step 2000: train loss 2.3277, val loss 2.3451\n",
            "step 2500: train loss 2.2955, val loss 2.3156\n",
            "step 3000: train loss 2.2826, val loss 2.2922\n",
            "step 3500: train loss 2.2455, val loss 2.2727\n",
            "step 4000: train loss 2.2436, val loss 2.2459\n",
            "step 4500: train loss 2.2292, val loss 2.2417\n",
            "\n",
            "And they tridcowf,\n",
            "The lay ble\n",
            "bairet bube to tarvirt.\n",
            "\n",
            "MBRCELTUS:\n",
            "Far baparuus hith bubar dilth ane awith my.\n",
            "\n",
            "HDER:\n",
            "Ay onoth\n",
            "Yowns, to uit he cove lind lincaes if ees, hain lat Heacl ov the and to pomant.\n",
            "\n",
            "Wables lill dite litens;\n",
            "Honcelly:\n",
            "Augh aiss hit yevell nal nordopetelavle\n",
            "Momtell, demet aklloal-nou wher eiibys to th dour warce hidend to-LOR:\n",
            "Bhe the the danterth po so;\n",
            "Ang. Wam:\n",
            "\n",
            "EDI youled atw, af Pried my of.\n",
            "\n",
            "HKING ERCKH:\n",
            "Puis:\n",
            "Arost Waced and to din cour ay aney Rry to chan thour y\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI ran this code it was giving me the train loss and val loss\\nand we see that we convert to somewhere around 2.5 with the bigram model \\n\\nand then here's\\nthe sample that we produced at the end and so we have everything packaged up in\\nthe script and we're in a good position now to iterate on this\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "\"\"\"\n",
        "I took the code that we\n",
        "developed in this Jupiter notebook and I converted it to be a script and \n",
        "I'm doing this because I just want to\n",
        "simplify our intermediate work into just the final product that we have \n",
        "at this point\n",
        "\n",
        "bigram.py\n",
        "\n",
        "New additions\n",
        "1. Enabled gpu - run on cuda\n",
        "2. estimate_loss()\n",
        "3. model.eval(), model.train() phases\n",
        "4. @torch.no_grad() - \n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "# ================================================================\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 # 3000\n",
        "eval_interval = 500 # 300\n",
        "learning_rate = 1e-3 # 1e-2\n",
        "\n",
        "# added gpu capability if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200\n",
        "\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "# ================================================================\n",
        "\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Read Data\n",
        "# ================================================================ \n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "# ================================================================\n",
        "\n",
        "# Encoder and Decoder\n",
        "# ================================================================ \n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "# ================================================================\n",
        "\n",
        "# Create Train and Test Splits\n",
        "# ================================================================\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# ================================================================\n",
        "\n",
        "# data loading - gets a batch of the inputs and targets\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "this context manager torch.nograd and this is just telling pytorch \n",
        "that everything that happens\n",
        "inside this function we will not call that backward on and \n",
        "\n",
        "so pytorch can be a\n",
        "lot more efficient with its memory use because it doesn't have to store all \n",
        "the intermediate variables because we're\n",
        "never going to call backward and \n",
        "so it can it can be a lot more memory efficient in that way\n",
        "\n",
        "a good practice to tell Pi torch when we don't intend to do back propagation\n",
        "\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    it averages up the loss over multiple batches \n",
        "    so in particular \n",
        "    we're going to iterate invalider times and \n",
        "    we're going to\n",
        "    basically get our loss and then we're going to get the average loss \n",
        "    for both splits and so this will be a lot less\n",
        "    noisy\n",
        "\n",
        "    when we call the estimate loss we're going to report the pretty\n",
        "    accurate train and validation loss\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval() # setting phases\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # setting phases\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        \"\"\"\n",
        "        I want to do is I don't want to actually create I want to create like a\n",
        "        level of interaction here where we don't directly go to the embedding \n",
        "        for the um logits but instead we go through this\n",
        "        intermediate phase because we're going to start making that bigger\n",
        "        \"\"\"\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # self.sa_head = Head(n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4) \n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        # i.e. 4 heads of 8-dimensional self-attention\n",
        "        \n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # x = self.sa_head(x) # apply one head of self-attention (B,T,C)\n",
        "        x = self.sa_heads(x) # apply one head of self-attention. (B,T,C)\n",
        "        x = self.ffwd(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# no need to pass vocab_size into the constructor, already defined as a\n",
        "# global variable\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "# ================================================================\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\"\"\"\n",
        "I ran this code it was giving me the train loss and val loss\n",
        "and we see that we convert to somewhere around 2.5 with the bigram model \n",
        "\n",
        "and then here's\n",
        "the sample that we produced at the end and so we have everything packaged up in\n",
        "the script and we're in a good position now to iterate on this\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPiqGT2v5FNj"
      },
      "source": [
        "#### No Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOe4ML7cgdlx",
        "outputId": "70599d86-5e90-463a-d1f8-0e8a1c196bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2022, val loss 4.2019\n",
            "step 500: train loss 2.6144, val loss 2.6230\n",
            "step 1000: train loss 2.4766, val loss 2.4768\n",
            "step 1500: train loss 2.3985, val loss 2.3938\n",
            "step 2000: train loss 2.3277, val loss 2.3451\n",
            "step 2500: train loss 2.2955, val loss 2.3156\n",
            "step 3000: train loss 2.2826, val loss 2.2922\n",
            "step 3500: train loss 2.2455, val loss 2.2727\n",
            "step 4000: train loss 2.2436, val loss 2.2459\n",
            "step 4500: train loss 2.2292, val loss 2.2417\n",
            "\n",
            "And they tridcowf,\n",
            "The lay ble\n",
            "bairet bube to tarvirt.\n",
            "\n",
            "MBRCELTUS:\n",
            "Far baparuus hith bubar dilth ane awith my.\n",
            "\n",
            "HDER:\n",
            "Ay onoth\n",
            "Yowns, to uit he cove lind lincaes if ees, hain lat Heacl ov the and to pomant.\n",
            "\n",
            "Wables lill dite litens;\n",
            "Honcelly:\n",
            "Augh aiss hit yevell nal nordopetelavle\n",
            "Momtell, demet aklloal-nou wher eiibys to th dour warce hidend to-LOR:\n",
            "Bhe the the danterth po so;\n",
            "Ang. Wam:\n",
            "\n",
            "EDI youled atw, af Pried my of.\n",
            "\n",
            "HKING ERCKH:\n",
            "Puis:\n",
            "Arost Waced and to din cour ay aney Rry to chan thour y\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # i.e. 4 heads of 8-dimensional self-attention\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.sa_heads(x) # apply one head of self-attention. (B,T,C)\n",
        "        x = self.ffwd(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AECLdbZe5IUr"
      },
      "source": [
        "### Residual Connections\n",
        "- https://github.com/karpathy/ng-video-lecture/commit/5c3a2d299592603581129fdb14c5b06f5c50938c\n",
        "- https://arxiv.org/abs/1512.03385"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "J62xZhJ47qza",
        "outputId": "aa18805c-eddb-4bb6-b594-84f96a26ee39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwe're starting to actually get like a pretty deep neural net and \\ndeep neural Nets uh suffer from optimization issues and I\\nthink that's where we're kind of like slightly starting to run into \\n\\nso we need one more idea that we can borrow from\\nthe um Transformer paper to resolve those difficulties \\n\\nnow there are two optimizations that dramatically help\\nwith the depth of these networks and make sure \\nthat the networks remain optimizable \\n\\nlet's talk about the first one the first one in this diagram is \\nyou see this Arrow here and then this arrow and this Arrow those\\nare skip connections or sometimes called residual connections \\n\\nthey come from this paper uh the\\nprocedural learning form and recognition from about 2015. \\nhttps://arxiv.org/abs/1512.03385\\n\\nthat introduced the concept now \\nthese are basically what it means is you transform the data but then you have\\na skip connection with addition from the previous features \\nnow the way I like to visualize it that I prefer is the following \\nhere the computation happens from the top to bottom and\\nbasically you have this uh residual pathway and \\nyou are free to Fork off from the residual pathway perform some\\n\\ncomputation and then project back to the residual pathway via addition and so \\nyou go from the the inputs to the\\ntargets only the plus and plus and plus and the reason this is useful is \\nbecause during that propagation remember from\\nour micrograd video earlier addition distributes gradients equally to \\nboth of its branches that that fat as the input\\nand so the supervision or the gradients from the loss basically hop\\nthrough every addition node all the way to the input and then also Fork off into\\nthe residual blocks \\n\\nbut basically you have this gradient Super Highway \\nthat goes directly from\\nthe supervision all the way to the input, unimpeded and then \\nthese virtual blocks are usually initialized in the beginning\\nso they contribute very very little if anything to the residual pathway \\nthey are initialized that way so in the\\nbeginning they are sort of almost kind of like not there but then \\nduring the optimization they come online over time\\nand they start to contribute but at least at the initialization you can go\\nfrom directly supervision to the input gradient is unimpeded and \\njust close and then the blocks over time kick in and so\\nthat dramatically helps with the optimization \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "\"\"\"\n",
        "we're starting to actually get like a pretty deep neural net and \n",
        "deep neural Nets uh suffer from optimization issues and I\n",
        "think that's where we're kind of like slightly starting to run into \n",
        "\n",
        "so we need one more idea that we can borrow from\n",
        "the um Transformer paper to resolve those difficulties \n",
        "\n",
        "now there are two optimizations that dramatically help\n",
        "with the depth of these networks and make sure \n",
        "that the networks remain optimizable \n",
        "\n",
        "let's talk about the first one the first one in this diagram is \n",
        "you see this Arrow here and then this arrow and this Arrow those\n",
        "are skip connections or sometimes called residual connections \n",
        "\n",
        "they come from this paper uh the\n",
        "procedural learning form and recognition from about 2015. \n",
        "https://arxiv.org/abs/1512.03385\n",
        "\n",
        "that introduced the concept now \n",
        "these are basically what it means is you transform the data but then you have\n",
        "a skip connection with addition from the previous features \n",
        "now the way I like to visualize it that I prefer is the following \n",
        "here the computation happens from the top to bottom and\n",
        "basically you have this uh residual pathway and \n",
        "you are free to Fork off from the residual pathway perform some\n",
        "\n",
        "computation and then project back to the residual pathway via addition and so \n",
        "you go from the the inputs to the\n",
        "targets only the plus and plus and plus and the reason this is useful is \n",
        "because during that propagation remember from\n",
        "our micrograd video earlier addition distributes gradients equally to \n",
        "both of its branches that that fat as the input\n",
        "and so the supervision or the gradients from the loss basically hop\n",
        "through every addition node all the way to the input and then also Fork off into\n",
        "the residual blocks \n",
        "\n",
        "but basically you have this gradient Super Highway \n",
        "that goes directly from\n",
        "the supervision all the way to the input, unimpeded and then \n",
        "these virtual blocks are usually initialized in the beginning\n",
        "so they contribute very very little if anything to the residual pathway \n",
        "they are initialized that way so in the\n",
        "beginning they are sort of almost kind of like not there but then \n",
        "during the optimization they come online over time\n",
        "and they start to contribute but at least at the initialization you can go\n",
        "from directly supervision to the input gradient is unimpeded and \n",
        "just close and then the blocks over time kick in and so\n",
        "that dramatically helps with the optimization \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "MFPiN1lE5KJc",
        "outputId": "03f18e02-fb4d-4ab0-bd46-4922f52b633c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6328, val loss 4.6313\n",
            "step 500: train loss 2.3721, val loss 2.3673\n",
            "step 1000: train loss 2.2588, val loss 2.2626\n",
            "step 1500: train loss 2.1726, val loss 2.1961\n",
            "step 2000: train loss 2.1308, val loss 2.1718\n",
            "step 2500: train loss 2.0991, val loss 2.1479\n",
            "step 3000: train loss 2.0615, val loss 2.1318\n",
            "step 3500: train loss 2.0522, val loss 2.1113\n",
            "step 4000: train loss 2.0198, val loss 2.1015\n",
            "step 4500: train loss 1.9975, val loss 2.0942\n",
            "step 4999: train loss 1.9896, val loss 2.0735\n",
            "\n",
            "And they bridce.\n",
            "\n",
            "SOROLOUS:\n",
            "Ay, a selk of our tarther'ds me?\n",
            "That suard that us hath buby, dilay a endway, my feanstar, zoknow\n",
            "You some fuitio be this now\n",
            "Whige miseets, hein latisely movets, and the now on you muself in you liet uprecce in the priness him you lord.\n",
            "In Bookes, and whome:\n",
            "Whed moth?\n",
            "\n",
            "Ko Winso what eis as the modour fall ey, me sto-deal the the deard nubrupt for treagis! muft wity.\n",
            "\n",
            "MUENTIUS:\n",
            "Marred my of.\n",
            "\n",
            "HKING ESLOUKES:\n",
            "Wardads.\n",
            "Wice age, thisin cour a save\n",
            "Hiry the have for hi\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI ran this code it was giving me the train loss and val loss\\nand we see that we convert to somewhere around 2.5 with the bigram model \\n\\nand then here's\\nthe sample that we produced at the end and so we have everything packaged up in\\nthe script and we're in a good position now to iterate on this\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "\"\"\"\n",
        "I took the code that we\n",
        "developed in this Jupiter notebook and I converted it to be a script and \n",
        "I'm doing this because I just want to\n",
        "simplify our intermediate work into just the final product that we have \n",
        "at this point\n",
        "\n",
        "bigram.py\n",
        "\n",
        "New additions\n",
        "1. Enabled gpu - run on cuda\n",
        "2. estimate_loss()\n",
        "3. model.eval(), model.train() phases\n",
        "4. @torch.no_grad() - \n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "# ================================================================\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 # 3000\n",
        "eval_interval = 500 # 300\n",
        "learning_rate = 1e-3 # 1e-2\n",
        "\n",
        "# added gpu capability if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200\n",
        "\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "# ================================================================\n",
        "\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Read Data\n",
        "# ================================================================ \n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "# ================================================================\n",
        "\n",
        "# Encoder and Decoder\n",
        "# ================================================================ \n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "# ================================================================\n",
        "\n",
        "# Create Train and Test Splits\n",
        "# ================================================================\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# ================================================================\n",
        "\n",
        "# data loading - gets a batch of the inputs and targets\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "this context manager torch.nograd and this is just telling pytorch \n",
        "that everything that happens\n",
        "inside this function we will not call that backward on and \n",
        "\n",
        "so pytorch can be a\n",
        "lot more efficient with its memory use because it doesn't have to store all \n",
        "the intermediate variables because we're\n",
        "never going to call backward and \n",
        "so it can it can be a lot more memory efficient in that way\n",
        "\n",
        "a good practice to tell Pi torch when we don't intend to do back propagation\n",
        "\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    it averages up the loss over multiple batches \n",
        "    so in particular \n",
        "    we're going to iterate invalider times and \n",
        "    we're going to\n",
        "    basically get our loss and then we're going to get the average loss \n",
        "    for both splits and so this will be a lot less\n",
        "    noisy\n",
        "\n",
        "    when we call the estimate loss we're going to report the pretty\n",
        "    accurate train and validation loss\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval() # setting phases\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # setting phases\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        \"\"\"\n",
        "        I want to do is I don't want to actually create I want to create like a\n",
        "        level of interaction here where we don't directly go to the embedding \n",
        "        for the um logits but instead we go through this\n",
        "        intermediate phase because we're going to start making that bigger\n",
        "        \"\"\"\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # self.sa_head = Head(n_embd)\n",
        "        # self.sa_heads = MultiHeadAttention(4, n_embd//4) \n",
        "        # self.ffwd = FeedFoward(n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "        # i.e. 4 heads of 8-dimensional self-attention\n",
        "        \n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # x = self.sa_head(x) # apply one head of self-attention (B,T,C)\n",
        "        # x = self.sa_heads(x) # apply one head of self-attention. (B,T,C)\n",
        "        # x = self.ffwd(x) # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# no need to pass vocab_size into the constructor, already defined as a\n",
        "# global variable\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "# ================================================================\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\"\"\"\n",
        "I ran this code it was giving me the train loss and val loss\n",
        "and we see that we convert to somewhere around 2.5 with the bigram model \n",
        "\n",
        "and then here's\n",
        "the sample that we produced at the end and so we have everything packaged up in\n",
        "the script and we're in a good position now to iterate on this\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uSK3FvD928Z"
      },
      "source": [
        "#### No Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B9HLE5K5KBK",
        "outputId": "19673a02-1d3f-4121-876d-6063823575d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6328, val loss 4.6313\n",
            "step 500: train loss 2.3721, val loss 2.3673\n",
            "step 1000: train loss 2.2588, val loss 2.2626\n",
            "step 1500: train loss 2.1726, val loss 2.1961\n",
            "step 2000: train loss 2.1308, val loss 2.1718\n",
            "step 2500: train loss 2.0991, val loss 2.1479\n",
            "step 3000: train loss 2.0615, val loss 2.1318\n",
            "step 3500: train loss 2.0522, val loss 2.1113\n",
            "step 4000: train loss 2.0198, val loss 2.1015\n",
            "step 4500: train loss 1.9975, val loss 2.0942\n",
            "step 4999: train loss 1.9896, val loss 2.0735\n",
            "\n",
            "And they bridce.\n",
            "\n",
            "SOROLOUS:\n",
            "Ay, a selk of our tarther'ds me?\n",
            "That suard that us hath buby, dilay a endway, my feanstar, zoknow\n",
            "You some fuitio be this now\n",
            "Whige miseets, hein latisely movets, and the now on you muself in you liet uprecce in the priness him you lord.\n",
            "In Bookes, and whome:\n",
            "Whed moth?\n",
            "\n",
            "Ko Winso what eis as the modour fall ey, me sto-deal the the deard nubrupt for treagis! muft wity.\n",
            "\n",
            "MUENTIUS:\n",
            "Marred my of.\n",
            "\n",
            "HKING ESLOUKES:\n",
            "Wardads.\n",
            "Wice age, thisin cour a save\n",
            "Hiry the have for hi\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zosx7BXC6IYx"
      },
      "source": [
        "### Layer Normalization\n",
        "- https://arxiv.org/abs/1607.06450\n",
        "\n",
        "Implemented in PyTorch\n",
        "- https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "WbB0MWzh6H9K",
        "outputId": "af437414-c194-4372-ec9a-73b5b91f35c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nlayer Norm is very very similar to batch norm \\n\\nso remember back to our make more series part three we implemented\\nbatch normalization and batch normalization basically just made sure that across the batch\\nDimension any individual neuron had unit gaussian\\ndistribution so it was zero mean and unit standard deviation one standard deviation output\\nso what I did here is I'm copy pasting The Bachelor 1D that we developed in our makemore series\\nand see here we can initialize for example this module and we can have a batch of 32 100 dimensional vectors\\nfeeding through the bathroom layer so what this does is it guarantees\\nthat when we look at just the zeroth column it's a zero mean one standard deviation\\nso it's normalizing every single column of this input now the rows are not going to be\\nnormalized by default because we're just normalizing columns so let's now implement the layer Norm uh it's very\\ncomplicated look we come here we change this from 0 to 1. so we don't normalize\\nThe Columns we normalize the rows and now we've implemented layer Norm\\nso now the columns are not going to be normalized but the rows are going to be normalized\\nfor every individual example it's 100 dimensional Vector is normalized in this way and because our computation Now does\\nnot span across examples we can delete all of this buffers stuff because we can\\nalways apply this operation and don't need to maintain any running buffers so\\nwe don't need the buffers we don't There's no distinction between\\ntraining and test time and we don't need these running buffers we do keep gamma and beta we don't need\\nthe momentum we don't care if it's training or not and this is now a layer Norm\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "\"\"\"\n",
        "layer Norm is very very similar to batch norm \n",
        "\n",
        "so remember back to our make more series part three we implemented\n",
        "batch normalization and batch normalization basically just made sure that across the batch\n",
        "Dimension any individual neuron had unit gaussian\n",
        "distribution so it was zero mean and unit standard deviation one standard deviation output\n",
        "so what I did here is I'm copy pasting The Bachelor 1D that we developed in our makemore series\n",
        "and see here we can initialize for example this module and we can have a batch of 32 100 dimensional vectors\n",
        "feeding through the bathroom layer so what this does is it guarantees\n",
        "that when we look at just the zeroth column it's a zero mean one standard deviation\n",
        "so it's normalizing every single column of this input now the rows are not going to be\n",
        "normalized by default because we're just normalizing columns so let's now implement the layer Norm uh it's very\n",
        "complicated look we come here we change this from 0 to 1. so we don't normalize\n",
        "The Columns we normalize the rows and now we've implemented layer Norm\n",
        "so now the columns are not going to be normalized but the rows are going to be normalized\n",
        "for every individual example it's 100 dimensional Vector is normalized in this way and because our computation Now does\n",
        "not span across examples we can delete all of this buffers stuff because we can\n",
        "always apply this operation and don't need to maintain any running buffers so\n",
        "we don't need the buffers we don't There's no distinction between\n",
        "training and test time and we don't need these running buffers we do keep gamma and beta we don't need\n",
        "the momentum we don't care if it's training or not and this is now a layer Norm\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQfBzWbgAxku"
      },
      "source": [
        "#### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFM562Dq6H5w",
        "outputId": "d5966528-9984-430e-9f1e-4fa048225d89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "\"\"\"\n",
        "Implement Layer Normalization\n",
        "\"\"\"\n",
        "\n",
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "VK-OUP4sTetf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de4c752-7f63-4df7-b372-ddfad4555412"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "hiwLm-GgTf6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757ea822-fdcf-4667-9cfc-22cbbf4aa276"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVMhwZPPA3Qy"
      },
      "source": [
        "#### Note"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "2C1r6lAwAdT-",
        "outputId": "f6e5bc3e-36fc-4afb-cc01-89d430a3d33a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nbefore I incorporate the layer Norm I just wanted to note that as\\nI said very few details about the Transformer have changed in the \\nlast five years but this is actually something that slightly departs from the\\noriginal paper you see that the ADD and Norm is applied after the transformation\\n\\nbut um in now it is a bit more basically common to apply the layer Norm before\\nthe transformation \\n\\nso there's a reshuffling of the layer Norms uh so \\nthis is called the pre-norm formulation\\nand that's the one that we're going to implement as well so slight deviation \\nfrom the original paper \\n\\nbasically we need two layer Norms layer\\nNorm one is an N dot layer norm and we tell it how many\\num what is the embedding dimension and we need \\nthe second layer Norm and then here the layer rooms are\\napplied immediately on x so self.layer number one in applied on x\\nand salt on layer number two applied on X before it goes into sulfur tension \\nand feed forward\\nand the size of the layer Norm here is an embeds of 32. so when the layer Norm\\nis normalizing our features it is the normalization here\\nhappens the mean and the variance are taking over 32 numbers so the batch \\nand the time act as batch Dimensions both of\\nthem so this is kind of like a per token transformation that just normalizes the\\nfeatures and makes them a unit mean unit gaussian at initialization\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "\"\"\"\n",
        "before I incorporate the layer Norm I just wanted to note that as\n",
        "I said very few details about the Transformer have changed in the \n",
        "last five years but this is actually something that slightly departs from the\n",
        "original paper you see that the ADD and Norm is applied after the transformation\n",
        "\n",
        "but um in now it is a bit more basically common to apply the layer Norm before\n",
        "the transformation \n",
        "\n",
        "so there's a reshuffling of the layer Norms uh so \n",
        "this is called the pre-norm formulation\n",
        "and that's the one that we're going to implement as well so slight deviation \n",
        "from the original paper \n",
        "\n",
        "basically we need two layer Norms layer\n",
        "Norm one is an N dot layer norm and we tell it how many\n",
        "um what is the embedding dimension and we need \n",
        "the second layer Norm and then here the layer rooms are\n",
        "applied immediately on x so self.layer number one in applied on x\n",
        "and salt on layer number two applied on X before it goes into sulfur tension \n",
        "and feed forward\n",
        "and the size of the layer Norm here is an embeds of 32. so when the layer Norm\n",
        "is normalizing our features it is the normalization here\n",
        "happens the mean and the variance are taking over 32 numbers so the batch \n",
        "and the time act as batch Dimensions both of\n",
        "them so this is kind of like a per token transformation that just normalizes the\n",
        "features and makes them a unit mean unit gaussian at initialization\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "VFsm_OjKBN4H",
        "outputId": "1eb55b20-3b33-4fdf-87b0-06b0a48d0acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3072, val loss 4.3054\n",
            "step 500: train loss 2.3786, val loss 2.3743\n",
            "step 1000: train loss 2.2618, val loss 2.2563\n",
            "step 1500: train loss 2.1680, val loss 2.1906\n",
            "step 2000: train loss 2.1240, val loss 2.1591\n",
            "step 2500: train loss 2.0784, val loss 2.1302\n",
            "step 3000: train loss 2.0517, val loss 2.1186\n",
            "step 3500: train loss 2.0452, val loss 2.1062\n",
            "step 4000: train loss 2.0192, val loss 2.0996\n",
            "step 4500: train loss 1.9921, val loss 2.0934\n",
            "step 4999: train loss 1.9838, val loss 2.0680\n",
            "\n",
            "When before\n",
            "will aff Our felt maderen buber weranth the led?\n",
            "Thathe art this us hathert?\n",
            "F dilay anessway, my feanstal mzorn heavens, tof is heart milend lib,\n",
            "Whiire, sengein;\n",
            "Stistlidrevens, and the now on you mes liling me littise, oncely spear; ais allw you:\n",
            "That I mand and at gonour you, my thake onWindot her eignase, and dour was in him,\n",
            "And Long encore\n",
            "To king thrust for are\n",
            "grean whit, dy ale of whipfierr?\n",
            "\n",
            "KIS\n",
            "But Her, be!\n",
            "Athed is wards beaces and thising mustear tey Iry to chan you!\n",
            "An\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI ran this code it was giving me the train loss and val loss\\nand we see that we convert to somewhere around 2.5 with the bigram model \\n\\nand then here's\\nthe sample that we produced at the end and so we have everything packaged up in\\nthe script and we're in a good position now to iterate on this\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "\"\"\"\n",
        "I took the code that we\n",
        "developed in this Jupiter notebook and I converted it to be a script and \n",
        "I'm doing this because I just want to\n",
        "simplify our intermediate work into just the final product that we have \n",
        "at this point\n",
        "\n",
        "bigram.py\n",
        "\n",
        "New additions\n",
        "1. Enabled gpu - run on cuda\n",
        "2. estimate_loss()\n",
        "3. model.eval(), model.train() phases\n",
        "4. @torch.no_grad() - \n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "# ================================================================\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 # 3000\n",
        "eval_interval = 500 # 300\n",
        "learning_rate = 1e-3 # 1e-2\n",
        "\n",
        "# added gpu capability if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200\n",
        "\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "# ================================================================\n",
        "\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Read Data\n",
        "# ================================================================ \n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "# ================================================================\n",
        "\n",
        "# Encoder and Decoder\n",
        "# ================================================================ \n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "# ================================================================\n",
        "\n",
        "# Create Train and Test Splits\n",
        "# ================================================================\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# ================================================================\n",
        "\n",
        "# data loading - gets a batch of the inputs and targets\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "this context manager torch.nograd and this is just telling pytorch \n",
        "that everything that happens\n",
        "inside this function we will not call that backward on and \n",
        "\n",
        "so pytorch can be a\n",
        "lot more efficient with its memory use because it doesn't have to store all \n",
        "the intermediate variables because we're\n",
        "never going to call backward and \n",
        "so it can it can be a lot more memory efficient in that way\n",
        "\n",
        "a good practice to tell Pi torch when we don't intend to do back propagation\n",
        "\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    it averages up the loss over multiple batches \n",
        "    so in particular \n",
        "    we're going to iterate invalider times and \n",
        "    we're going to\n",
        "    basically get our loss and then we're going to get the average loss \n",
        "    for both splits and so this will be a lot less\n",
        "    noisy\n",
        "\n",
        "    when we call the estimate loss we're going to report the pretty\n",
        "    accurate train and validation loss\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval() # setting phases\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # setting phases\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        \"\"\"\n",
        "        I want to do is I don't want to actually create I want to create like a\n",
        "        level of interaction here where we don't directly go to the embedding \n",
        "        for the um logits but instead we go through this\n",
        "        intermediate phase because we're going to start making that bigger\n",
        "        \"\"\"\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # self.sa_head = Head(n_embd)\n",
        "        # self.sa_heads = MultiHeadAttention(4, n_embd//4) \n",
        "        # self.ffwd = FeedFoward(n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            nn.LayerNorm(n_embd),\n",
        "        )\n",
        "        # i.e. 4 heads of 8-dimensional self-attention\n",
        "        \n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # x = self.sa_head(x) # apply one head of self-attention (B,T,C)\n",
        "        # x = self.sa_heads(x) # apply one head of self-attention. (B,T,C)\n",
        "        # x = self.ffwd(x) # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# no need to pass vocab_size into the constructor, already defined as a\n",
        "# global variable\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "# ================================================================\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\"\"\"\n",
        "I ran this code it was giving me the train loss and val loss\n",
        "and we see that we convert to somewhere around 2.5 with the bigram model \n",
        "\n",
        "and then here's\n",
        "the sample that we produced at the end and so we have everything packaged up in\n",
        "the script and we're in a good position now to iterate on this\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcLIUf4lRX9t"
      },
      "source": [
        "#### No Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPMPc-_ORXgA",
        "outputId": "bac910f9-b7cc-4f8e-9fb9-56e952ab6ba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3103, val loss 4.3100\n",
            "step 500: train loss 2.3808, val loss 2.3804\n",
            "step 1000: train loss 2.2503, val loss 2.2550\n",
            "step 1500: train loss 2.1566, val loss 2.1836\n",
            "step 2000: train loss 2.1200, val loss 2.1594\n",
            "step 2500: train loss 2.0701, val loss 2.1246\n",
            "step 3000: train loss 2.0397, val loss 2.1207\n",
            "step 3500: train loss 2.0342, val loss 2.1001\n",
            "step 4000: train loss 2.0054, val loss 2.0900\n",
            "step 4500: train loss 1.9905, val loss 2.0904\n",
            "step 4999: train loss 1.9755, val loss 2.0625\n",
            "\n",
            "And they bridce.\n",
            "\n",
            "SOROROTES:\n",
            "KING PANTIBbeed enaStirn'd the gatands:\n",
            "Wanther us heart. Wardethat ane away, my feanstatue of my\n",
            "Yout proof is heart milend lixcaes is ensen cin;\n",
            "Stiselid ove the the me now on that spelplind me litles;\n",
            "Honce by prupernisell why mold name.\n",
            "Book this down'd\n",
            "Is would thake of in on her eights would dour was genfience poor of his but that non this suke; ign the flow I male of whith Pried my of.\n",
            "\n",
            "HKING ESLA,\n",
            "So is wards.\n",
            "Wices a for hoppecs.\n",
            "\n",
            "DUKARD\n",
            "HIONG EDWARD VES:\n",
            "My\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            nn.LayerNorm(n_embd),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKpgtEsg6LS6"
      },
      "source": [
        "### Scaling Up the Model\n",
        "- https://github.com/karpathy/ng-video-lecture/commit/482b15d53a3a330e7515d11dfb9702839dd5f586\n",
        "\n",
        "Cosmetic Changes\n",
        "- n_layer\n",
        "- dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "81j6h2bP6H0S",
        "outputId": "9dfd19b0-816e-4e25-9285-1dbe73f543ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2846, val loss 4.2820\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-2999f1b2b094>\u001b[0m in \u001b[0;36m<cell line: 196>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Reference: Full Finished Code\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "1cd94095-5377-45db-fa19-b7ab9e082a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4109, val loss 4.4016\n",
            "step 100: train loss 2.6511, val loss 2.6604\n",
            "step 200: train loss 2.4941, val loss 2.4903\n",
            "step 300: train loss 2.3907, val loss 2.4023\n",
            "step 400: train loss 2.3215, val loss 2.3287\n",
            "step 500: train loss 2.2544, val loss 2.2712\n",
            "step 600: train loss 2.1955, val loss 2.2051\n",
            "step 700: train loss 2.1671, val loss 2.1900\n",
            "step 800: train loss 2.1254, val loss 2.1529\n",
            "step 900: train loss 2.0779, val loss 2.1124\n",
            "step 1000: train loss 2.0565, val loss 2.0908\n",
            "step 1100: train loss 2.0292, val loss 2.0837\n",
            "step 1200: train loss 1.9997, val loss 2.0475\n",
            "step 1300: train loss 1.9891, val loss 2.0360\n",
            "step 1400: train loss 1.9549, val loss 2.0084\n",
            "step 1500: train loss 1.9353, val loss 1.9998\n",
            "step 1600: train loss 1.9234, val loss 2.0112\n",
            "step 1700: train loss 1.9145, val loss 1.9927\n",
            "step 1800: train loss 1.8807, val loss 1.9749\n",
            "step 1900: train loss 1.8771, val loss 1.9624\n",
            "step 2000: train loss 1.8524, val loss 1.9698\n",
            "step 2100: train loss 1.8502, val loss 1.9672\n",
            "step 2200: train loss 1.8386, val loss 1.9458\n",
            "step 2300: train loss 1.8305, val loss 1.9370\n",
            "step 2400: train loss 1.8228, val loss 1.9239\n",
            "step 2500: train loss 1.7934, val loss 1.9252\n",
            "step 2600: train loss 1.7986, val loss 1.9141\n",
            "step 2700: train loss 1.7957, val loss 1.9235\n",
            "step 2800: train loss 1.7863, val loss 1.9141\n",
            "step 2900: train loss 1.7886, val loss 1.9130\n",
            "step 3000: train loss 1.7763, val loss 1.9035\n",
            "step 3100: train loss 1.7498, val loss 1.9052\n",
            "step 3200: train loss 1.7361, val loss 1.8927\n",
            "step 3300: train loss 1.7428, val loss 1.8894\n",
            "step 3400: train loss 1.7435, val loss 1.8769\n",
            "step 3500: train loss 1.7265, val loss 1.8786\n",
            "step 3600: train loss 1.7184, val loss 1.8727\n",
            "step 3700: train loss 1.7146, val loss 1.8644\n",
            "step 3800: train loss 1.7087, val loss 1.8701\n",
            "step 3900: train loss 1.7075, val loss 1.8600\n",
            "step 4000: train loss 1.7049, val loss 1.8481\n",
            "step 4100: train loss 1.7035, val loss 1.8551\n",
            "step 4200: train loss 1.6933, val loss 1.8482\n",
            "step 4300: train loss 1.6887, val loss 1.8284\n",
            "step 4400: train loss 1.6959, val loss 1.8485\n",
            "step 4500: train loss 1.6803, val loss 1.8392\n",
            "step 4600: train loss 1.6813, val loss 1.8215\n",
            "step 4700: train loss 1.6763, val loss 1.8283\n",
            "step 4800: train loss 1.6626, val loss 1.8313\n",
            "step 4900: train loss 1.6639, val loss 1.8246\n",
            "step 4999: train loss 1.6628, val loss 1.8183\n",
            "\n",
            "And they brince.\n",
            "\n",
            "SOTOLYCUS:\n",
            "King thou was doe.\n",
            "Stirt's the gate\n",
            "his raither us crown. Warwith.\n",
            "\n",
            "GLettrace, I knonst to zutom,\n",
            "You shaleful the covert; I long\n",
            "ef is ensen contlatistly; ov the does flong,\n",
            "Will abouself indeed up thus queen,\n",
            "You are night, why. Hust, sentrad, I lave whom\n",
            "the selmeth kleon your whrone in sound myself?\n",
            "Edween in From-of our forguate and thrught! I are grean us thee inlenout\n",
            "To fight?\n",
            "\n",
            "KING HENRY-BY:\n",
            "Hast you saw ads beaces and this have knear teyour-bust,\n",
            "And fove young to to Rome.\n",
            "And you haste?\n",
            " so a celtsenternius but with ready?\n",
            "\n",
            "HORTENSIO:\n",
            "You with own.\n",
            "\n",
            "Thund friences:\n",
            "Dects lome\n",
            "Is as detter injursseled Peed menoven:\n",
            "He passed out sight vit some stings his reagunet,\n",
            "Thy, come, against if his loudy forger may Greath,\n",
            "And that froving with some. Dove long made so lack.\n",
            "\n",
            "Privy, YORK:\n",
            "In amself.\n",
            "\n",
            "KING HENRY God, do this maid thee is is deter not the summe will there what thou to coust this belt marry sold;\n",
            "For for me what, thus wout is noth;\n",
            "you gonged where and thou softs,\n",
            "Whetwenn as humber the varth conneditanous,\n",
            "On bay his his well.\n",
            "First things sweet worting! You--\n",
            "And great fanty his cheas friend!\n",
            "\n",
            "BUCKINGIUS:\n",
            "You canience as seeks you begk to doth?\n",
            "Our my brancesing bromeng to-ladwere would in thas stroud's leste\n",
            "Shalf unletslow! thy colity not jurt deer not;\n",
            "Mush'st, and but bring nighth stid just torment.\n",
            "\n",
            "HEMBENMAMAREEL:\n",
            "Now they off, Cryblet to the lave.\n",
            "And I not; I they nend to me with tengly.\n",
            "Well, I isson eyet,\n",
            "But wouldly toesell smest one thou grantion,\n",
            "Was said,\n",
            "If but nake--a scace:\n",
            "Ourthreate this thee, whil\n",
            "what I breaks, yet with himse are.\n",
            "\n",
            "BUCKINGHARD\n",
            "OX RAHE:\n",
            "Unlards, unluntless shoulg, that becomfort,\n",
            "Musmaraster no, God the have bloy not,\n",
            "That be flouth for thy battiry,--\n",
            "Pilt, or mungles.\n",
            "\n",
            "PLAULEY:\n",
            "By where with fair?\n",
            "But I have burses it, if thel flight.\n",
            "Silf yours to the art; whult-or.\n",
            "\n",
            "COMINIUS:\n",
            "Whom the griavensent, Joanhalt.\n",
            "\n",
            "POMPEY:\n",
            "Whis Parpartion: I will down'd futh thee such more.\n",
            "\n",
            "MARCIUS:\n",
            "\n",
            "Marr\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) \n",
        "    # when device becomes cuda up then we need to make sure \n",
        "    # that when we load the  data we move it to device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # Corrected in the tutorial\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device) \n",
        "# when we create the model we want to move the model parameters to device\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# when I'm creating the context that feeds into generate \n",
        "# I have to make sure that I create on the device\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSzt1hn81Rzg"
      },
      "source": [
        "### Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BRmft0YsfS_"
      },
      "source": [
        "#### Encoder, Decoder, Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "dHnnA3QWsdUT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "01e1e3ed-d4e3-4f48-ce38-4c44135bd66c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnow the reason that the original paper had an Encoder-Decoder architecture is\\nbecause it is a machine translation paper so \\nit is concerned with a different setting \\nin particular it expects some tokens that encode say for example French\\nand then it is expected to decode the translation in English\\n\\ntypically these here are special tokens so you are expected \\nto read in this and condition on it and\\nthen you start off the generation with a special token called <START> \\nso this is a special new token that you introduce and\\nalways place in the beginning and then the network is expected \\nto output \"neural networks are awesome\" and then a\\nspecial <END> token to finish a generation\\n\\nso this part here will be decoded\\nexactly as we we\\'ve done it neural networks are awesome will \\nbe identical to what we did\\n\\nbut unlike what we did \\nthey want to condition the generation on some\\nadditional information and in that case \\nthis additional information is the French sentence \\nthat they should be translating\\n\\nso what they do now is \\nthey bring in the Encoder \\nnow the encoder reads this part here so \\nwe\\'re only going to take the part of French and \\nwe\\'re going to create tokens from it exactly as we\\'ve seen in our video and\\nwe\\'re going to put a Transformer on it but \\nthere\\'s going to be no triangular mask and so all the tokens are allowed\\nto talk to each other as much as they want and \\nthey\\'re just encoding whatever\\'s the content of this French sentence \\n\\nonce they\\'ve encoded it they\\'ve they basically come out in the\\ntop here and then what happens here is in our Decoder \\nwhich does the language modeling\\nthere\\'s an additional connection here to the outputs of the Encoder\\n\\nand that is brought in through a Cross-Attention \\nso the queries are still generated from X but now \\nthe keys and the values are coming from the side \\nthe keys and the values are coming from the top\\ngenerated by the nodes that came outside of the Encoder and \\nthose tops, the keys and the values\\nthere the top of it , feed it on the side \\ninto every single block of the decoder and so \\nthat\\'s why there\\'s an additional Cross-Attention\\n\\nand really what it\\'s doing is \\nit\\'s conditioning the decoding not just on\\nthe past of this current decoding \\nbut also on having seen the full \\nfully encoded French prompt sort of and so \\n\\nit\\'s an Encoder-Decoder model \\nwhich is why we have those two Transformers \\nan additional block and so on so \\nwe did not do this because we have\\nno we have nothing to encode there\\'s no conditioning \\nwe just have a text file and \\nwe just want to imitate it and \\nthat\\'s why we are using a decoder only\\nTransformer exactly as done in GPT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "\"\"\"\n",
        "what we implemented here is a Decoder-only Transformer \n",
        "so there's no component here \n",
        "this part is called the Encoder and \n",
        "there's no Cross-Attention block here\n",
        "\n",
        "our block only has a Self-Attention and the Feed Forward so it is missing this\n",
        "third in between piece here \n",
        "this piece does Cross-Attention \n",
        "so we don't have it\n",
        "\n",
        "and we don't have the Encoder \n",
        "we just have the Decoder and \n",
        "the reason we have a Dcoder-only\n",
        "is because we are just generating text and it's unconditioned on anything \n",
        "we're just we're just blabbering on according\n",
        "to a given data set \n",
        "\n",
        "what makes it a Decoder is that we are using the Triangular mask \n",
        "in our Transformer so \n",
        "it has this Auto regressive property \n",
        "where we can just go and sample from it\n",
        "\n",
        "so the fact that it's using the Triangular triangular mask \n",
        "to mask out the attention makes it a Decoder and it\n",
        "can be used for Language Modeling\n",
        "\"\"\"\n",
        "\n",
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n",
        "\"\"\"\n",
        "now the reason that the original paper had an Encoder-Decoder architecture is\n",
        "because it is a machine translation paper so \n",
        "it is concerned with a different setting \n",
        "in particular it expects some tokens that encode say for example French\n",
        "and then it is expected to decode the translation in English\n",
        "\n",
        "typically these here are special tokens so you are expected \n",
        "to read in this and condition on it and\n",
        "then you start off the generation with a special token called <START> \n",
        "so this is a special new token that you introduce and\n",
        "always place in the beginning and then the network is expected \n",
        "to output \"neural networks are awesome\" and then a\n",
        "special <END> token to finish a generation\n",
        "\n",
        "so this part here will be decoded\n",
        "exactly as we we've done it neural networks are awesome will \n",
        "be identical to what we did\n",
        "\n",
        "but unlike what we did \n",
        "they want to condition the generation on some\n",
        "additional information and in that case \n",
        "this additional information is the French sentence \n",
        "that they should be translating\n",
        "\n",
        "so what they do now is \n",
        "they bring in the Encoder \n",
        "now the encoder reads this part here so \n",
        "we're only going to take the part of French and \n",
        "we're going to create tokens from it exactly as we've seen in our video and\n",
        "we're going to put a Transformer on it but \n",
        "there's going to be no triangular mask and so all the tokens are allowed\n",
        "to talk to each other as much as they want and \n",
        "they're just encoding whatever's the content of this French sentence \n",
        "\n",
        "once they've encoded it they've they basically come out in the\n",
        "top here and then what happens here is in our Decoder \n",
        "which does the language modeling\n",
        "there's an additional connection here to the outputs of the Encoder\n",
        "\n",
        "and that is brought in through a Cross-Attention \n",
        "so the queries are still generated from X but now \n",
        "the keys and the values are coming from the side \n",
        "the keys and the values are coming from the top\n",
        "generated by the nodes that came outside of the Encoder and \n",
        "those tops, the keys and the values\n",
        "there the top of it , feed it on the side \n",
        "into every single block of the decoder and so \n",
        "that's why there's an additional Cross-Attention\n",
        "\n",
        "and really what it's doing is \n",
        "it's conditioning the decoding not just on\n",
        "the past of this current decoding \n",
        "but also on having seen the full \n",
        "fully encoded French prompt sort of and so \n",
        "\n",
        "it's an Encoder-Decoder model \n",
        "which is why we have those two Transformers \n",
        "an additional block and so on so \n",
        "we did not do this because we have\n",
        "no we have nothing to encode there's no conditioning \n",
        "we just have a text file and \n",
        "we just want to imitate it and \n",
        "that's why we are using a decoder only\n",
        "Transformer exactly as done in GPT\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDr5zX3D0Dbe"
      },
      "source": [
        "#### ChatGPT, GPT-3, Pre-training, Fine-tuning, RLHF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "Ezb_zAdKz2ax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "1166fcc9-a124-45aa-dd63-389797092b73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nso let's now bring things back to \\nChatGPT, GPT-3, Pretraining vs. Finetuning, RLHF\\n\\nwhat would it look like if we wanted to train ChatGPT ourselves and \\nhow does it relate to what we learned today\\n\\nwell to train in ChatGPT there are roughly two stages:\\n(1) Pre-training\\n(2) Fine-tuning\\n\\nFirst: Pre-training\\nIn the pre-training stage we are training on a large chunk of internet and \\njust trying to get a first Decoder-only Transformer to Babel text\\nso it's very very similar to what we've done ourselves \\nexcept we've done like a tiny little baby pre-training step and so \\nin our case uh this is how you print a number of parameters \\n\\nI printed it and it's about 10 million so this Transformer \\nthat I created here to create little Shakespeare um Transformer \\nwas about 10 million parameters \\nour data set is roughly 1 million uh characters \\nso roughly 1 million tokens \\nbut you have to remember that opening uses different vocabulary\\nthey're not on the Character level they use these um subword chunks of words and\\nso they have a vocabulary of 50,000 roughly elements and \\nso their sequences are a bit more condensed\\nso our data set the Shakespeare data set would be probably around 300,000 tokens\\nin the OpenAI vocabulary roughly so \\nwe trained about 10 million parameter\\nmodel and roughly 300,000 tokens \\n\\nwhen you go to the GPT-3 paper\\nand you look at the Transformers that they trained \\nthey trained a number of Transformers of\\ndifferent sizes but the biggest Transformer here has 175 billion parameters \\nso ours is again 10 million\\nthey used this number of layers in the Transformer \\nThis is the n embed \\nthis is the number of heads and this is\\nthe head size and then this is the batch size so ours was 65\\nand the learning rate is similar \\n\\nnow when they train this Transformer they trained on 300 billion tokens\\nso again remember ours is about 300,000 so this is uh about a million fold\\nincrease and this number would not be even that large by today's standards \\nyou'd be going up uh one trillion and\\nabove so they are training a significantly larger model\\non a good chunk of the internet and that is the pre-training stage \\n\\nbut otherwise\\nthese hyper parameters should be fairly recognizable to you and \\nthe architecture is actually like nearly identical to\\nwhat we implemented ourselves but \\nof course it's a massive infrastructure challenge to train this \\nyou're talking about typically thousands of gpus having to you know\\ntalk to each other \\nto train models of this size so that's just a pre-training stage \\n\\nnow after you complete the pre-training stage \\nyou don't get something that responds to\\nyour questions with answers and is not helpful and Etc \\nyou get a document completer right so it babbles \\nbut it doesn't babbles Shakespeare \\nit babbles the internet \\nit will create arbitrary news articles and documents and \\nit will try to complete documents because that's what it's trained for \\nit's trying to complete the sequence so \\nwhen you give it a question it would just uh\\npotentially just give you more questions \\nit would follow with more questions \\nit will do whatever it looks like the some closed document would do \\nin the training data on the internet and \\nso who knows you're getting kind of like undefined behavior \\nit might basically answer with two questions with other questions \\nit might ignore your question it might just\\ntry to complete some news article it's totally underlined as we say \\n\\nSecond: Fine-tuning\\nthe second fine tuning stage is to actually align it to be an assistant and \\nthis is the second stage and so this ChatGPT blog post \\nhttps://openai.com/blog/chatgpt\\nfrom OpenAI talks a little bit about how the stage is achieved \\n\\nwe basically um\\nthere's roughly three steps to  this stage\\n\\n(1) Fine-tuning\\nso what  they do here is they start to collect training data that\\nlooks specifically like what an assistant would do so \\nyou have documents that have the format where the question is on top and then \\nan answer is below and \\nthey have a large number of these but \\nprobably not on the order of the internet \\nthis is probably on the\\norder of maybe thousands of examples and so \\nthey then fine-tuned the model to basically only focus on documents \\nthat look like that and so\\nyou're starting to slowly align it so \\nit's going to expect a question at the top and \\nit's going to expect to complete the answer\\nand uh these very very large models are very sample efficient \\nduring their fine tuning so this actually somehow works\\nbut that's just step one that's just fine-tuning\\n\\n(2)\\nthen they actually have more steps where okay the second step is\\nyou let the model respond and then different Raters \\nlook at the different responses and rank them for their\\npreference as to which one is better than the other \\n\\nthey use that to train a reward model so they can predict\\nbasically using a different network \\nhow much of any candidate response would be\\ndesirable and then once they have a reward model they run PPO \\nwhich is a form of policy gradient reinforcement learning optimizer \\nto fine-tune this sampling policy so\\nthat the answers that  ChatGPT now generates, are expected to score a high\\nreward according to the reward model and so \\n\\nbasically there's a whole the alining stage here or fine-tuning stage\\nit's got multiple steps in between there as well and \\nit takes the model from  being a document completer to a question answerer and \\nthat's like a whole separate stage \\n\\na lot of this data is not available publicly it is internal to OpenAI and \\nit's much harder to replicate this stage um and \\n\\nso that's roughly what would give you a ChatGPT and \\n\\nnanoGPT focuses on the pre-training stage \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "\"\"\"\n",
        "so let's now bring things back to \n",
        "ChatGPT, GPT-3, Pretraining vs. Finetuning, RLHF\n",
        "\n",
        "what would it look like if we wanted to train ChatGPT ourselves and \n",
        "how does it relate to what we learned today\n",
        "\n",
        "well to train in ChatGPT there are roughly two stages:\n",
        "(1) Pre-training\n",
        "(2) Fine-tuning\n",
        "\n",
        "First: Pre-training\n",
        "In the pre-training stage we are training on a large chunk of internet and \n",
        "just trying to get a first Decoder-only Transformer to Babel text\n",
        "so it's very very similar to what we've done ourselves \n",
        "except we've done like a tiny little baby pre-training step and so \n",
        "in our case uh this is how you print a number of parameters \n",
        "\n",
        "I printed it and it's about 10 million so this Transformer \n",
        "that I created here to create little Shakespeare um Transformer \n",
        "was about 10 million parameters \n",
        "our data set is roughly 1 million uh characters \n",
        "so roughly 1 million tokens \n",
        "but you have to remember that opening uses different vocabulary\n",
        "they're not on the Character level they use these um subword chunks of words and\n",
        "so they have a vocabulary of 50,000 roughly elements and \n",
        "so their sequences are a bit more condensed\n",
        "so our data set the Shakespeare data set would be probably around 300,000 tokens\n",
        "in the OpenAI vocabulary roughly so \n",
        "we trained about 10 million parameter\n",
        "model and roughly 300,000 tokens \n",
        "\n",
        "when you go to the GPT-3 paper\n",
        "and you look at the Transformers that they trained \n",
        "they trained a number of Transformers of\n",
        "different sizes but the biggest Transformer here has 175 billion parameters \n",
        "so ours is again 10 million\n",
        "they used this number of layers in the Transformer \n",
        "This is the n embed \n",
        "this is the number of heads and this is\n",
        "the head size and then this is the batch size so ours was 65\n",
        "and the learning rate is similar \n",
        "\n",
        "now when they train this Transformer they trained on 300 billion tokens\n",
        "so again remember ours is about 300,000 so this is uh about a million fold\n",
        "increase and this number would not be even that large by today's standards \n",
        "you'd be going up uh one trillion and\n",
        "above so they are training a significantly larger model\n",
        "on a good chunk of the internet and that is the pre-training stage \n",
        "\n",
        "but otherwise\n",
        "these hyper parameters should be fairly recognizable to you and \n",
        "the architecture is actually like nearly identical to\n",
        "what we implemented ourselves but \n",
        "of course it's a massive infrastructure challenge to train this \n",
        "you're talking about typically thousands of gpus having to you know\n",
        "talk to each other \n",
        "to train models of this size so that's just a pre-training stage \n",
        "\n",
        "now after you complete the pre-training stage \n",
        "you don't get something that responds to\n",
        "your questions with answers and is not helpful and Etc \n",
        "you get a document completer right so it babbles \n",
        "but it doesn't babbles Shakespeare \n",
        "it babbles the internet \n",
        "it will create arbitrary news articles and documents and \n",
        "it will try to complete documents because that's what it's trained for \n",
        "it's trying to complete the sequence so \n",
        "when you give it a question it would just uh\n",
        "potentially just give you more questions \n",
        "it would follow with more questions \n",
        "it will do whatever it looks like the some closed document would do \n",
        "in the training data on the internet and \n",
        "so who knows you're getting kind of like undefined behavior \n",
        "it might basically answer with two questions with other questions \n",
        "it might ignore your question it might just\n",
        "try to complete some news article it's totally underlined as we say \n",
        "\n",
        "Second: Fine-tuning\n",
        "the second fine tuning stage is to actually align it to be an assistant and \n",
        "this is the second stage and so this ChatGPT blog post \n",
        "https://openai.com/blog/chatgpt\n",
        "from OpenAI talks a little bit about how the stage is achieved \n",
        "\n",
        "we basically um\n",
        "there's roughly three steps to  this stage\n",
        "\n",
        "(1) Fine-tuning\n",
        "so what  they do here is they start to collect training data that\n",
        "looks specifically like what an assistant would do so \n",
        "you have documents that have the format where the question is on top and then \n",
        "an answer is below and \n",
        "they have a large number of these but \n",
        "probably not on the order of the internet \n",
        "this is probably on the\n",
        "order of maybe thousands of examples and so \n",
        "they then fine-tuned the model to basically only focus on documents \n",
        "that look like that and so\n",
        "you're starting to slowly align it so \n",
        "it's going to expect a question at the top and \n",
        "it's going to expect to complete the answer\n",
        "and uh these very very large models are very sample efficient \n",
        "during their fine tuning so this actually somehow works\n",
        "but that's just step one that's just fine-tuning\n",
        "\n",
        "(2)\n",
        "then they actually have more steps where okay the second step is\n",
        "you let the model respond and then different Raters \n",
        "look at the different responses and rank them for their\n",
        "preference as to which one is better than the other \n",
        "\n",
        "they use that to train a reward model so they can predict\n",
        "basically using a different network \n",
        "how much of any candidate response would be\n",
        "desirable and then once they have a reward model they run PPO \n",
        "which is a form of policy gradient reinforcement learning optimizer \n",
        "to fine-tune this sampling policy so\n",
        "that the answers that  ChatGPT now generates, are expected to score a high\n",
        "reward according to the reward model and so \n",
        "\n",
        "basically there's a whole the alining stage here or fine-tuning stage\n",
        "it's got multiple steps in between there as well and \n",
        "it takes the model from  being a document completer to a question answerer and \n",
        "that's like a whole separate stage \n",
        "\n",
        "a lot of this data is not available publicly it is internal to OpenAI and \n",
        "it's much harder to replicate this stage um and \n",
        "\n",
        "so that's roughly what would give you a ChatGPT and \n",
        "\n",
        "nanoGPT focuses on the pre-training stage \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRKdC1JPvnkz"
      },
      "source": [
        "#### Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ZRGPS5VWvpS3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "207f2716-fc55-4726-db97-ec5996116473"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nso we trained to summarize a Decoder-only Transformer \\nfollowing this famous paper Attention is All You Need from 2017\\n\\nand so that's basically a GPT \\nwe trained it on a Tiny Shakespeare and got sensible results \\n\\nall of the training code is roughly 200 lines of code \\nI will be releasing this code base so also it comes with\\nall the git log commits along the way as we built it up \\n\\nin addition to this code I'm going to release the notebook of course \\nthe Google collab and I hope that gave you a sense for how\\nyou can train um these models like say GPT-3 there will be \\narchitecturally basically identical to\\nwhat we have but they are somewhere between ten thousand and one million times \\nbigger depending on how you count\\n\\nand so that's all I have for now \\nwe did not talk about any of the fine-tuning\\nstages that would typically go on top of this so if you're interested \\nin something that's not just language modeling but you actually want to you\\nknow say perform tasks or you want them to be aligned in a specific way or you\\nwant to detect sentiment or anything like that \\nbasically anytime you don't want something that's just a document completer \\n\\nyou have to complete further stages of fine-tuning which we did not cover and \\nthat could be simple supervised fine-tuning (SFT) or \\nit can be something more fancy \\nlike we see in chargept we actually train a reward model and then\\ndo rounds of PPO to align it with respect to the reward model so \\nthere's a lot more that can be done on top of it \\n\\nI think for now we're starting to get to about two hours mark so I'm going to\\num kind of finish here I hope you enjoyed the lecture and \\n\\nuh yeah go forth and transform, see you later\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "\"\"\"\n",
        "so we trained to summarize a Decoder-only Transformer \n",
        "following this famous paper Attention is All You Need from 2017\n",
        "\n",
        "and so that's basically a GPT \n",
        "we trained it on a Tiny Shakespeare and got sensible results \n",
        "\n",
        "all of the training code is roughly 200 lines of code \n",
        "I will be releasing this code base so also it comes with\n",
        "all the git log commits along the way as we built it up \n",
        "\n",
        "in addition to this code I'm going to release the notebook of course \n",
        "the Google collab and I hope that gave you a sense for how\n",
        "you can train um these models like say GPT-3 there will be \n",
        "architecturally basically identical to\n",
        "what we have but they are somewhere between ten thousand and one million times \n",
        "bigger depending on how you count\n",
        "\n",
        "and so that's all I have for now \n",
        "we did not talk about any of the fine-tuning\n",
        "stages that would typically go on top of this so if you're interested \n",
        "in something that's not just language modeling but you actually want to you\n",
        "know say perform tasks or you want them to be aligned in a specific way or you\n",
        "want to detect sentiment or anything like that \n",
        "basically anytime you don't want something that's just a document completer \n",
        "\n",
        "you have to complete further stages of fine-tuning which we did not cover and \n",
        "that could be simple supervised fine-tuning (SFT) or \n",
        "it can be something more fancy \n",
        "like we see in chargept we actually train a reward model and then\n",
        "do rounds of PPO to align it with respect to the reward model so \n",
        "there's a lot more that can be done on top of it \n",
        "\n",
        "I think for now we're starting to get to about two hours mark so I'm going to\n",
        "um kind of finish here I hope you enjoyed the lecture and \n",
        "\n",
        "uh yeah go forth and transform, see you later\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addVCjPBB73s"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "l23c3L-ZB8vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4733b754-b575-4e0e-a7a4-44b20ace8104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.10/dist-packages (from session-info) (0.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install session-info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "gAKQizrIB-LH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7aced64-b316-47d3-9721-897a2ef5a15d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "session_info        1.0.0\n",
              "torch               2.0.0+cu118\n",
              "-----\n",
              "</pre>\n",
              "<details>\n",
              "<summary>Click to view modules imported as dependencies</summary>\n",
              "<pre>\n",
              "PIL                 8.4.0\n",
              "astunparse          1.6.3\n",
              "backcall            0.2.0\n",
              "certifi             2022.12.07\n",
              "cffi                1.15.1\n",
              "cycler              0.10.0\n",
              "cython_runtime      NA\n",
              "dateutil            2.8.2\n",
              "debugpy             1.6.6\n",
              "decorator           4.4.2\n",
              "defusedxml          0.7.1\n",
              "dot_parser          NA\n",
              "google              NA\n",
              "httplib2            0.21.0\n",
              "ipykernel           5.5.6\n",
              "ipython_genutils    0.2.0\n",
              "kiwisolver          1.4.4\n",
              "matplotlib          3.7.1\n",
              "matplotlib_inline   0.1.6\n",
              "mpl_toolkits        NA\n",
              "mpmath              1.3.0\n",
              "numpy               1.22.4\n",
              "nvfuser             NA\n",
              "opt_einsum          v3.3.0\n",
              "packaging           23.1\n",
              "pexpect             4.8.0\n",
              "pickleshare         0.7.5\n",
              "pkg_resources       NA\n",
              "platformdirs        3.3.0\n",
              "portpicker          NA\n",
              "prompt_toolkit      3.0.38\n",
              "psutil              5.9.5\n",
              "ptyprocess          0.7.0\n",
              "pydev_ipython       NA\n",
              "pydevconsole        NA\n",
              "pydevd              2.9.5\n",
              "pydevd_file_utils   NA\n",
              "pydevd_plugins      NA\n",
              "pydevd_tracing      NA\n",
              "pydot               1.4.2\n",
              "pygments            2.14.0\n",
              "pyparsing           3.0.9\n",
              "sitecustomize       NA\n",
              "six                 1.16.0\n",
              "socks               1.7.1\n",
              "sphinxcontrib       NA\n",
              "storemagic          NA\n",
              "sympy               1.11.1\n",
              "tornado             6.2\n",
              "tqdm                4.65.0\n",
              "traitlets           5.7.1\n",
              "typing_extensions   NA\n",
              "wcwidth             0.2.6\n",
              "zmq                 23.2.1\n",
              "</pre>\n",
              "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
              "<pre>\n",
              "-----\n",
              "IPython             7.34.0\n",
              "jupyter_client      6.1.12\n",
              "jupyter_core        5.3.0\n",
              "notebook            6.4.8\n",
              "-----\n",
              "Python 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
              "Linux-5.10.147+-x86_64-with-glibc2.31\n",
              "-----\n",
              "Session information updated at 2023-05-06 15:35\n",
              "</pre>\n",
              "</details>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "import session_info\n",
        "\n",
        "session_info.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OVrbwQTiCvu-",
        "hLIZxSv7C0zj",
        "LccvHTXlEnML",
        "BG5IWSvjE2my",
        "mueAUGjWKemf",
        "e4zCMAKMU17R",
        "Th3sB15HVMHi"
      ],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}